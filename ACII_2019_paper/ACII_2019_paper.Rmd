---
title: Are facial expressions the genuine display of individuals' subjective feeling? A comparison of human and automatic recognition. 
affiliation:
  ## use one only of the following
  # author-columnar: true         ## one column per author
  institution-columnar: true  ## one column per institution (multiple autors eventually)
  # wide: true                  ## one column wide author/affiliation fields

  institution:
    - name: Dublin City University
      location: Dublin, Ireland
      email: damien.dupre@dcu.ie
      mark: 1
      author:
        - name: Damien Dupr√©
    - name: Grenoble Alpes University
      location: Grenoble, France
      mark: 2
      author:
        - name: Anna Tcherkassof
          email: anna.tcherkassof@univ-grenoble-alpes.fr
abstract: |
  While it has been taken for granted in the development of several automatic facial expression recognition tools, the question of the link between subjective feelings and facial expressions is still a subject of debate. On one hand the behaviorist approach conceives emotions as genetically hardwired and therefore being genuinely displayed through facial expressions. On the other hand the constructivist approach conceives emotions are socially constructed and facial expression as social messages that are not related to emotions. In order to evaluate the link between the subjective feeling of emotions and their recognition based on facial expression, 232 videos of participants recruited to perform an emotion elicitation task were annotated by 1383 human observers as well as by an automatic facial expression classifier. The results show a low accuracy of human observers and of the automatic classifier to infer from the facial expression the subjective feeling of the participants recorded. Based on these results, the hypothesis of genetically hardwired emotion genuinely display is difficult to support whereas the idea of emotion socially constructed and facial expression as display of social messages appears to be more likely. Then, the way to infer emotional and mental state based on facial expressions should be questioned.
header-includes:
  - \usepackage{booktabs}
  - \usepackage{float}
  - \usepackage{tabu}
bibliography: mybibfile.bib
output: rticles::ieee_article
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  cache.extra = knitr::rand_seed, 
  message = FALSE, 
  warning = FALSE, 
  error = FALSE, 
  echo = FALSE,
  fig.pos='H')
#
set.seed(123)# Seed for random number generation
options(scipen = 999)# Disable scientific number format
#
library(kableExtra)
library(tidyverse) # Data wrangling & grammar of graphics
library(here) # Locate files relative to project root
library(caret)
library(plotROC)
library(pROC)
```

```{r data}
# self_report_data
source(here::here("script/data_wrangling_self_report.R"))
self_report_score <- self_report_score %>% 
  dplyr::select(C_Video, sr_emotion = emotion, sr_score = value)
# human_recognition_data
source(here::here("script/data_wrangling_human_recognition.R"))
human_recognition_score <- human_recognition_score %>% 
  dplyr::select(C_Video, hr_emotion = emotion, hr_score = confidence_score)
# automatic_recognition_data
source(here::here("script/data_wrangling_automatic_recognition.R"))
automatic_recognition_score <- automatic_recognition_score %>% 
  dplyr::select(C_Video, ar_emotion = emotion, ar_score = confidence_score) %>% 
  dplyr::filter(C_Video %in% ppt_per_video$C_Video)
```

Introduction
=============
```{r}
# note to myself: instead of calculating the proportion on all the video, calculate the marginal proportion
```

With the development of commercial automatic facial expression recognition tools [see @dupre2018accuracy for a non-exhaustive list of available tools], industries and governments are gradually implementing this technology in order to track humans' emotions in various scenarios (e.g., marketing, healthcare, automotive to name a few). Beside the ethical question of measuring human emotions, these tools are challenging psychological theories about inferring emotions from facial expressions. When automatic facial expression recognition tools are used to measure human emotions, they rest on the premise that facial expressions provide a direct access to expressers' emotions. However the link between emotion felt and facial expressions is far from being established and still remain a hot topic in psychology research.

## The link between emotion and facial expression thought the behaviorist approach
Based on the behaviorist approach initiated by Darwin in *The Expression of the Emotions in Man and Animals* [@darwin1872expression], facial expression are conceived as a genuine display of individuals inner emotional state. This hypothesis is used as a basis for the Basic Emotion Theory (BET) which states that a set of six or seven emotions are universally displayed and are genetically hardwired not only in humans [@ekman1992argument] but also in different animal species [@de2019mama]. According to this view, "when emotions are aroused by perception of a social event, a set of central commands produce patterned emotion-specific changes in multiple systems, including [...] facial expressions." [@ekman2007directed, p49]. Even if this theory obtained a popular support, it fails to explain how individuals can feel emotions without expressing them and how individuals can express emotions without feeling them.

## The link between emotion and facial expression thought the social constructivist approach
Detractors of the Basic Emotion Theory are perceiving emotion not as genetically hardwired but as a learnt association between a given situation and an appropriate response [@averill1980constructivist, @barrett2017emotions]. For the tenants of the constructivist approach, emotions are "concepts" based on past experiences and which are "a collection of embodied, whole brain representations that predict what is about to happen in the sensory environment, what the best action is to deal with impending events, and their consequences for allostasis" [@barrett2017theory, p12]. Following this assumption, faces are used as tools to display signals in social interactions [@crivelli2018facial]. These signals can convey individuals' motivations and readiness [@frijda1997facial] or social messages [@fridlund1995human].

This current paper investigates the link between the subjective feeling of emotions and their recognition from facial expressions. If emotions are hardwired, individuals emotional subjective feeling should be correlated to the recognition of facial expressions from both human observers and automatic classifiers whereas if emotions are social constructs, no correlation between subjective feeling and facial expression recognition should be observed.

Method
=============

To evaluate the link between subjective feeling of emotions and their recognition from facial expressions, participants were recruited to perform an emotion elicitation task while their facial expression was video recorded. Then, the videos was shown to human observer and analysed by an automatic classifier in order to identify which emotion was displayed.

## Emotion Elicitation

```{r self_report_method}
self_report_gender <- self_report_data %>% 
  dplyr::group_by(genre_c) %>% 
  dplyr::summarise(n= n())

self_report_age <- self_report_data %>%
  dplyr::summarise(Age_m = mean(age), Age_sd = sd(age)) %>% 
  dplyr::mutate_all(round,1)
```

For the emotion elicitation experiment, `r nrow(self_report_data)` French participants (`r self_report_gender[self_report_gender$genre_c == "F","n"]` females, `r self_report_gender[self_report_gender$genre_c == "H","n"]` males, *M*age = `r self_report_age[,"Age_m"]`, *SD*age = `r self_report_age[,"Age_sd"]`) were recruited to perform one out of 11 emotions elicitation tasks designed to trigger a positive, negative or neutral emotional state [see @tcherkassof2013dynemo for a description of tasks and procedure]. Participants' face were recorded using an hidden camera resulting `r nrow(self_report_data)` front facing 768x576 videos varying from `r min(metadata_video$ffprobe_duration)`s to `r max(metadata_video$ffprobe_duration)`s.

After their emotion elicitation task the participants had to rate their subjective feeling during the task on likert scales from 0 ("not at all") to 5 ("strongly") according six "basic" emotion labels (i.e., *anger*, *disgust*, *fear*, *happiness*, *surprise* and *sadness*) as well as six "non-basic" emotion labels (i.e., *pride*, *curiosity*, *boredom*, *shame*, *humiliation*, and *disappointment*).

Finally, a debriefing session was perform to ensure that participants were not durably affected by the emotion elicitation task. All the participants gave their agreement on their data and video to be processed for research purpose only.

## Human Facial Expression Recognition

```{r human_recognition_method}
unique_ppt_count <- human_recognition_data %>% 
  dplyr::group_by(source,SEXE_Juge, C_Juge) %>% 
  dplyr::summarise(n = n())

unique_video_count <- human_recognition_data %>% 
  dplyr::select(source,SEXE_Juge, C_Juge, C_Video) %>% 
  unique() %>% 
  dplyr::group_by(C_Video) %>% 
  dplyr::summarise(n = n())
```

For the human facial expression recognition method, `r nrow(unique_ppt_count)` student participants were recruited to annotate `r nrow(unique_video_count)` out of the `r nrow(self_report_data)` video, therefore only the `r nrow(unique_video_count)` annotated videos will be analysed in this paper. Each participants had to annotate between `r min(unique_ppt_count$n)` and `r max(unique_ppt_count$n)` videos resulting that each video was annotated `r mean(unique_video_count$n) %>% round(0)` times on average (*SD* = `r sd(unique_video_count$n) %>% round(0)`).

The annotation of facial expressions was performed on-site using *Oudjat*, a software for designing video annotation experiments [@dupre2015oudjat]. For each video, the annotation procedure hat two steps. First, the participants had to identify the emotional sequences by pressing the space bar of their keyboard to indicate the beginning and the end of the emotional sequences while watching the video. Second, the participants watched each emotional sequence previously identified and had to label the sequence using one of the 12 emotion labels proposed including six "basic" emotion labels (i.e., *anger*, *disgust*, *fear*, *happiness*, *surprise* and *sadness*) and six "non-basic" emotion labels (i.e., *pride*, *curiosity*, *boredom*, *shame*, *humiliation*, and *disappointment*). They also had the possibility to indicate that the sequence was not expressing one of the proposed emotion. 

This annotation procedure results in a uni-dimensional time-series for each video per human observer identifying for each second of the video which emotion was recognized. Then time-series corresponding to the same video were aggregated to calculate the proportion of human observers for each second of the video per emotional label. The sum of each label proportion per second was used as a score to determine which labels corresponds to the overall video (i.e., the highest score). In case of more than one label having the maximum value, the emotion is described as undetermined.

## Automatic Facial Expression Recognition

The `r nrow(unique_video_count)` annotated video were processed with Affdex (SDK v3.4.1). Affdex is an automatic facial expression recognition classifier developed and distributed by Affectiva is a spin-off company resulting from the research activities of MIT media lab created in 2009 [@mcduff2016affdex]. Affdex‚Äôs algorithm uses Histogram of Oriented Gradient (HOG) features and Support Vector Machine (SVM) classifiers in order to recognize facial expressions. For each video frame, Affdex identify the probability of the face to express each of the six "basic" emotion labels (i.e., *anger*, *disgust*, *fear*, *happiness*, *surprise* and *sadness*) as well as additional psychological states such as *valence*, *engagement* or *contempt*, and facial features such as *cheek raise*, *eye widen* or *jaw drop*.

To determine which of the six emotion label can be used to identify each video, the recognition probability for each label by frame was converted into odd ratio by frame [@dente2017measures]. The highest sum of each odd ratio time-series defines the label recognized by the automatic classifier.

Results
============

Whereas the self-reports, the human annotations and the automatic recognition include data on "non-basic" emotion labels and features, the analysis is performed using only the six "basic" emotion labels in order to compare them. The maximum score for self-reports, human annotations and automatic recognition is used to label the video. In case of more than one label obtaining the maximum value, the video is labeled as undetermined.

## Correlation between self-report and human facial expression recognition

```{r sr_hr}
comparison_sr_hr <- dplyr::inner_join(self_report_score,human_recognition_score, by = "C_Video") %>% 
  dplyr::ungroup() %>% 
  dplyr::mutate_if(is.character,as.factor)

confusionMatrix_sr_hr <- caret::confusionMatrix(
  data = comparison_sr_hr$hr_emotion,
  reference = comparison_sr_hr$sr_emotion
  )
```

Emotions self-reported as being characteristic of the elicitation are compared with the emotion recognized by the human observers in a confusion matrix (Figure \ref{fig:confusionMatrix_sr_hr}).

```{r confusionMatrix_sr_hr, fig.height=4, fig.cap="Confusion matrix of between the emotion self-reported as being characteristic of the elicitation with the emotion recognized by the human observers."}
confusionMatrix_sr_hr_freq <- confusionMatrix_sr_hr$table %>% 
  as.data.frame() %>% 
  dplyr::rename(hr_emotion = Prediction, sr_emotion = Reference) %>% 
  dplyr::mutate(prop_video = Freq/sum(Freq)) 

confusionMatrix_sr_hr_freq %>% 
  ggplot(mapping = aes(x = sr_emotion, y = hr_emotion)) +
  geom_tile(aes(fill = Freq), colour = "white") +
  geom_text(
    aes(label = Freq),
    color = "black",
    family="serif",
    size = 4,
    parse = FALSE, 
    lineheight = 0.7
  ) +
  scale_fill_gradient(
    name = "Frequency",
    low = "white", 
    high = "red", 
    ) +
  scale_x_discrete(name = "Self-Reports") +
  scale_y_discrete(name = "Human Observers") +
  theme_minimal() +
  theme(text = element_text(size=16,family="serif"),
        axis.text.x = element_text(size=16, angle = 45, hjust = 0.75,vjust=0.9),
        axis.text.y = element_text(size=16, hjust = 0.5),
        axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0)),
        legend.text = element_text(size=8),
        legend.position = "none")
```

The result of the confusion matrix show a low agreement between emotion felt during the elicitation and emotion recognized by the human annotators (Accuracy = `r round(confusionMatrix_sr_hr$overall["Accuracy"],2)`, 95%CI[`r round(confusionMatrix_sr_hr$overall["AccuracyLower"],2)`,`r round(confusionMatrix_sr_hr$overall["AccuracyUpper"],2)`]; Kappa = `r round(confusionMatrix_sr_hr$overall["Kappa"],2)`) except for *happiness* (`r scales::percent(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "happiness" & confusionMatrix_sr_hr_freq$sr_emotion == "happiness", "prop_video"])`), *surprise* (`r scales::percent(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "surprise" & confusionMatrix_sr_hr_freq$sr_emotion == "surprise", "prop_video"])`) and *disgust* (`r scales::percent(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "disgust" & confusionMatrix_sr_hr_freq$sr_emotion == "disgust", "prop_video"])`). Sensitivity, specificity, precision and F1 score for each emotion can be found Table \ref{tab:confusionTable_sr_hr}. Interestingly human annotators seem to recognize as *surprise* videos in which *happiness* was the highest self-reported emotion (`r scales::percent(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "surprise" & confusionMatrix_sr_hr_freq$sr_emotion == "happiness", "prop_video"])`), and in a lower instance *happiness* videos in which *surprise* was the highest self-reported emotion (`r scales::percent(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "happiness" & confusionMatrix_sr_hr_freq$sr_emotion == "surprise", "prop_video"])`).

```{r confusionTable_sr_hr, results="asis"}
confusionMatrix_sr_hr_table <- confusionMatrix_sr_hr %>%
  magrittr::use_series(byClass) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("Emotion") %>%
  dplyr::mutate(Emotion = gsub(pattern = "Class: ",replacement = "",x = Emotion)) %>% 
  dplyr::select(Emotion, Sensitivity, Specificity, Precision, F1) %>% 
  dplyr::mutate_if(is.numeric,round,2) %>% 
  replace(is.na(.), "\\textit{na.}")

confusionMatrix_sr_hr_table %>%
  knitr::kable(
    "latex",
    caption = "Agreement accuracy metrics for each emotion.",
    booktabs = TRUE,
    digits = 2,
    linesep = "",
    escape = FALSE,
    align = c('l',rep('c', 4))
  ) %>%
  kableExtra::kable_styling(
    font_size = 8,
    full_width = TRUE,
    latex_options = c("hold_position")) %>%
   add_footnote("Note. \\textit{na.} values are produced when not enough data are available to compute accuracy indicators.", notation="none", escape = FALSE)
```

However, the self-report show a very high proportion of undetermined emotional states which reveals not only the possibility of the emotion elicitation tasks to trigger more than one emotion but also the potential limit of using 6-points likert scales for which the participants can easily score to the maximum for more than one emotion.

## Correlation between self-report and automatic facial expression recognition

```{r sr_ar}
comparison_sr_ar <- dplyr::inner_join(
    self_report_score, 
    automatic_recognition_score, 
    by = "C_Video") %>% 
  dplyr::ungroup() %>% 
  dplyr::mutate_if(is.character,as.factor)

dat_table <- table(
  comparison_sr_ar$ar_emotion,
  comparison_sr_ar$sr_emotion
)
emotions <- c("anger","disgust","fear","happiness","sadness","surprise", "undetermined")
emo_table <- matrix(
  0,
  length(emotions),
  length(emotions),
  dimnames = list(emotions, emotions))
emo_table[row.names(dat_table), colnames(dat_table)] <- dat_table

confusionMatrix_sr_ar <- caret::confusionMatrix(as.table(emo_table))
```

As in the previous analysis, emotions self-reported as being characteristic of the elicitation are compared with the emotion recognized by the automatic classifier in a confusion matrix (Figure \ref{fig:confusionMatrix_sr_ar}).

```{r confusionMatrix_sr_ar, fig.height=4, fig.cap="Confusion matrix of between the emotion self-reported as being characteristic of the elicitation with the emotion recognized by the automatic classifier."}
confusionMatrix_sr_ar_freq <- confusionMatrix_sr_ar$table %>% 
  as.data.frame() %>% 
  dplyr::rename(ar_emotion = Var1, sr_emotion = Var2) %>% 
  dplyr::mutate(prop_video = Freq/sum(Freq))

confusionMatrix_sr_ar_freq %>% 
  ggplot(mapping = aes(x = sr_emotion, y = ar_emotion)) +
  geom_tile(aes(fill = Freq), colour = "white") +
  geom_text(
    aes(label = Freq),
    color = "black",
    family="serif",
    size = 4,
    parse = FALSE, 
    lineheight = 0.7
  ) +
  scale_fill_gradient(
    name = "Frequency",
    low = "white", 
    high = "red", 
    ) +
  scale_x_discrete(name = "Self-Reports") +
  scale_y_discrete(name = "Automatic Classifier") +
  theme_minimal() +
  theme(text = element_text(size=16,family="serif"),
        axis.text.x = element_text(size=16, angle = 45, hjust = 0.75,vjust=0.9),
        axis.text.y = element_text(size=16, hjust = 0.5),
        axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0)),
        legend.text = element_text(size=8),
        legend.position = "none")
```

Results obtained for the comparison between emotions self-reported and recognized by the automatic classifier are similar to the ones with human observers (Table \ref{tab:confusionTable_sr_ar}). Overall there is a low agreement between emotion self-reported and emotion recognized by the automatic classifier (Accuracy = `r round(confusionMatrix_sr_ar$overall["Accuracy"],2)`, 95%CI[`r round(confusionMatrix_sr_ar$overall["AccuracyLower"],2)`,`r round(confusionMatrix_sr_ar$overall["AccuracyUpper"],2)`]; Kappa = `r round(confusionMatrix_sr_ar$overall["Kappa"],2)`) except for *happiness* (`r scales::percent(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "happiness" & confusionMatrix_sr_ar_freq$sr_emotion == "happiness", "prop_video"])`) and *surprise* (`r scales::percent(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "surprise" & confusionMatrix_sr_ar_freq$sr_emotion == "surprise", "prop_video"])`). Surprisingly the automatic classifier incorrectly recognized as *disgust* an important proportion of videos in which *happiness* was the highest self-reported emotion (`r scales::percent(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "disgust" & confusionMatrix_sr_ar_freq$sr_emotion == "happiness", "prop_video"])`). In parallel, the automatic classifier recognized as *happiness* and *disgust* videos in which *surprise* was the highest self-reported emotion (respectively `r scales::percent(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "happiness" & confusionMatrix_sr_ar_freq$sr_emotion == "surprise", "prop_video"])` and `r scales::percent(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "disgust" & confusionMatrix_sr_ar_freq$sr_emotion == "surprise", "prop_video"])`).

```{r confusionTable_sr_ar, results="asis"}
confusionMatrix_sr_ar_table <- confusionMatrix_sr_ar %>%
  magrittr::use_series(byClass) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("Emotion") %>%
  dplyr::mutate(Emotion = gsub(pattern = "Class: ",replacement = "",x = Emotion)) %>% 
  dplyr::select(Emotion, Sensitivity, Specificity, Precision, F1) %>% 
  dplyr::mutate_if(is.numeric,round,2) %>% 
  replace(is.na(.), "\\textit{na.}")

confusionMatrix_sr_ar_table %>%
  knitr::kable(
    "latex",
    caption = "Agreement accuracy metrics for each emotion.",
    booktabs = T,
    digits = 2,
    linesep = "",
    escape = FALSE,
    align = c('l',rep('c', 4))
  ) %>%
  kableExtra::kable_styling(
    font_size = 8,
    full_width = TRUE,
    latex_options = c("hold_position")) %>%
   add_footnote("Note. \\textit{na.} values are produced when not enough data are available to compute accuracy indicators.", notation="none", escape = FALSE)
```

A comparable explanation can be provided as the level of undetermined emotion are very high for the self reports.

## Comparison between human and automatic recognition

As previously mentioned, the accuracy of humans observers and the automatic classifier have some similarities. In order to compare which recognition has the highest accuracy a Receiver Operating Characteristic (ROC) curve was calculated (Figure \ref{fig:roc}).

```{r roc, fig.height=6, fig.cap="ROC curve comparing the accuracy in inferring subjective feelings from facial expressions by human observers and automatic recognition."}
roc_sr_hr <- comparison_sr_hr %>% 
  dplyr::mutate_if(is.factor, as.character) %>% 
  dplyr::mutate(correct_classification = case_when(
    sr_emotion == hr_emotion ~ 1,
    sr_emotion != hr_emotion ~ 0
  )) %>% 
  dplyr::rename(rec_score = hr_score) %>% 
  dplyr::mutate(rec_type = "human")

roc_sr_ar <- comparison_sr_ar %>% 
  dplyr::mutate_if(is.factor, as.character) %>% 
  dplyr::mutate(correct_classification = case_when(
    sr_emotion == ar_emotion ~ 1,
    sr_emotion != ar_emotion ~ 0
  )) %>% 
  dplyr::rename(rec_score = ar_score) %>% 
  dplyr::mutate(rec_type = "automatic")

auc <- dplyr::bind_rows(roc_sr_hr,roc_sr_ar) %>% 
  tidyr::nest(-rec_type) %>%
  dplyr::mutate(rec_type = as.factor(rec_type),
                roc_obj = purrr::map(data, ~ roc(.$correct_classification,.$rec_score)),
                roc_value = purrr::map(roc_obj,auc)) %>%
  tidyr::unnest(roc_value) %>% 
  dplyr::select(rec_type,roc_value) %>% 
  dplyr::mutate(roc_value = round(roc_value,2))

dplyr::bind_rows(roc_sr_hr,roc_sr_ar) %>% 
  ggplot(aes(d = correct_classification, m = rec_score, color = rec_type)) + 
  geom_roc(labels = FALSE,pointsize = 0) +
  style_roc(xlab = "False Positive Rate",
            ylab = "True Positive Rate", 
            theme = theme_minimal) + 
  scale_color_manual(
    "Classifiers",
    values = c("#e41a1c","#377eb8")
  ) +
  theme(text = element_text(size=18,family="serif"),
        strip.text.x = element_text(face="bold",size=18),
        axis.title.x = element_text(margin = margin(t = 20, r = 0, b = 0, l = 0)),
        axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0)),
        axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 10),
        legend.text = element_text(size=10),
        legend.position = "bottom") + 
  geom_abline(intercept = 0, slope = 1, linetype = "dashed")
```

The ROC curve and its Area Under the Curve (AUC) values shows that the automatic classifier are more accurate than human observers in inferring subjective feeling from facial expressions (human AUC = `r auc[auc$rec_type == "human","roc_value"]` *vs.* automatic AUC = `r auc[auc$rec_type == "automatic","roc_value"]`).

Conclusion
============
Despite being one on the most investigated question in affective science, the link between emotion felt and facial expression is a hot topic and no clear evidence have been found to definitely answer it. However, with the growing interest of industries and government to monitor individual's psychological states, evidences are showing that facial expressions are in reality not expressing emotions [@mckeown2013analogical]. This research aimed to provide some empirical data to the question. The subjective feeling of participants was compared with human recognition on one side and automatic recognition on the other side. The results reveals a low accuracy for both humans and automatic classifier to accurately identify the inner emotional states of these individuals based on their facial expressions.

Some limitations to this process should be stated over the use of self-reports to evaluate individual's subjective feelings. Accessing the inner subjective feeling can be biased if not impossible. Moreover the laboratory setting can trigger ambiguous and "non-basic" emotion labels which were not analysed in this research. The procedure used for human annotation can also be open to dispute. Instead of asking the human annotators to provide an unique label, a more subtle approach was chosen to mimic results provided by the automatic classifier. In this regard, the results of the human annotation could have been more ambiguous because it is not the natural way that people are inferring human emotions. Finally, the automatic classifier algorithm can also be problematic. Based on training datasets which are most of the time using prototypical facial expression of the "basic" emotion labels, the algorithm to classify facial expressions can be held in check by the spontaneity of the expressions analysed which are less likely to be prototypical.

Considering the above, the results provides an additional evidence that individuals' subjective feeling can not be inferred from facial expressions and in our case invalidate the hypothesis of hardwired emotions. Advancements in identifying "non-basic" emotion labels [@mcduff2016discovering] as well as non-prototypical facial expression have been made in the developpement of automatic facial expression recognition tools, however this result suggests that automatic facial expression recognition tools should be focused on evaluating facial morphology features such as action units [e.g., OpenFace @baltruvsaitis2016openface] rather than inferring potential emotional or affective states.  

Acknowledgment {#acknowledgment}
==============

The authors would like to thank Brigitte Meillon and Jean Michel Adam who developed the software used to collect and preprocess human observer annotations.

References {#references .numbered}
==========
