---
title: "Coherence between subjective feeling and facial expression: A comparison of human and automatic recognition" 
affiliation:
  institution-columnar: true

  institution:
    - name: Dublin City University
      location: Dublin, Ireland
      email: damien.dupre@dcu.ie
      mark: 1
      author:
        - name: Damien Dupré
    - name: Grenoble Alpes University
      location: Grenoble, France
      mark: 2
      author:
        - name: Anna Tcherkassof
          email: anna.tcherkassof@univ-grenoble-alpes.fr
abstract: |
  While it has been taken for granted in the development of several automatic facial expression recognition tools, the question of the link between subjective feelings and facial expressions is still a subject of debate. On one hand the behaviorist approach conceives emotions as genetically hardwired and therefore being genuinely displayed through facial expressions. On the other hand the constructivist approach conceives emotions as socially constructed, the emotional meaning of a facial expression being inferred by the observer. In order to evaluate the link between the subjective feeling of emotions and their recognition based on facial expression, 232 videos of encoders recruited to carry out an emotion elicitation task were annotated by 1383 human observers as well as by an automatic facial expression classifier. Results show a low accuracy of human observers and of the automatic classifier to infer the subjective feeling from the facial expressions displayed by encoders . They also show a weak link between self-reported emotional states and facial emotional displays. Based on these results, the hypothesis of genetically hardwired emotion genuinely display is difficult to support whereas the idea of emotion and facial expression socially constructed appears to be more likely. Then automatic emotion recognition tools based on facial expressions should be questioned.

header-includes:
  - \usepackage{booktabs}
  - \usepackage{float}
  - \usepackage{tabu}
  - \usepackage{wrapfig}
  - \usepackage[none]{hyphenat}
  - \floatplacement{figure}{H}
  
bibliography: mybibfile.bib
output: word_document
csl: http://www.zotero.org/styles/ieee
fig_caption: yes
keep_tex: yes
number_sections: yes
includes:
  in_header: preamble-latex.tex
---

\begin{IEEEkeywords}
Facial expression, self-report, human observer, automatic recognition.
\end{IEEEkeywords}

```{r setup, include = FALSE}
# options ----------------------------------------------------------------------
knitr::opts_chunk$set(
  cache.extra = knitr::rand_seed, 
  message = FALSE, 
  warning = FALSE, 
  error = FALSE, 
  echo = FALSE)

set.seed(123)# Seed for random number generation
options(scipen = 999)# Disable scientific number format

# packages ----------------------------------------------------------------------
library(kableExtra) # Tables
library(tidyverse) # Data wrangling & grammar of graphics
library(here) # Locate files relative to project root
library(caret) # Confusion matrices
library(papaja) # Correlation results
```

```{r data}
# self_report_data
source(here::here("script/data_wrangling_self_report.R"))
self_report_score <- self_report_score %>% 
  dplyr::select(C_Video, sr_emotion = emotion, sr_score = value)
# human_recognition_data
source(here::here("script/data_wrangling_human_recognition.R"))
human_recognition_score <- human_recognition_score %>% 
  dplyr::select(C_Video, hr_emotion = emotion, hr_score = confidence_score)
# automatic_recognition_data
source(here::here("script/data_wrangling_automatic_recognition.R"))
automatic_recognition_score <- automatic_recognition_score %>% 
  dplyr::select(C_Video, ar_emotion = emotion, ar_score = confidence_score) %>% 
  dplyr::filter(C_Video %in% ppt_per_video$C_Video)
```

I. Introduction
=============

With the development of commercial automatic facial expression recognition tools [see @dupre2018accuracy for a non-exhaustive list of available tools], industries and governments are gradually implementing this technology in order to track humans’ emotions in various scenarios (*e.g.*, marketing, healthcare, automotive to name a few). This technoloogy rests on the premise that facial expressions provide a direct access to individuals’ subjective feeling. Even if this premise is central to the modern mainstream approach of human emotion, recent research in affective science are challenging it. Once the two competing approaches briefly described as well as the predictions they respectively entail, an experiment testing these hypotheses will be presented and its results analyzed in order to provide empirical evidence to contribute to answer the question.

## A. The Behaviorist Approach
Based on the behaviorist approach initiated by Darwin in *The Expression of the Emotions in Man and Animals* [@darwin1872expression], facial expressions are conceived as a genuine displays of individuals’ inner emotional state. This hypothesis is used as a basis for the Basic Emotion Theory (BET) which states that a set of six emotions are universally displayed and are genetically hardwired not only in humans [@ekman1992argument] but also in different animal species [@de2019mama]. According to this view, *“when emotions are aroused by perception of a social event, a set of central commands produce patterned emotion-specific changes in multiple systems, including […] facial expressions.”* [@ekman2007directed, 49]. To cope with critics, several amendments have been made to the BET, increasing the number of basic emotions from six to seven [@ekman1988universality] as well as adding the concept of “display rules” to explain cultural differences in the management of facial expressions [@ekman1987universals]. 

Even if this theory obtained a popular support, it fails to explain how individuals can feel emotions without expressing them and how individuals can express emotions without feeling them, in instances in which display rules cannot be called upon [@kraut1979social; @duran2017coherence]. These evidences have led to an alternative conception, notably the social constructivist approach.

## B. The Social Constructivist Approach
Detractors of the Basic Emotion Theory consider emotion not as genetically hardwired but as a learnt association between a given situation and an appropriate response [@averill1980constructivist; @barrett2017emotions]. For the tenants of the constructivist approach, emotions are “concepts” based on past experiences and which are *“a collection of embodied, whole brain representations that predict what is about to happen in the sensory environment, what the best action is to deal with impending events, and their consequences for allostasis”* [@barrett2017theory, 12]. Following this assumption, faces are best conceived as tools displaying signals in social interactions [@crivelli2018facial]. These signals can convey individuals’ motivations and readiness [@frijda1997facial] or social messages [@fridlund1994human]. Therefore, facial expressions are thought as behaviors which meaning is inferred by the observer. Findings support this observer dependence [@lindquist2013s; @niedenthal2017embodied]. They show that to make meaning of another person’s facial behavior, the perceiver relies in particular on her/his knowledge about emotion categories.

## C. Emotional Facial Expression Recognition
Regarding emotional facial expression (EFE) recognition, the behaviorist approach assertions and the constructivist approach claims lead to two divergent predictions. The foremost postulates that, when triggered, each basic emotion is expressed by a prototypical face (non basic emotions being blends of the basic ones). Basic emotions are easily recognized by all human observers and emotional states are accessible by facial measurement. As the recognition of facial expressions is based on the identification of specific patterns of facial movements, it implies that EFE recognition of both human observers and automatic classifiers should be as accurate. The constructivist approach, in contrast, affirms that facial expressions do not provide a direct access to individuals’ subjective feeling. Even if the face possibly moves during an emotional episode, facial muscle movements are not linked in a one-to-one manner to a specific discrete emotional experience [@doyle2017language, p418]. Instead, emotions are mentally constructed by the perceiver and mental categories of emotions are needed to accurately categorize facial movements. Therefore, since there is no emotional prototypical face, one should expect human observers’ superior ability to accurately recognize EFE as compared to automatic EFE recognition tools (Figure \ref{fig:models_img}).

```{r models_img, fig.height=5, fig.cap="\\label{fig:models_img}Comparison of behaviorist and social constructivist approaches according to the expected differences in emotion recognition."}
# knitr::include_graphics("C:/Users/dupred/Desktop/Projects/dynemo_feel_express/img/dynemo_img.png", auto_pdf = FALSE)
raster::stack(here::here("./img/models.png")) %>%
  raster::plotRGB()
```

The aim of the current paper is to investigate the link between the subjective feeling of emotions and its recognition from facial expressions in order to give credit either to the behaviorist plea or to the constructivist’s one. Contrary to most studies using posed and static EFE, the present study focuses on natural EFE. Spontaneous and dynamic facial reactions to emotional elicitations are under consideration to ensure the generalizability of the results to emotional behaviors in ordinary life.

II. Method
=============

To evaluate the link between subjective feeling of emotions and their recognition from facial expressions, encoders were first recruited to perform an emotion elicitation task while their facial expression was video recorded. Then, the videos of the encoders’ faces were shown to human observers and were also analysed by an automatic classifier in order to identify which emotion was displayed.

## A. Emotion Elicitation

```{r self_report_method}
self_report_gender <- self_report_data %>% 
  dplyr::group_by(genre_c) %>% 
  dplyr::summarise(n= n())

self_report_age <- self_report_data %>%
  dplyr::summarise(Age_m = mean(age), Age_sd = sd(age)) %>% 
  dplyr::mutate_all(round,1)
```

For the emotion elicitation experiment, `r nrow(self_report_data)` French participants (`r self_report_gender[self_report_gender$genre_c == "F","n"]` females, `r self_report_gender[self_report_gender$genre_c == "H","n"]` males, *M*~age~ = `r self_report_age[,"Age_m"]`, *SD*~age~ = `r self_report_age[,"Age_sd"]`) were recruited to perform one out of 11 emotion elicitation tasks designed to trigger a positive, a specific negative or a neutral emotional state. Encoders’ face were recorded using an hidden camera resulting `r nrow(self_report_data)` front facing 768x576 videos varying from `r min(metadata_video$ffprobe_duration)`s to `r max(metadata_video$ffprobe_duration)`s (Figure \ref{fig:dynemo_img}). These recordings form the DynEmo database [see @tcherkassof2013dynemo for a full description of tasks and procedure].

```{r dynemo_img, fig.height=3, fig.cap="\\label{fig:dynemo_img}Example of a front facing recording synced with the full view of the participant and the elicitation task. This picture is taken from a pilot with projects collaborators and all gave a consent for the publication of their photos and videos."}
raster::stack(here::here("./img/dynemo_img.png")) %>%
  raster::plotRGB()
```

After the emotion elicitation task the encoders rated their subjective feeling on Likert scales from 0 ("not at all") to \nolinebreak 5 \nolinebreak ("strongly") related to six “basic” emotion labels (*i.e.*, *anger*, *disgust*, *fear*, *happiness*, *surprise* and *sadness*) as well as six “non-basic” emotion labels (*i.e.*, *pride*, *curiosity*, *boredom*, *shame*, *humiliation*, and *disappointment*).

Finally, a debriefing session was performed to ensure that encoders were not durably affected by the emotion elicitation task. The debriefing was also used to check that encoders did not guess the real purpose of the experiment (*e.g.*, being filmed while they were performing an emotional elicitation task) to guarantee facial expressions’ genuineness. All encoders gave their agreement on their data and video to be processed for research purpose only.

## B. Human Facial Expression Recognition

```{r human_recognition_method}
unique_ppt_count <- human_recognition_data %>% 
  dplyr::group_by(source,SEXE_Juge, C_Juge) %>% 
  dplyr::summarise(n = n())

unique_video_count <- human_recognition_data %>% 
  dplyr::select(source,SEXE_Juge, C_Juge, C_Video) %>% 
  unique() %>% 
  dplyr::group_by(C_Video) %>% 
  dplyr::summarise(n = n())
```

For the human facial expression recognition method, `r nrow(unique_ppt_count)` \nolinebreak student participants were recruited to annotate `r nrow(unique_video_count)` out of the `r nrow(self_report_data)` videos, therefore only the `r nrow(unique_video_count)` annotated videos will be analysed in this paper. Because videos have different durations, participants had to annotate a series of video corresponding to 30min long in total. Each video was annotated `r mean(unique_video_count$n) %>% round(0)` times on average (*SD* = `r sd(unique_video_count$n) %>% round(0)`).

The annotation of facial expressions was performed on-site using *Oudjat*, a software for designing video annotation experiments [@dupre2015oudjat]. For each video, the annotation procedure followed two steps. First, the participants had to identify the emotional sequences by pressing the space bar of their keyboard to indicate the beginning and the end of the emotional sequences while watching the video. Second, the participants watched each emotional sequence previously identified and labeled the sequence using one of the 12 \nolinebreak emotions proposed including six “basic” emotion labels (*i.e.*, *anger*, *disgust*, *fear*, *happiness*, *surprise* and *sadness*) and six "non-basic" emotion labels (*i.e.*, *pride*, *curiosity*, *boredom*, *shame*, *humiliation*, and *disappointment*). They also had the possibility to indicate that the sequence was expressing none of the proposed emotion labels.

This annotation procedure results in a uni-dimensional time-series for each video per human observer identifying for each second of the video which emotion was recognized. Then, time-series corresponding to the same video were aggregated to calculate the proportion of human observers $x_{video_{i}.label_{j}.t_{k}}$ for each second of the video per emotional label (EQ\ref{eq:1}).

\begin{equation}
\label{eq:1}
x_{video_{i}.label_{j}.t_{k}} = \frac{n_{video_{i}.label_{j}.t_{k}}}{n_{video_{k}}}
\end{equation}

where *i* is one of the `r nrow(unique_video_count)` videos, *j* is one of the six “basic” emotion labels, *k* for each second of the video.

## C. Automatic Facial Expression Recognition

The `r nrow(unique_video_count)` annotated video were processed with Affdex (SDK v3.4.1). Affdex is an automatic facial expression recognition classifier developed and distributed by Affectiva is a spin-off company resulting from the research activities of MIT media lab created in 2009 [@mcduff2016affdex]. Affdex’s algorithm uses Histogram of Oriented Gradient (HOG) features and Support Vector Machine (SVM) classifiers in order to recognize facial expressions. For each video frame, Affdex identifies the probability $p_{video_{i}.label_{j}.t_{k}}$ from 0 to 100 (rescaled to 0 to 1 for the analysis) of the face as expressing one of the six “basic” emotion labels (*i.e.*, *anger*, *disgust*, *fear*, *happiness*, *surprise* and *sadness*) as well as additional psychological states such as *valence*, *engagement* or *contempt*, and facial features such as *cheek raise*, *eye widen* or *jaw drop*.

For both human and automatic recognition, to determine which of the six “basic” emotions can be used to identify each video, the recognition probability for each label by frame was converted into odd ratio by label [@dente2017measures]. The highest sum of each odd ratio time-series defines the label recognized by the automatic classifier (EQ\ref{eq:2} for human recognition and EQ\ref{eq:3} for automatic recognition).
\begin{equation}
\label{eq:2}
video_{i}.label = \max\left(\frac{\sum_{k=1}^{n}x_{video_{i}.label_{j}.t_{k}}}{\sum_{k=1}^{n}x_{video_{i}.t_{k}}}\right)
\end{equation}

\begin{equation}
\label{eq:3}
video_{i}.label = \max\left(\frac{\sum_{k=1}^{f}p_{video_{i}.label_{j}.t_{k}}}{\sum_{k=1}^{f}p_{video_{i}.t_{k}}}\right)
\end{equation}

where *i* is one of the `r nrow(unique_video_count)` videos, *j* is one of the six “basic” emotion labels, *k* for each second of the *n* second video or for each frame of the *f* frame video.

III. Results
============

Since encoders’ self-reports, human annotations and the automatic recognition include data on “non-basic” emotion labels and features, the analysis is performed using only the six “basic” emotion labels in order to compare them. The maximum score for self-reports, human annotations and automatic recognition is used to label the video. In case of more than one label obtaining the maximum value, the video is labeled as undetermined.

```{r corr-matrix}
list_video <- human_recognition_score$C_Video
list_emotion <- c("disgust","fear","happiness","surprise","sadness","anger")

grid_data <- expand.grid(list_video = list_video, list_emotion = list_emotion)

self_report_result <- self_report_score %>%
  dplyr::filter(sr_emotion != "undetermined") %>%
  dplyr::mutate(sr_score = 1)

human_recognition_result <- human_recognition_score %>%
  dplyr::filter(hr_emotion != "undetermined") %>%
  dplyr::mutate(hr_score = 1)

automatic_recognition_result <- automatic_recognition_score %>%
  dplyr::filter(ar_emotion != "undetermined") %>%
  dplyr::mutate(ar_score = 1)

recognition_result <- grid_data %>%
  dplyr::left_join(self_report_result, by = c(
    "list_video" = "C_Video",
    "list_emotion" = "sr_emotion"
    )
  ) %>%
  dplyr::left_join(human_recognition_result, by = c(
    "list_video" = "C_Video",
    "list_emotion" = "hr_emotion"
    )
  ) %>%
  dplyr::left_join(automatic_recognition_result, by = c(
    "list_video" = "C_Video",
    "list_emotion" = "ar_emotion"
    )
  ) %>%
  replace(is.na(.), 0)

corr_sr_hr <- cor.test(
  recognition_result$sr_score, 
  recognition_result$hr_score, 
  alternative = "two.sided"
  ) %>% 
  papaja::apa_print()

corr_sr_ar <- cor.test(
  recognition_result$sr_score, 
  recognition_result$ar_score, 
  alternative = "two.sided"
  ) %>% 
  papaja::apa_print()

corr_hr_ar <- cor.test(
  recognition_result$hr_score, 
  recognition_result$ar_score, 
  alternative = "two.sided"
  ) %>% 
  papaja::apa_print()
```

## A. Human Observers' Accuracy

```{r sr_hr}
comparison_sr_hr <- dplyr::inner_join(self_report_score,human_recognition_score, by = "C_Video") %>% 
  dplyr::ungroup() %>% 
  dplyr::filter(hr_emotion != "undetermined") %>% 
  dplyr::filter(sr_emotion != "undetermined") %>% 
  dplyr::mutate_if(is.character,as.factor)

confusionMatrix_sr_hr <- caret::confusionMatrix(
  data = comparison_sr_hr$hr_emotion,
  reference = comparison_sr_hr$sr_emotion
  )
```

The overall correlation of recognition and non-recognition between self-reported emotions and human observers recognition is significant but low (`r corr_sr_hr$full_result`). In order to identify differences according the emotional labels, encoders’ subjective feeling is compared with human observers recognition in a confusion matrix (Figure \ref{fig:confusionMatrix_sr_hr}).

```{r confusionMatrix_sr_hr, fig.height=6, fig.cap="\\label{fig:confusionMatrix_sr_hr}Confusion matrix of between the emotion self-reported as being characteristic of the elicitation with the emotion recognized by the human observers."}
confusionMatrix_sr_hr_freq <- confusionMatrix_sr_hr$table %>% 
  as.data.frame() %>% 
  dplyr::rename(hr_emotion = Prediction, sr_emotion = Reference) %>% 
  dplyr::mutate(n_video = sum(Freq)) %>% 
  dplyr::mutate(prop_video = Freq/sum(Freq)) %>% 
  dplyr::group_by(sr_emotion) %>% 
  dplyr::mutate(n_sr = sum(Freq)) %>%
  dplyr::mutate(prop_sr = Freq/sum(Freq)) %>% 
  dplyr::ungroup()

confusionMatrix_sr_hr_freq %>% 
  ggplot(mapping = aes(x = sr_emotion, y = hr_emotion)) +
  geom_tile(aes(fill = prop_sr), colour = "white") +
  geom_text(
    aes(label = paste0(scales::percent(prop_sr), "\n(", Freq,"/",n_sr, ")")),
    color = "black",
    size = 4,
    family="serif",
    parse = FALSE,
    lineheight = 0.7)+
  scale_fill_gradient(
    name = "Proportion of Recognized\nSelf-Reported Emotion",
    low = "white", 
    high = "red", 
    labels = scales::percent,
    limits = c(0,1)
  ) +
  scale_x_discrete(name = "Self-Reports") +
  scale_y_discrete(name = "Human Observers") +
  theme_minimal() +
  theme(text = element_text(size=16,family="serif"),
        axis.text.x = element_text(size=16, angle = 45, hjust = 0.75,vjust=0.9),
        axis.text.y = element_text(size=16, hjust = 0.5),
        axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0)),
        legend.text = element_text(size=8),
        legend.position = "bottom")
```

Each emotion label used to describe encoders’ self-reported subjective feeling (*i.e.*, the label rated with the highest value) is compared with the emotion labels which were rated with the highest score by human observers. Results of the confusion matrix show a moderate agreement between the emotion felt by the encoder during the elicitation and the emotion recognized by the human observers (Accuracy \nolinebreak = \nolinebreak `r round(confusionMatrix_sr_hr$overall["Accuracy"],2)`, 95% CI [`r round(confusionMatrix_sr_hr$overall["AccuracyLower"],2)`,`r round(confusionMatrix_sr_hr$overall["AccuracyUpper"],2)`]; Kappa = `r round(confusionMatrix_sr_hr$overall["Kappa"],2)`) except for *disgust* (`r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "disgust" & confusionMatrix_sr_hr_freq$sr_emotion == "disgust", "prop_sr"]))` of the videos self-reported). These results are far from those classicaly obtained in the literature for emotional facial expression recognition which ranges between 60% and 80% accuracy. However these results are mostly obtained with static (*i.e.,* pictures) and posed (*i.e.,* displayed by actors) facial expressions using only 6 emotional labels in a forced-choice paradigm. 

Interestingly human observers seem to recognize *surprise* expressed in videos where *anger*, *fear* *happiness* and *sadness* was the highest self-reported emotion (respectively `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "surprise" & confusionMatrix_sr_hr_freq$sr_emotion == "anger", "prop_sr"]))`, `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "surprise" & confusionMatrix_sr_hr_freq$sr_emotion == "fear", "prop_sr"]))`, `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "surprise" & confusionMatrix_sr_hr_freq$sr_emotion == "happiness", "prop_sr"]))` and `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "surprise" & confusionMatrix_sr_hr_freq$sr_emotion == "sadness", "prop_sr"]))` of the videos self-reported), and in a lower instance *happiness* was recognized in videos where *fear* and *surprise* was the highest self-reported emotion (respectively `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "happiness" & confusionMatrix_sr_hr_freq$sr_emotion == "fear", "prop_sr"]))` and `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "happiness" & confusionMatrix_sr_hr_freq$sr_emotion == "surprise", "prop_sr"]))` of the videos self-reported). 

Sensitivity, specificity, precision and F1 score for each emotion reveals that *happiness* has the highest coherence ratio whereas *sadness* has the lowest coherence ratio between true positives and false positives (Table \ref{table:confusionTable_sr_hr}).

```{r confusionTable_sr_hr, results="asis"}
confusionMatrix_sr_hr_table <- confusionMatrix_sr_hr %>%
  magrittr::use_series(byClass) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("Emotion") %>%
  dplyr::mutate(Emotion = gsub(pattern = "Class: ",replacement = "",x = Emotion)) %>% 
  dplyr::select(Emotion, Sensitivity, Specificity, Precision, F1) %>% 
  dplyr::mutate_if(is.numeric,round,2) %>% 
  replace(is.na(.), "\\textit{na.}")

confusionMatrix_sr_hr_table %>%
  knitr::kable(
    "latex",
    caption = "\\label{table:confusionTable_sr_hr}Human recognition accuracy metrics for each emotion.",
    booktabs = TRUE,
    digits = 2,
    linesep = "",
    escape = FALSE,
    align = c('l',rep('c', 4))
  ) %>%
  kableExtra::kable_styling(
    latex_options = "HOLD_position",
    font_size = 8,
    full_width = TRUE 
    ) %>%
   add_footnote("Note. \\textit{na.} values are produced when not enough data are available to compute accuracy indicators.", notation="none", escape = FALSE)
```

Accuracy metrics by emotional labels indicate a discrepancy in the ratio of true/false positives. Whereas *happiness* and *disgust* obtain the highest scores, *anger*, *surprise* and *sadness* have the lowest recognition ratio. The underlying effect of expression intensity may explain why *happiness* and *disgust* are easily recognized. *Anger* and *sadness* as non-socialy desirable emotion may be have been felt but not expressed.

However, self-reports show a significant proportion of undetermined emotional states (`r scales::percent(nrow(list_automatic_ties)/358)` of the 358 videos) which reveals the potential limit of using 6-points likert scales to measure emotional self-reports. Indeed, encoders can easily score to the maximum for more than one emotion.

## B. Automatic Classifier's Accuracy

```{r sr_ar}
comparison_sr_ar <- dplyr::inner_join(self_report_score,automatic_recognition_score, by = "C_Video") %>% 
  dplyr::ungroup() %>% 
  dplyr::filter(ar_emotion != "undetermined") %>% 
  dplyr::filter(sr_emotion != "undetermined") %>% 
  dplyr::mutate_if(is.character,as.factor)

confusionMatrix_sr_ar <- caret::confusionMatrix(
  data = comparison_sr_ar$ar_emotion,
  reference = comparison_sr_ar$sr_emotion
  )
```

Similarly to the previous analysis, the overall correlation of recognition and non-recognition revealed a significant but very low link between self-reported emotions and automatic \nolinebreak classifier's recognition (`r corr_sr_ar$full_result`). A confusion matrix was used to compare encoders’ subjective feeling with the emotion label recognized by the automatic classifier (Figure \ref{fig:confusionMatrix_sr_ar}).

```{r confusionMatrix_sr_ar, fig.height=6, fig.cap="\\label{fig:confusionMatrix_sr_ar}Confusion matrix of between the emotion self-reported as being characteristic of the elicitation with the emotion recognized by the automatic classifier."}
confusionMatrix_sr_ar_freq <- confusionMatrix_sr_ar$table %>% 
  as.data.frame() %>% 
  dplyr::rename(ar_emotion = Prediction, sr_emotion = Reference) %>% 
  dplyr::mutate(n_video = sum(Freq)) %>% 
  dplyr::mutate(prop_video = Freq/sum(Freq)) %>% 
  dplyr::group_by(sr_emotion) %>% 
  dplyr::mutate(n_sr = sum(Freq)) %>%
  dplyr::mutate(prop_sr = Freq/sum(Freq)) %>% 
  dplyr::ungroup()

confusionMatrix_sr_ar_freq %>% 
  ggplot(mapping = aes(x = sr_emotion, y = ar_emotion)) +
  geom_tile(aes(fill = prop_sr), colour = "white") +
  geom_text(
    aes(label = paste0(scales::percent(prop_sr), "\n(", Freq,"/",n_sr, ")")),
    color = "black",
    size = 4,
    family="serif",
    parse = FALSE,
    lineheight = 0.7)+
  scale_fill_gradient(
    name = "Proportion of Recognized\nSelf-Reported Emotion",
    low = "white", 
    high = "red", 
    labels = scales::percent,
    limits = c(0,1)
  ) +
  scale_x_discrete(name = "Self-Reports") +
  scale_y_discrete(name = "Automatic Classifier") +
  theme_minimal() +
  theme(text = element_text(size=16,family="serif"),
        axis.text.x = element_text(size=16, angle = 45, hjust = 0.75,vjust=0.9),
        axis.text.y = element_text(size=16, hjust = 0.5),
        axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0)),
        legend.text = element_text(size=8),
        legend.position = "bottom")
```

Results obtained for the comparison between emotions self-reported and recognized by the automatic classifier are somewhat similar to the ones with human observers (Table \nolinebreak \ref{table:confusionTable_sr_ar}). Overall, a low agreement between emotion self-reported and emotion recognized by the automatic classifier (Accuracy \nolinebreak = \nolinebreak `r round(confusionMatrix_sr_ar$overall["Accuracy"],2)`, 95% CI [`r round(confusionMatrix_sr_ar$overall["AccuracyLower"],2)`,`r round(confusionMatrix_sr_ar$overall["AccuracyUpper"],2)`]; Kappa \nolinebreak = \nolinebreak `r round(confusionMatrix_sr_ar$overall["Kappa"],2)`) except for *happiness* (`r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "happiness" & confusionMatrix_sr_ar_freq$sr_emotion == "happiness", "prop_sr"]))` of the video self-reported) is watched.

Surprisingly the automatic classifier incorrectly recognized as *disgust* an important proportion of videos in which *anger*, *happiness* and *surprise* was the highest self-reported emotion (respectively `r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "disgust" & confusionMatrix_sr_ar_freq$sr_emotion == "anger", "prop_sr"]))`, `r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "disgust" & confusionMatrix_sr_ar_freq$sr_emotion == "happiness", "prop_sr"]))` and `r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "disgust" & confusionMatrix_sr_ar_freq$sr_emotion == "surprise", "prop_sr"]))` of the videos self-reported). In parallel, the automatic classifier recognized as *happiness* videos in which *fear* and *surprise* was the highest self-reported emotion (respectively `r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "happiness" & confusionMatrix_sr_ar_freq$sr_emotion == "fear", "prop_sr"]))` and `r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "disgust" & confusionMatrix_sr_ar_freq$sr_emotion == "surprise", "prop_sr"]))` of the videos self-reported).

```{r confusionTable_sr_ar, results="asis"}
confusionMatrix_sr_ar_table <- confusionMatrix_sr_ar %>%
  magrittr::use_series(byClass) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("Emotion") %>%
  dplyr::mutate(Emotion = gsub(pattern = "Class: ",replacement = "",x = Emotion)) %>% 
  dplyr::select(Emotion, Sensitivity, Specificity, Precision, F1) %>% 
  dplyr::mutate_if(is.numeric,round,2) %>% 
  replace(is.na(.), "\\textit{na.}")

confusionMatrix_sr_ar_table %>%
  knitr::kable(
    "latex",
    caption = "\\label{table:confusionTable_sr_ar}Autonatic recognition accuracy metrics for each emotion.",
    booktabs = T,
    digits = 2,
    linesep = "",
    escape = FALSE,
    align = c('l',rep('c', 4))
  ) %>%
  kableExtra::kable_styling(
    latex_options = c("HOLD_position"),
    font_size = 8,
    full_width = TRUE
    ) %>%
   add_footnote("Note. \\textit{na.} values are produced when not enough data are available to compute accuracy indicators.", notation="none", escape = FALSE)
```

A comparable explanation involing the amount of undertermined video based on self-reports can be provided as the level of undetermined emotions are very high for the self reports.

## C. Comparison Between Human and Automatic Recognition

As previously mentioned, human observers appear to be more accurate than the automatic classifier to recognize individuals’ subjective feeling (human observers Accuracy \nolinebreak = \nolinebreak`r round(confusionMatrix_sr_hr$overall["Accuracy"],2)`; automatic classifier Accuracy = `r round(confusionMatrix_sr_ar$overall["Accuracy"],2)`; `r corr_hr_ar$full_result`). However, both make mistakes.

A third confusion matrix is used to compare similarities (diagonal) and differences between human observers and automatic classifier in classifing the six emotion labels (Figure \nolinebreak \ref{fig:confusionMatrix_hr_ar}).

```{r confusionMatrix_hr_ar, fig.height=6, fig.cap="\\label{fig:confusionMatrix_hr_ar}Proportion of emotion labels classified by human observers which are recognized by the automatif classifier."}
comparison_hr_ar <- dplyr::inner_join(human_recognition_score,automatic_recognition_score, by = "C_Video") %>% 
  dplyr::ungroup() %>% 
  dplyr::filter(ar_emotion != "undetermined") %>% 
  dplyr::filter(hr_emotion != "undetermined") %>% 
  dplyr::mutate_if(is.character,as.factor)

confusionMatrix_hr_ar <- caret::confusionMatrix(
  data = comparison_hr_ar$ar_emotion,
  reference = comparison_hr_ar$hr_emotion
)

confusionMatrix_hr_ar_freq <- confusionMatrix_hr_ar$table %>% 
  as.data.frame() %>% 
  dplyr::rename(ar_emotion = Prediction, hr_emotion = Reference) %>% 
  dplyr::mutate(n_video = sum(Freq)) %>% 
  dplyr::mutate(prop_video = Freq/sum(Freq)) %>% 
  dplyr::group_by(hr_emotion) %>% 
  dplyr::mutate(n_hr = sum(Freq)) %>%
  dplyr::mutate(prop_hr = Freq/sum(Freq)) %>% 
  dplyr::ungroup()

confusionMatrix_hr_ar_freq %>% 
  ggplot(mapping = aes(x = hr_emotion, y = ar_emotion)) +
  geom_tile(aes(fill = prop_hr), colour = "white") +
  geom_text(
    aes(label = paste0(scales::percent(prop_hr), "\n(", Freq,"/",n_hr, ")")),
    color = "black",
    size = 4,
    family="serif",
    parse = FALSE,
    lineheight = 0.7)+
  scale_fill_gradient(
    name = "Proportion Recognized Emotion",
    low = "white", 
    high = "red", 
    labels = scales::percent,
    limits = c(0,1)
  ) +
  scale_x_discrete(name = "Human Observers") +
  scale_y_discrete(name = "Automatic Classifier") +
  theme_minimal() +
  theme(text = element_text(size=16,family="serif"),
        axis.text.x = element_text(size=16, angle = 45, hjust = 0.75,vjust=0.9),
        axis.text.y = element_text(size=16, hjust = 0.5),
        axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0)),
        legend.text = element_text(size=8),
        legend.position = "bottom")
```

The overall agreement between human observers and the automatic classifier is in fact very low (Kappa = `r round(confusionMatrix_hr_ar$overall["Kappa"],2)`). Except for *happiness* and *disgust* (respectively `r scales::percent(as.numeric(confusionMatrix_hr_ar_freq[confusionMatrix_hr_ar_freq$ar_emotion == "happiness" & confusionMatrix_hr_ar_freq$hr_emotion == "happiness", "prop_hr"]))` and `r scales::percent(as.numeric(confusionMatrix_hr_ar_freq[confusionMatrix_hr_ar_freq$ar_emotion == "disgust" & confusionMatrix_hr_ar_freq$hr_emotion == "disgust", "prop_hr"]))` of common labelling), there is no clear common pattern. Moreover, the automatic classifier has a tendency to label as *disgust* videos labeled as *sadness* by human observers and as *happiness* videos labeled as *fear* by human observers 

IV. Discussion and Conclusion
============

Despite being one on the most investigated question in affective science, the coherence between emotion felt and facially displayed and facial expression recognized is a hot topic. To date no clear evidence has been found to definitely answer it. Yet, with the growing interest of industries and government to monitor individual’s psychological states, this issue is under high pressure. The present research aims to provide some empirical data to the question. The faces encoders displayed when confronted with an emotional eliciting task were submitted both to human and to automatic recognition. The criterion for recognition accuracy was the subjective feeling self-reported by the encoder once the elicitation task carried out. Results first reveal a low coherence between emotion felt and facial expression displayed. Secondly, results show a low accuracy for both humans and automatic classifier in identifying the inner emotional states of these encoders based on their facial expressions. Thirdly, human observers prove to be better at recognizing the emotion facially expressed than the automatic recognition tool is. 

Such results support the hypothesis advanced by some authors of a low emotion–expression coherence [*e.g.*, @kappas2003facial]. In many instances, facial displays are not associated with a concordant emotional state, even any emotional state at all [@bonanno2004brief; @fernandez2013emotion]. More and more evidences are showing that facial expressions are in reality not expressing emotions [@mckeown2013analogical]. As well as other nonverbal behaviors, facial movements are not only assumed to be determined by emotion but also by various other causes, such as psychological states (*e.g.*, \nolinebreak motivations or pain), to say nothing of social context and sociocultural norms [such as “display rules”, @ekman1987universals]. This multiple determination excludes any possibility of drawing an inference from facial activity on the underlying psychological state (emotional or other).

Beyond the present observations showing a weak coherence between subjective feelings and spontaneous facial expressions, this study provides some light on the controversy between the behaviorist and the constructivist approaches. The first of those two lines of thinking [@ekman1992argument] assumes that expressions of emotion are brief and coherent patterns of facial muscle movements that co-vary with discrete subjective experiences. Emotions and their related prototypical facial behavior are universal because they are considered as innate mechanisms allowing individuals to respond adaptively to evolutionary significant events (threats, opportunities…) encountered in the environment. Instead of viewing emotions as natural kinds [@barrett2017theory], the constructivist approach supposes that emotions are social constructions. The specific emotion categories identified on the face of another person are the result of categorization. In other words, the emotions that are recognized by the observer are constructed in her/his mind, primarily based on her/his conceptual knowledge of emotion. Therefore, facial movements do not express specific emotions. It is the observer that infers the emotional meaning of the facial expression. As a consequence, one can predict from the first line of thinking that individuals’ emotional subjective feeling should be correlated to the recognition of facial expressions from both human observers and automatic classifiers whereas if emotions are social constructs, as stated by the second line of thinking, human observer’s should be better at perceiving emotions in face than automatic classifiers. 

Present results plead in favor of the latter stance. They show that human observers are more accurate than the automatic recognition tool to identify individuals’ subjective feeling on the basis of their face. Moreover, mistakes made by human observers look less arbitrary to the ones made by the classifier. For instance, even if mix-up between disgust and anger is sometimes reported in recognition studies, confusions such as the ones produced by the classifier have never been noted for human observers. A possible explanation is that human observers are assessing dynamic expressive sequences. They have access to the full video which provides a kinetic context whereas the automatic classifier only assesses the videos frame by frame. Further research is needed to shed light on this issue.

Some limitations should be stated, notably regarding the use of self-reports to evaluate encoders’ subjective feelings. Accessing the inner subjective feeling can be biased if not impossible. Moreover the procedure used for human observation can also be open to dispute. Instead of asking the human annotators to provide a unique label, a more subtle approach was chosen to mimic results provided by the automatic classifier. Whereas this paradigm is longer and more complicated, it can lead to more robust results in reducing the forced-choice bais [@russell1993forced]. However this procedure can also reduce the human observers accuracy. In this regard, the results of the human observation could have been more ambiguous because it is not the natural way that people are inferring meaning from facial expressions. An alternative explanation relies in reducing the recognition bais involved in the classic recognition paradigm. Classic forced-choice paradigms obtain artificial high results, thus by using a more evolved approach observers’ accuracy may have been lowered.

Considering the above, the results provide additional evidence that individuals’ subjective feeling can not be inferred from facial expressions and in our case invalidate the hypothesis of hardwired emotions unambiguously displayed on the face. Even if emotions were hardwired, in everyday life one does not observe prototypical facial expressions and therefore research should be focused on analysing non-prototypical facial expressions. Advancements in identifying “non-basic” emotion labels [@mcduff2016discovering] as well as non-prototypical facial expression have been made in the developpement of automatic facial expression recognition tools. However this result suggests that automatic facial expression recognition tools should merely evaluate facial morphology features such as action units [*e.g.*, OpenFace presented in @baltruvsaitis2016openface] rather than inferring supposedly emotional or affective states.

V. Acknowledgment {#acknowledgment}
==============

The authors would like to thank Brigitte Meillon and Jean Michel Adam who developed the software used to collect and preprocess human observer results.

VI. References {#references .numbered}
==========
