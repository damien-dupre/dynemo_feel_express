---
title             : "The emotion–facial expression link: Evidence from human and automatic expression recognition"
shorttitle        : "The emotion facial expression link"

author: 
  - name          : "Anna Tcherkassof"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Bâtiment Michel Dubois, Université Grenoble Alpes, 1251 Avenue Centrale, 38400 Saint-Martin-d'Hères"
    email         : "anna.tcherkassof@univ-grenoble-alpes.fr"
  - name          : "Damien Dupré"
    affiliation   : "2"

affiliation:
  - id            : "1"
    institution   : "University Grenoble Alpes, Grenoble, France"
  - id            : "2"
    institution   : "Dublin City University, Dublin, Ireland"

abstract: |
  While it has been taken for granted in the development of several automatic facial expression recognition tools, the question of the coherence between subjective feelings and facial expressions is still a subject of debate. On one hand, the "Basic Emotion View" conceives emotions as genetically hardwired and therefore being genuinely displayed through facial expressions. On the other hand, the constructivist approach conceives emotions as socially constructed; the emotional meaning of a facial expression being inferred by the observer. In order to evaluate the coherence between the subjective feeling of emotions and their recognition based on facial expression, 232 videos of encoders recruited to carry out an emotion elicitation task were annotated by 1383 human observers as well as by an automatic facial expression classifier. Results show low accuracy of human observers and of the automatic classifier to infer the subjective feeling from the facial expressions displayed by encoders. They also show a weak consistency between self-reported emotional states and facial emotional displays. Based on these results, the hypothesis of genetically hardwired emotion genuinely displayed is difficult to support, whereas the idea of emotion and facial expression as being socially constructed appears to be more likely. Accordingly, automatic emotion recognition tools based on facial expressions should be questioned.
  
keywords          : "emotion, facial expression, self-report, human observer, automatic recognition."
wordcount         : "7943"

bibliography      : ["bib/manuscript_ref.bib"]
csl               : ["csl/apa7.csl"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa7"
classoption       : "man"

output: papaja::apa6_word

# output            :
#   bookdown::pdf_book:
#     toc: false
#     number_sections: false
#     base_format: papaja::apa6_pdf

header-includes:
  - \usepackage{booktabs}
  - \usepackage{float}
  - \usepackage{tabu}
  - \usepackage{wrapfig}
  - \usepackage[none]{hyphenat}
  - \floatplacement{figure}{H}
  - \usepackage{flushend}
  - \usepackage{biblatex}
---
(ref:confusionMatrix-sr-hr) Confusion matrix between the emotion self-reported as being characteristic of the elicitation with the emotion recognized by the human observers.
(ref:confusionMatrix-sr-ar) Confusion matrix of between the emotion self-reported as being characteristic of the elicitation with the emotion recognized by the automatic classifier.
(ref:confusionMatrix-hr-ar) Proportion of emotion labels classified by human observers which are recognized by the automatic classifier.
(ref:confusionTable-sr-hr) Human recognition accuracy metrics for each emotion.
(ref:confusionTable-sr-ar) Automatic recognition accuracy metrics for each emotion.

```{r setup, include = FALSE}
library(tidyverse) # Data wrangling & grammar of graphics
library(here) # Locate files relative to project root
library(papaja) # Manuscript design and results
library(kableExtra) # Tables
library(caret) # Confusion matrices
```

```{r analysis-preferences}
set.seed(123) # Seed for random number generation
options(scipen = 999)# Disable scientific number format
knitr::opts_chunk$set(
  cache.extra = knitr::rand_seed,
  cache = FALSE,
  message = FALSE, 
  warning = FALSE, 
  error = FALSE, 
  echo = FALSE
  )
```

```{r data}
## self_report_data
source(here::here("script/data_wrangling_self_report.R"))
self_report_score <- self_report_score %>%
  dplyr::select(C_Video, sr_emotion = emotion, sr_score = value)

## human_recognition_data
source(here::here("script/data_wrangling_human_recognition.R"))
human_recognition_score <- human_recognition_score %>%
  dplyr::select(C_Video, hr_emotion = emotion, hr_score = confidence_score)

## automatic_recognition_data
source(here::here("script/data_wrangling_automatic_recognition.R"))
automatic_recognition_score <- automatic_recognition_score %>%
  dplyr::select(C_Video, ar_emotion = emotion, ar_score = confidence_score) %>%
  dplyr::filter(C_Video %in% ppt_per_video$C_Video)
```

# Introduction

With the development of commercial automatic facial expression recognition tools [@dupre2018accuracy; @dupre2020performance], industries and governments are gradually implementing this technology in order to monitor humans’ emotions in various scenarios (*e.g.*, in marketing, healthcare, and the automotive industry to name a few). This technology rests on the premise that facial expressions provide direct access to individuals’ subjective feelings, and that one can read the emotions displayed on a person’s face as one would an open book. This premise is derived from the "Basic Emotion View", a theory which suggests a one-to-one mapping between subjective feeling and facial expression [@ekman1987universals; @ekman1988universality; @ekman1992argument; @ekman2007directed]. Even if this premise is central to the modern mainstream approach of human emotion, recent research in affective science is challenging it. Once the Basic Emotion View is briefly described, its foremost criticisms will be exposed. Inconsistencies and unpredicted findings come both from field observations and laboratory experiments. They relate to two different issues. The first issue is the sender’s production of spontaneous facial displays. The second issue relates to the receiver’s interpretation of these facial displays. Building on this evidence, and as an attempt to shed further light on the emotion–face link, the present study's main goal is to tackle these two issues at the same time. It aims at identifying the extent to which ordinary people’s experienced emotions are displayed through identifiable Emotional Facial Expressions (EFE) recognized as such both by human individuals, and by automatic facial expression recognition tools.

## The Basic Emotion View

The belief that facial expression is linked to emotional states can be traced back to Darwin in *The Expression of the Emotions in Man and Animals* [@darwin1872expression]. Countless studies have fortified the idea that emotional states are inherently coupled to a set of prototypic facial expressions [*e.g.*, @ekman2011meant]. The Basic Emotion View holds that facial expressions are genuine displays of an individual’s inner emotional state. More specifically, a set of six emotions (*fear*, *anger*, *surprise*, *disgust*, *sadness* and *joy*) are universally displayed and are genetically hardwired not only in humans [@ekman1992argument], but also in different animal species [@de2019mama]. According to the Basic Emotion View, *“when emotions are aroused by perception of a social event, a set of central commands produce patterned emotion-specific changes in multiple systems, including […] facial expressions.”* [@ekman2007directed, 49]. To respond to criticisms, several amendments have been made to the Basic Emotion View, increasing the number of basic emotions from six to seven [@ekman1988universality] as well as adding the concept of “display rules” to explain cultural differences in the management of facial expressions [@ekman1987universals]. 

Relatively few studies have actually tested Basic Emotion View’s fundamental claim regarding the facial production (so-called "expression") of emotions. @rosenberg1994coherence reported the first evidence of coherence between self-report of emotion and displayed facial expressions. Participants were shown emotionally evocative films and reported their own emotions using a moment-by-moment reporting procedure. Analysis of participants’ facial expressions and reports of emotions showed that there was a high degree of temporal linkage and emotional agreement between facial expressions and self-reports. Notwithstanding this study, few have used purely descriptive methods such as electromyography or objective face coding systems to identify and to measure the actual changes in the face when a given emotion is felt [see @wagner1997methods, for methodological points]. Much of the research has focused instead on the recognition of EFE, that is on the issue of the viewer’s interpretation of facial displays. As people have been thought to display their internal state, EFE supposedly serve as a window into the emotions of others. Viewers “recognize” the facial displays and infer the corresponding emotional state. The Readout Hypothesis [@buck1985prime] formalizes this folk theory. According to it, facial expressions have evolved to provide *“an external readout of those motivational-emotional processes that have had social implications during the course of evolution”* [@buck1985prime, 396-397]. Thus, as readouts of emotional states, spontaneous expressive displays are directly accessible to other organisms. Research on the inference of emotions from facial expressions has established that viewers show considerable agreement that the so-called basic emotions—happiness, anger, fear, disgust, sadness, and surprise—are associated with specific facial displays [*e.g.*, reviews from @elfenbein2002universality; @russell1994there]. 

Yet, in spite of the popular support it has received, the empirical data called upon by the Basic Emotion View remains unpersuasive. Notably, it fails to explain, in instances in which display rules cannot be called upon, how individuals can feel emotions without expressing them or how individuals can express emotions without feeling them, and why observers can make mistakes in recognizing facial expressions of basic emotions, among others. The following  is a summary of key findings and conclusions both from field observations and laboratory experiments on spontaneous expression of emotions, all of them strongly subverting the Basic Emotion viewpoint [for reviews, @fernandez2013emotion; @reisenzein2013coherence; @russell2003facial]. Mains issues regarding recognition studies are also briefly discussed afterwards. 

## Spontaneous Facial Expression in naturalistic studies

Naturalistic studies look at the ecological frequency of co-occurrence of certain emotions and facial displays [@fernandez2013emotion]. Though they cannot be considered as a straightforward test of the triggering role of emotion on facial behavior, they have the advantage of considering situations that cannot be created in a laboratory. For instance, the ethological study of @kraut1979social contradicts the Basic Emotion View premise that a smile is only the major component of a facial display associated with and caused by feelings of joy or happiness. Naturalistic observation at a bowling alley showed that bowlers do not necessarily smile after scoring a spare or a strike (a situation likely to elicit a positive emotion). Rather, they often smile when interacting with other people. More generally, examination of bowlers’ facial display showed that they rarely smiled while facing the pins but often smiled when facing their friends. These findings were confirmed by @ruiz2003spontaneous who analyzed the facial displays of bowlers after a strike and soccer fans after their team scored. Their results show a low probability of smiling when participants were not interacting with someone else. These findings have been supported in other realistic field settings such as the one of @fernandez1995smiles suggesting that happiness is not a sufficient cause of smiling. @fernandez1995smiles watched extremely happy gold medalist athletes displaying facial expressions of sadness (sometimes associated with tears) during their Olympics awards ceremonies. More specifically, winners showed Duchenne smiles and other types of smiles when they were interacting with other people during the awards ceremony, but smiles were scarce or nonexistent when waiting behind the podium and/or when turning toward the flagpoles and focusing their attention on the flags and the national anthem. @crivelli2015smiles also found that the strongest predictor for the occurrence of Duchenne smiles in judo winners is when they are engaged in social interaction and not when they just won their match.

Happiness/joy is not the only emotion weakly associated with the predicted facial expression. A naturalistic study conducted by @scherer1997lost in an airport’s baggage handling office showed that passengers claiming for their lost luggage displayed very few facial expressions of negative emotions while self-reporting subjective feeling states of anger or sadness among others. The covariation between passengers’ self-ratings and the claims agents’ attributions of the passengers’ emotions was very low. Another refutation of the Basic Emotion View’s predictions regarding the link between emotions and facial expressions is provided by naturalistic observations of infants’ productions of facial expressions. It is often acknowledged that adults regulate their expressive behavior. In accordance with various display rules (personal, social, cultural ones), they exert a control over the supposed automatic readouts of their emotions. Developmental studies provide relevant settings in which display rules are inoperative. Camras and her colleagues observed that facial expressions of negative emotions were displayed in circumstances that were unlikely to have elicited those emotions. For instance, infants displayed "fear" expressions in settings not related to fear [@camras1991development]. @bennett2002facial videotaped a sample of 4-month-old infants during tickle, sour taste, jack-in-the-box, arm restraint, and masked stranger situations. Infants displayed a variety of facial expressions in each eliciting situation. Yet, more infants exhibited positive than negative facial expressions across all situations –except sour taste. No evidence for emotion-specific facial expressions corresponding to anger, fear, and sadness was obtained. Camras and her colleagues also observed that 11-month-old European-American, Chinese, and Japanese infants did not display distinct negative emotion-specific patterns of facial muscles in response to two elicitors meant to induce fear and anger [@camras2007infants]. Thus, these findings in naturalistic settings provide little support for the one-to-one mapping of subjective feeling and facial expression.

## Spontaneous Facial Expression in laboratory studies

Laboratory findings also support field studies. As stressed by @reisenzein2013coherence, experimental studies permit both better control of various factors (*e.g.*, emotion elicitors) and tests about likely moderators of the emotion–facial expression link (*e.g.*, the social context). As an example, a strong disconfirmation of the Basic Emotion View’s premise is put forward by Reisenzein’s studies on surprise [*e.g.*, @reisenzein2000exploring]. In eight controlled laboratory situations, surprise was induced by @reisenzein2006evidence by establishing and then invalidating a set of beliefs concerning the experimental events such as the unexpected appearance of a picture of one's own face as the last picture in a series of portraits that had to be rated. Visible or electromyography-detected facial displays of surprise occurred only in few participants. Yet, most participants reported subjective feelings of surprise and most believed that they had shown a strong surprise facial expression. @schutzwohl2012facial also observed similarly low frequencies of surprise facial expressions when their participants, after leaving the laboratory, unexpectedly found themselves not in the corridor but in a new room with green walls and a red office chair. Less than a quarter of the participants displayed an expression of surprise: only 5% showed widened eyes, raised eyebrows, and opened mouth which correspond to the complete expression of surprise according to the Basic Emotion View and 17%  showed widened eyes and raised eyebrows. Again, participants overestimated their surprise expressivity.

Studies on happiness and related positive emotions such as sensory pleasantness also show a low coherence between emotion and facial display [@duran2017coherence; @reisenzein2013coherence]. In fact, only experiments on amusement provide a fairly strong association between emotion and smiling. When confronted to humorous events (*e.g.*, jokes, being tickled), the number of participants smiling and laughing is indeed quite fair, whereas when confronted to other positive emotions (*e.g.*, happiness), few participants show the expected facial expressions [for instance, the Duchenne smile or any kind of expression related to happiness; see @mehu2007smiles; @lee2002effect]. More largely, the meta-analysis conducted by @duran2017coherence on the degree of statistical covariation between emotions and facial expressions provides conclusive evidence. First of all, the basic emotions are weakly correlated with the specific configuration of facial muscles that the Basic Emotion View ascribes to them (the correlation drops when happiness/amusement studies are excluded). Very few participants who relived an experience of sadness were reactive, only a small number of them showing components of a sad facial expression with oblique eyebrows or lip corners pulled down [@tsai2002emotional]. Regarding fear, about one third of spider phobic participants exposed to a live tarantula displayed some components of the prototypical fear expression such as eye widening, brow raising and knitting [@vernon2002disgust]. Similar observation is made for anger. The proportion of facially reactive participants displaying at least one component among frowning or lid/lip tightening and reporting anger does not exceed 35% [@johnson2010smile; @tsai2002emotional]. The same proportion of reactive participants confronted to disgusting elicitors is observed [*e.g.*, @ekman1980facial; @fernandez1997spontaneous]. In any case neither an insufficient intensity of the emotion to cause a facial expression nor the intervention of display rules (nor measurement issues) can explain the low emotion–facial expression consistency.

Hence, laboratory studies show that facial expressions of emotion are often not displayed in situations in which the Basic Emotion View would predict them to be expressed. Moreover, when corresponding facial expressions are indeed displayed, they are only partially displayed. All in all, research on the spontaneous expression of emotions does not yield strong support for the Basic Emotion View. The available evidence steadily indicates weak links between emotions and their predicted facial expressions both in natural and in semi-naturalistic settings. 

## Emotional Facial Expression Recognition studies

The Basic Emotion View postulates that, when triggered, each basic emotion is expressed by a prototypical face (non-basic emotions being blends of the basic ones). In return, the recognition of EFE is claimed to be based on the identification of specific patterns of facial movements associated with each emotion, as if expression and recognition were the two sides of the same coin. Seemingly compelling evidence supports this claim, sustaining the possibility of a clear readout of subjective feelings from facial expressions. Furthermore, recognition systems rely on such a principle and are considered as an objective coding tool because they are based on the identification of specific muscular changes in the face. Many people strongly agree that so-called basic emotions are associated with specific facial configurations [@ekman2017facial; @elfenbein2002universality] and also argue that this is strong evidence for the Basic Emotion View. Moreover, it implies that EFE recognition of human observers should be as accurate as automatic classifiers'.

Yet, some researchers have highlighted the limitations of Basic Emotion View empirical research. Among others, evidence has been questioned on methodological grounds [*e.g.*, @russell1994there]. The response format usually used in recognition studies (*i.e*, forced choice: selection of one word from a pre-specified list of emotion labels), notably, leads to a biased consensus[@russell1993forced]. Depending on the list of emotion labels at participants’ disposal, EFE of sadness can easily be categorized as sad expressions as well as fear expressions, as one example. @russell1987relativity also shown that the same facial expression can be seen as expressing different types of emotions, depending on what other faces are seen. @digirolamo2017emotion conducted seven experiments that establish that high agreement between participants can be an artifact of the standard method used by EFE recognition studies. Thus, results gathered with forced choice cannot demonstrate the unequivocal link between emotion and facial expressions claimed by the Basic Emotion View. Using alternative recognition methods [emotion satiation procedure, face-matching task, sorting task; *e.g.*, @lindquist2006language; @gendron2014perceptions], it has been shown on the contrary that facial muscle movements are not linked in a one-to-one manner to a specific discrete emotional experience. Instead, emotions are probably mentally constructed by the perceiver and mental categories of emotions are needed to accurately categorize facial movements among contextual information. 

To the methodological limitations contaminating the hundreds of studies apparently supporting the Basic Emotion View, a stimulus bias must be added. Facial stimuli used in experiments also constitute a methodological bias because they are unrepresentative of ordinary facial expressions. Basic Emotion View empirical evidence is based for the most part on methods using a static and unnatural material, namely, still photographs of posed facial expressions of emotion (*e.g.*, intentionally encoded by the sender). This kind of methodology raises questions about its ecological validity and the generalizability of the results to real interpersonal emotional communication [*e.g.*, @tcherkassof2007facial]. Indeed, a number of pieces of evidence indicate that research cannot content itself with data collected with static and posed material. These data come from researches studying the case of dynamic and/or spontaneous facial expressions of emotion. They show that the dynamic aspects of facial movement are likely to be of importance [*e.g.*, @kamachi2013dynamic]. @cohn2003timing have shown that spontaneous smiles are of smaller amplitude and have a more consistent relation between amplitude and duration than deliberate smiles. @hess1990differentiating have also pointed out the importance of the dynamics of facial movements, and particularly the irregularity, or phasic changes, of the expressions’ unfolding. Thus, the motion of facial expression provides observers with other information than the one provided by static expressions. It may be that differences in the social information displayed by static and dynamic expressions leads to facial recognition differential effects. Regarding the issue of spontaneous *vs.* posed expressions (the latter are overused in experiments), as @meillon2010dynemo conclude, EFE have been typically studied as static displays. As a consequence, even though the central role of the dynamics of facial expressions is endorsed, little is still known about the temporal course of facial expressions. Furthermore, studied EFE exhibit emotions simulated or posed by actors. Yet, the lack of spontaneity and naturalness of this material constitutes a serious objection raised against such studies [@kanade2000comprehensive].

Finally, as many doubts can be raised about the standard method, experiments conducted with such a method cannot be considered as providing solid empirical support to the Basic Emotion View. Based on the numerous methodological criticisms, but also theoretical, addressed to this view, alternative conceptions have emerged. Among them, the constructivist approach is gaining in importance. The constructivist approach represents a different way of understanding the emotion–facial expression link. It affirms that facial expressions do not provide a direct access to individuals’ subjective feelings. Therefore, instead of considering that emotions can be “read” on facial displays, it claims that the emotion is “in the eye” of the perceiver [@barrett2019emotional]. 

## Constructivist Approach

Starting from the empirical evidence suggesting that spontaneous facial expressions in ordinary life are equivocal, @dols2017natural argues in favor of a pragmatic conception of natural facial displays. He makes a plea for the idea that natural facial displays, rather than "saying"--because facial expressions do not have a specific meaning--“make” things. Facial expressions are actions in a communicative interaction. They do not express emotions but they *"prompt, on the receiver’s side, important inferences about the context, the sender, and the course of the interaction between sender and receiver"* [@dols2017natural, 466]. As such, the fact that facial displays are able to signal emotions is a byproduct of one of their main functions; implementing actions performing practical ends. Therefore, expressive displays hold a motor intention [@pacherie2003modes]. They implement their aim which is their motor intention. They are not primarily communicative signals, and even less so can they be considered the outlet of an internal state. Facial displays are parts of pragmatic actions aiming at orienting the person’s relation to their environment. For instance facial displays are maintaining, breaking or restoring the relationship between sender and perceiver [@frijda2012recognition]. Facial displays are not recognized in semantic terms but are perceived as intentional actions. In the face of the continual flow of uninterrupted facial movements, perceivers see behaviors directed towards a goal. They translate the continuous flow of movements into coordinated sequences of actions holding a beginning and an end. Facial displays are not simple strings of action units, the morphological configuration of which would be the prototype of a given emotion, and consequently identified as such [@ekman1987universals]. They are best conceived as a Gestalt, the same way as a string of musical notes establishes a melody [@tcherkassof2014emotions]. This is why even inauthentic facial displays can still be recognized as emotional expressions. Duchenne de Boulogne explains that the artist who has shaped the famous Laocoon antic sculpture, exhibited in the Vatican’s museum, has made a modelling mistake since no face can display its emotional expression [@duchenne1876mecanisme]. Indeed, no muscular contraction can produce it. He even rectifies the "mistake" by presenting a statue which face is shaped according to the physiology of facial expressive movements. His demonstration gives food for thought. Even though no objective coding system can correctly code the discordant facial features of the Laocoon’s face, anyone can easily recognize the suffering and despair he expresses. This example is aligned with the constructivist approach.

The constructivist approach claims that facial displays are behaviors whose meaning is inferred by perceivers. Findings support this observer dependence [@lindquist2013s; niedenthal2017embodied]. They show that to make meaning of another person’s facial behavior, the perceiver relies in particular on her/his knowledge about emotion categories. For instance, Gendron and her colleagues used a face-sorting task allowing them to manipulate the influence of emotion concepts on how facial expressions were perceived. They conducted their experiment among U.S. participants and Himba participants from remote regions of Namibia [@gendron2014perceptions] and Hazda participants of the Eastern Rift Valley of Tanzania [@gendron2018emotion] both groups with limited exposure to Western culture. Gendron and her colleagues demonstrated that facial expressions were not universally recognized in discrete emotional terms. Indeed, when Himba and Hazda participants did not have emotion concepts at their disposal to structure perception, they perceived the facial expressions as behaviors, such as looking or smelling, that didn’t have a necessary relationship to emotions. They did not infer inner states, rather they proceeded with action identification that pointed out the functions of behaviors [see also @crivelli2017recognizing for similar observations among a small-scale society of Papua New Guinea]. The constructivist approach considers that specific emotion categories, as conceptualized by Western cultures’ knowledge, are cast on the perceived face to make meaning of the sender’s facial displays. Following this approach, faces convey a range of information essential for social communication. They are best conceived as tools displaying signals in social interactions [@crivelli2018facial]. These signals can convey individuals’ motivations and readiness [@frijda1997facial] or social messages [@fridlund1994human]. As for emotional meaning, more specifically, this is shaped by the perceiver according to the specific context in which the facial displays are observed.

Having reached this point, one can assert that numerous questions regarding the link between emotions and facial expressions remain unanswered. The two main competing approaches to facial expressiveness, the Basic Emotion View and the constructivist approach, entail completely opposite predictions regarding the decoding of facial expressions, as evidenced above. The present study aims to examine these predictions in order to provide empirical evidence to allow the discussion to evolve. To date, no systematic study has looked at facial expressions spontaneously displayed in reaction to emotional triggers and how they are decoded, both by human observers and by automatic emotion recognition tools based on the detection of facial muscular configurations. This study fills that gap. It intends to investigate the consistency between the subjective feeling of emotions and its recognition from facial expressions. Spontaneous and dynamic facial reactions to emotional elicitations are under consideration to ensure the generalizability of the results to emotional behaviors in ordinary life. More specifically, this study aims to examine the recognition of EFE produced by ordinary people during situations judged and/or self-reported to involve different emotions. It (a) examines consistency between ordinary people’s self-reported emotional experience and observers’ judgments of these ordinary people’s EFE, and (b) examines consistency between ordinary people’s self-reported emotional experience and an automatic classifier’s analysis of these ordinary people’s EFE. In other words, it is interested in how people actually move their faces to express self-reported emotions, in how human observers accurately infer the expresser’s emotional state, and in how automatic recognition accurately codes the expresser’s emotional state. Because of their superior ability to exactly recognize EFE, it is expected that human observers’ will have a higher accuracy to identify senders' subjective feeling than automatic EFE recognition tools.

# Methods

To evaluate the consistency between subjective feeling of emotions and their recognition from facial expressions, encoders were first recruited to perform an emotion elicitation task while their facial expression was video recorded. In order to reduce the likelihood of facial control, the encoders were alone in the room and were filmed by hidden cameras, so they had no reason to comply with social display rules. Then, the videos of the encoders’ faces were shown to human observers and were also analyzed by an automatic classifier in order to identify which emotion was displayed.

## Emotion Elicitation

```{r self-report-method}
self_report_gender <- self_report_data %>% 
  dplyr::count(genre_c) %>% 
  dplyr::pull(n, genre_c)

self_report_age <- self_report_data %>%
  dplyr::summarise(
    age_m = mean(age) %>% round(1), 
    age_sd = sd(age) %>% round(1)
  )
```

For the emotion elicitation experiment, `r nrow(self_report_data)` encoding participants (`r self_report_gender["F"]` females, `r self_report_gender["H"]` males, *M*~age~ = `r self_report_age["age_m"]`, *SD*~age~ = `r self_report_age["age_sd"]`) were recruited to perform one out of 11 emotion elicitation tasks designed to trigger a positive, a specific negative or a neutral emotional state. These participants were recruited by a private company for a study supposedly devoted to an "ergonomic visual task" (cover story). After a description of the general aims, participants agreed and signed the experiment consent form. At the end of the elicitation task, they received an equivalent of €50 in voucher for their participation. A second consent form was signed by the participants to allow their video to be processed for research purposes. 

Two different types of task were used to elicit emotions. Some tasks were passive and consisted in the participant watching videos selected to trigger emotions (e.g. television commercials). Other tasks were active; they required the participant to interact with a computer (e.g. to answer questions or to test a flawed software). Encoders’ faces were recorded using a hidden camera resulting `r nrow(self_report_data)` front facing 768x576 videos varying from `r min(metadata_video$ffprobe_duration)`s to `r max(metadata_video$ffprobe_duration)`s. These recordings form the DynEmo database [@tcherkassof2013dynemo].

After the emotion elicitation task, the encoders rated their subjective feeling on Likert scales from 0 ("not at all") to 5 ("strongly") related to six "basic" emotion labels (*i.e.*, *anger*, *disgust*, *fear*, *happiness*, *surprise* and *sadness*) as well as six "non-basic" emotion labels (*i.e.*, *pride*, *curiosity*, *boredom*, *shame*, *humiliation*, and *disappointment*).

Finally, a debriefing session was performed to ensure that encoders were not durably affected by the emotion elicitation task. The debriefing was also used to check that encoders did not guess the real purpose of the experiment (*e.g.*, being filmed while they were performing an emotional elicitation task) to guarantee facial expressions’ genuineness. All encoders gave their agreement on their data and recordings being processed for research purposes only.

## Human Facial Expression Recognition

```{r human-recognition-method}
unique_ppt_count <- human_recognition_data %>% 
  dplyr::count(source, SEXE_Juge, C_Juge)

unique_video_count <- human_recognition_data %>% 
  dplyr::distinct(source, SEXE_Juge, C_Juge, C_Video) %>% 
  dplyr::count(C_Video) %>% 
  dplyr::summarise(
    vid_n = n(),
    vid_m = mean(n) %>% round(0), 
    vid_sd = sd(n) %>% round(0)
  )
```

To analyse how the recorded facial expressions are perceived, `r nrow(unique_ppt_count)` participants were recruited among social science under- and post-graduates through advertising to watch and to annotate these recordings. They received a course credit for their participation. After a description of the general aims, participants agreed and signed the experiment consent form. An iterative procedure was setup to insure that each recording has been annotated by at least 20 participants: 10% of the video database were randomly selected and annotated until all the videos reached the amount of annotation required, then another section of the database is randomly selected among the remaining recordings to be annotated. To avoid potential decrease in the quality of the annotations due to cognitive fatigue, each participant only annotated recordings during a maximum of 30min. As a results only `r unique_video_count["vid_n"]` out of the `r nrow(self_report_data)` videos have been annotated. Each video was annotated `r unique_video_count["vid_m"]` times on average (*SD* = `r unique_video_count["vid_sd"]`).

The annotation of facial expressions was performed on-site using *Oudjat*, a software for designing video annotation experiments [@dupre2015oudjat]. For each video, the annotation procedure followed two steps. First, the participants had to identify the emotional sequences by pressing the space bar of their keyboard to indicate the beginning and the end of the emotional sequences while watching the video. Second, the participants watched each emotional sequence previously identified and labeled the sequence using one of the 12 emotions proposed including six basic emotion labels (*i.e.*, *anger*, *disgust*, *fear*, *happiness*, *surprise* and *sadness*) and six "non-basic" emotion labels (*i.e.*, *pride*, *curiosity*, *boredom*, *shame*, *humiliation*, and *disappointment*). They also had the possibility to indicate that the sequence was expressing none of the proposed emotion labels.

This annotation procedure resulted in a uni-dimensional time-series for each video per human observer, identifying for each second of the video which emotion was recognized. Then, time-series corresponding to the same video were aggregated to calculate the proportion of human observers $x_{video_{i}.label_{j}.t_{k}}$ for each second of the video per emotional label shown in Equation \@ref(eq:one).

\begin{equation}
x_{video_{i}.label_{j}.t_{k}} = \frac{n_{video_{i}.label_{j}.t_{k}}}{n_{video_{k}}}(\#eq:one)
\end{equation}

where *i* is one of the `r unique_video_count["vid_n"]` videos, *j* is one of the six “basic” emotion labels, *k* for each second of the video.

## Automatic Facial Expression Recognition

The `r unique_video_count["vid_n"]` annotated video were processed with Affdex (SDK v3.4.1). Affdex is an automatic facial expression recognition classifier developed and distributed by Affectiva, which is a spin-off company resulting from the research activities of MIT media lab created in 2009 [@mcduff2016affdex]. Affdex’s algorithm uses Histogram of Oriented Gradient (HOG) features and Support Vector Machine (SVM) classifiers in order to recognize facial expressions. For each video frame, Affdex identifies the probability $p_{video_{i}.label_{j}.t_{k}}$ of expressing each of the six basic emotion labels (*i.e.*, *anger*, *disgust*, *fear*, *happiness*, *surprise* and *sadness*) as well as additional psychological states such as *valence*, *engagement* or *contempt*, and facial features such as *cheek raise*, *eye widen* or *jaw drop*. The result of Affdex is a mutivariate timeseries output which provides for each label a probability from 0 to 100 (rescaled from 0 to 1 for the analysis) of expressing the corresponding label. For this study only the six basic emotion labels are analysed to match with self-reports and human observers.

For both human and automatic recognition, to determine which of the six basic emotions can be used to identify each video, the recognition probability for each label by frame was converted into odd ratio by label [@dente2017measures]. The highest sum of each odd ratio time-series defines the label recognized (see Equation \@ref(eq:two) for human recognition and Equation \@ref(eq:three) for automatic recognition). 

\begin{equation}
video_{i}.label = \max\left(\frac{\sum_{k=1}^{n}x_{video_{i}.label_{j}.t_{k}}}{\sum_{k=1}^{n}x_{video_{i}.t_{k}}}\right)(\#eq:two)
\end{equation}

\begin{equation}
video_{i}.label = \max\left(\frac{\sum_{k=1}^{f}p_{video_{i}.label_{j}.t_{k}}}{\sum_{k=1}^{f}p_{video_{i}.t_{k}}}\right)(\#eq:three)
\end{equation}

where *i* is one of the `r unique_video_count["vid_n"]` videos, *j* is one of the six “basic” emotion labels, *k* for each second of the *n* second video or for each frame of the *f* frame video.

Since encoders’ self-reports, human annotations and the automatic recognition include data on “non-basic” emotion labels and features, the analysis is performed using only the six basic emotion labels in order to compare them. The maximum rating for self-reports, human annotations and automatic recognition is used to label the video. In case of more than one label obtaining the maximum rating, the video is labeled as undetermined.

The confusion matrices obtained by crossing encoders’ self-reports, human annotations and the automatic recognition are analysed by calculating two-tailed t-tests of Pearson's product-moment correlation between dichotomous variables, accuracy which corresponds to the overall agreement rate averaged over cross-validation iterations and Cohen’s (unweighted) Kappa.

# Results

```{r corr-matrix}
list_video <- human_recognition_score$C_Video
list_emotion <- c("disgust","fear","happiness","surprise","sadness","anger")

grid_data <- expand.grid(list_video = list_video, list_emotion = list_emotion)

self_report_result <- self_report_score %>%
  dplyr::filter(sr_emotion != "undetermined") %>%
  dplyr::mutate(sr_score = 1)

human_recognition_result <- human_recognition_score %>%
  dplyr::filter(hr_emotion != "undetermined") %>%
  dplyr::mutate(hr_score = 1)

automatic_recognition_result <- automatic_recognition_score %>%
  dplyr::filter(ar_emotion != "undetermined") %>%
  dplyr::mutate(ar_score = 1)

recognition_result <- grid_data %>%
  dplyr::left_join(
    self_report_result,
    by = c(
      "list_video" = "C_Video",
      "list_emotion" = "sr_emotion"
      )
    ) %>%
  dplyr::left_join(
    human_recognition_result,
    by = c(
      "list_video" = "C_Video",
      "list_emotion" = "hr_emotion"
      )
    ) %>%
  dplyr::left_join(
    automatic_recognition_result,
    by = c(
      "list_video" = "C_Video",
      "list_emotion" = "ar_emotion"
      )
    ) %>%
  replace(is.na(.), 0)

corr_sr_hr <- 
  cor.test(
    recognition_result$sr_score, 
    recognition_result$hr_score,
    method = "pearson",
    alternative = "two.sided"
  ) %>% 
  papaja::apa_print()

corr_sr_ar <- 
  cor.test(
    recognition_result$sr_score, 
    recognition_result$ar_score,
    method = "pearson",
    alternative = "two.sided"
  ) %>% 
  papaja::apa_print()

corr_hr_ar <- 
  cor.test(
    recognition_result$hr_score, 
    recognition_result$ar_score, 
    method = "pearson",
    alternative = "two.sided"
  ) %>% 
  papaja::apa_print()
```

## Human Observers’ Accuracy

```{r sr-hr}
comparison_sr_hr <- self_report_score %>% 
  dplyr::inner_join(human_recognition_score, by = "C_Video") %>% 
  dplyr::ungroup() %>% 
  dplyr::filter(hr_emotion != "undetermined") %>% 
  dplyr::filter(sr_emotion != "undetermined") %>% 
  dplyr::mutate_if(is.character, as.factor)

confusionMatrix_sr_hr <- 
  caret::confusionMatrix(
    data = comparison_sr_hr$hr_emotion,
    reference = comparison_sr_hr$sr_emotion
  )
```

The overall correlation of recognition and non-recognition between self-reported emotions and human observers recognition is significant but low (`r corr_sr_hr$full_result`). In order to identify differences according to the emotional labels, encoders’ subjective feelings are compared with human observers’ recognition in a confusion matrix (Figure \@ref(fig:confusionMatrix-sr-hr)).

```{r confusionMatrix-sr-hr, fig.height=6, out.width ="\\textwidth", fig.pos = "!h", fig.cap="(ref:confusionMatrix-sr-hr)"}
confusionMatrix_sr_hr_freq <- confusionMatrix_sr_hr$table %>% 
  as.data.frame() %>% 
  dplyr::rename(hr_emotion = Prediction, sr_emotion = Reference) %>% 
  dplyr::mutate(n_video = sum(Freq)) %>% 
  dplyr::mutate(prop_video = Freq/sum(Freq)) %>% 
  dplyr::group_by(sr_emotion) %>% 
  dplyr::mutate(n_sr = sum(Freq)) %>%
  dplyr::mutate(prop_sr = Freq/sum(Freq)) %>% 
  dplyr::ungroup()

confusionMatrix_sr_hr_freq %>%
  ggplot(mapping = aes(x = sr_emotion, y = hr_emotion)) +
  geom_tile(aes(fill = prop_sr), colour = "white") +
  geom_text(
    aes(label = paste0(
      scales::percent(prop_sr, accuracy = 1), "\n(", Freq, "/", n_sr, ")"
    )),
    color = "black",
    size = 4,
    family = "serif",
    parse = FALSE,
    lineheight = 0.7
  ) +
  scale_fill_gradient(
    name = "Proportion of Recognized\nSelf-Reported Emotion",
    low = "white",
    high = "red",
    labels = scales::percent,
    limits = c(0, 1)
  ) +
  scale_x_discrete(name = "Self-Reports") +
  scale_y_discrete(name = "Human Observers") +
  theme_minimal() +
  theme(
    text = element_text(size = 16, family = "serif"),
    axis.text.x = element_text(size = 16, angle = 45, hjust = 0.75, vjust = 0.9),
    axis.text.y = element_text(size = 16, hjust = 0.5),
    axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0)),
    legend.text = element_text(size = 8),
    legend.position = "bottom"
  )
```

Each emotion label used to describe encoders’ self-reported subjective feeling (*i.e.*, the label rated with the highest value) is compared with the emotion labels which were rated with the highest score by human observers. Results of the confusion matrix show a low agreement between the emotion felt by the encoder during the elicitation and the emotion recognized by the human observers (Accuracy = `r round(confusionMatrix_sr_hr$overall["Accuracy"],2)`, 95% CI [`r round(confusionMatrix_sr_hr$overall["AccuracyLower"],2)`,`r round(confusionMatrix_sr_hr$overall["AccuracyUpper"],2)`]; Kappa = `r round(confusionMatrix_sr_hr$overall["Kappa"],2)`) except for *disgust* (`r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "disgust" & confusionMatrix_sr_hr_freq$sr_emotion == "disgust", "prop_sr"]))` of the videos self-reported). These results are far from those classically obtained in the literature for emotional facial expression recognition [@krumhuber2020human, report an average accuracy of 65%]. However these results are mostly obtained with posed facial expressions (*i.e.*, displayed by actors) in a forced-choice recognition paradigm.

Interestingly human observers seem to recognize surprise expressed in videos where anger, fear happiness and sadness was the highest self-reported emotion (respectively `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "surprise" & confusionMatrix_sr_hr_freq$sr_emotion == "anger", "prop_sr"]))`, `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "surprise" & confusionMatrix_sr_hr_freq$sr_emotion == "fear", "prop_sr"]))`, `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "surprise" & confusionMatrix_sr_hr_freq$sr_emotion == "happiness", "prop_sr"]))` and `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "surprise" & confusionMatrix_sr_hr_freq$sr_emotion == "sadness", "prop_sr"]))` of the videos self-reported), and in a lower instance *happiness* was recognized in videos where *fear* and *surprise* was the highest self-reported emotion (respectively `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "happiness" & confusionMatrix_sr_hr_freq$sr_emotion == "fear", "prop_sr"]))` and `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "happiness" & confusionMatrix_sr_hr_freq$sr_emotion == "surprise", "prop_sr"]))` of the videos self-reported).

Sensitivity, specificity, precision and F1 scores for each emotion reveals that *happiness* has the highest coherence ratio whereas *sadness* has the lowest coherence ratio between true positives and false positives (Table \@ref(tab:confusionTable-sr-hr)).

```{r confusionTable-sr-hr, results="asis"}
confusionMatrix_sr_hr_table <- confusionMatrix_sr_hr %>%
  magrittr::use_series(byClass) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("Emotion") %>%
  dplyr::mutate(Emotion = gsub(pattern = "Class: ", replacement = "", x = Emotion)) %>% 
  dplyr::select(Emotion, Sensitivity, Specificity, Precision, F1) %>% 
  dplyr::mutate_if(is.numeric, round,2) %>% 
  replace(is.na(.), "na.")

confusionMatrix_sr_hr_table %>%
  papaja::apa_table(
    caption = "(ref:confusionTable-sr-hr)",
    note = "na. values are produced when not enough data are available to compute accuracy indicators.",
    escape = TRUE
  )
```

Accuracy metrics by emotional labels indicate a discrepancy in the ratio of true/false positives. Whereas *happiness* and *disgust* obtain the highest scores, *anger*, *surprise* and *sadness* have the lowest recognition ratio. This difference between emotions is usually observed in the literature [e.g. @krumhuber2020human]. Multiple reasons can explain why *happiness* and *disgust* are more easily recognized than *anger*, *surprise* and *sadness*. It is possible that these expressions are usually more intense than others. In addition, *anger* and *sadness* as non-socially desirable emotions may be have been felt but not expressed.

However, self-reports show a significant proportion of undetermined emotional states (`r scales::percent(nrow(list_automatic_ties)/358)` of the 358 videos) which reveals the potential limit of using 6-points Likert scales to measure emotional self-reports. Indeed, Likert scales may not be able to discriminate between two dominant emotion felt with different intensities due to their reduced number of possibilities. To better discriminate between emotions, an alternative would be to use a continuous slider from 0 to 100 [see for example @lottridge2009emotional].

## Automatic Classifier’s Accuracy

```{r sr-ar}
comparison_sr_ar <- self_report_score %>% 
  dplyr::inner_join(automatic_recognition_score, by = "C_Video") %>% 
  dplyr::ungroup() %>% 
  dplyr::filter(ar_emotion != "undetermined") %>% 
  dplyr::filter(sr_emotion != "undetermined") %>% 
  dplyr::mutate_if(is.character, as.factor)

confusionMatrix_sr_ar <- 
  caret::confusionMatrix(
    data = comparison_sr_ar$ar_emotion,
    reference = comparison_sr_ar$sr_emotion
  )
```

Similarly to the previous analysis, the overall correlation of recognition and non-recognition revealed a significant but very low coherence between self-reported emotions and automatic classifier’s recognition (`r corr_sr_ar$full_result`). A confusion matrix was used to compare encoders’ subjective feeling with the emotion label recognized by the automatic classifier (Figure \@ref(fig:confusionMatrix-sr-ar)).

```{r confusionMatrix-sr-ar, fig.height=6, out.width ="\\textwidth", fig.pos = "!h", fig.cap="(ref:confusionMatrix-sr-ar)"}
confusionMatrix_sr_ar_freq <- confusionMatrix_sr_ar$table %>% 
  as.data.frame() %>% 
  dplyr::rename(ar_emotion = Prediction, sr_emotion = Reference) %>% 
  dplyr::mutate(n_video = sum(Freq)) %>% 
  dplyr::mutate(prop_video = Freq/sum(Freq)) %>% 
  dplyr::group_by(sr_emotion) %>% 
  dplyr::mutate(n_sr = sum(Freq)) %>%
  dplyr::mutate(prop_sr = Freq/sum(Freq)) %>% 
  dplyr::ungroup()

confusionMatrix_sr_ar_freq %>%
  ggplot(mapping = aes(x = sr_emotion, y = ar_emotion)) +
  geom_tile(aes(fill = prop_sr), colour = "white") +
  geom_text(
    aes(label = paste0(
      scales::percent(prop_sr, accuracy = 1), "\n(", Freq, "/", n_sr, ")"
    )),
    color = "black",
    size = 4,
    family = "serif",
    parse = FALSE,
    lineheight = 0.7
  ) +
  scale_fill_gradient(
    name = "Proportion of Recognized\nSelf-Reported Emotion",
    low = "white",
    high = "red",
    labels = scales::percent,
    limits = c(0, 1)
  ) +
  scale_x_discrete(name = "Self-Reports") +
  scale_y_discrete(name = "Automatic Classifier") +
  theme_minimal() +
  theme(
    text = element_text(size = 16, family = "serif"),
    axis.text.x = element_text(size = 16, angle = 45, hjust = 0.75, vjust = 0.9 ),
    axis.text.y = element_text(size = 16, hjust = 0.5),
    axis.title.y = element_text(margin = margin( t = 0, r = 20, b = 0, l = 0)),
    legend.text = element_text(size = 8),
    legend.position = "bottom"
  )
```

Results obtained for the comparison between emotions self-reported and recognized by the automatic classifier are somewhat similar to the ones with human observers (Table \@ref(tab:confusionTable-sr-ar)). Overall, a low agreement between emotion self-reported and emotion recognized by the automatic classifier (Accuracy = `r round(confusionMatrix_sr_ar$overall["Accuracy"],2)`, 95% CI [`r round(confusionMatrix_sr_ar$overall["AccuracyLower"],2)`,`r round(confusionMatrix_sr_ar$overall["AccuracyUpper"],2)`]; Kappa = `r round(confusionMatrix_sr_ar$overall["Kappa"],2)`) except for *happiness* (`r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "happiness" & confusionMatrix_sr_ar_freq$sr_emotion == "happiness", "prop_sr"]))` of the video self-reported) is evident.

```{r confusionTable-sr-ar, results="asis"}
confusionMatrix_sr_ar_table <- confusionMatrix_sr_ar %>%
  magrittr::use_series(byClass) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("Emotion") %>%
  dplyr::mutate(Emotion = gsub(pattern = "Class: ", replacement = "", x = Emotion)) %>% 
  dplyr::select(Emotion, Sensitivity, Specificity, Precision, F1) %>% 
  dplyr::mutate_if(is.numeric, round, 2) %>% 
  replace(is.na(.), "na.")

confusionMatrix_sr_ar_table %>%
  papaja::apa_table(
    caption = "(ref:confusionTable-sr-ar)",
    note = "na. values are produced when not enough data are available to compute accuracy indicators.",
    escape = TRUE
  )
```

Surprisingly the automatic classifier incorrectly recognized as *disgust* a significant proportion of videos in which *anger*, *happiness* and *surprise* was the highest self-reported emotion (respectively `r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "disgust" & confusionMatrix_sr_ar_freq$sr_emotion == "anger", "prop_sr"]))`, `r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "disgust" & confusionMatrix_sr_ar_freq$sr_emotion == "happiness", "prop_sr"]))` and `r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "disgust" & confusionMatrix_sr_ar_freq$sr_emotion == "surprise", "prop_sr"]))` of the videos self-reported). In parallel, the automatic classifier recognized as *happiness* videos in which *fear* and *surprise* was the highest self-reported emotion (respectively `r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "happiness" & confusionMatrix_sr_ar_freq$sr_emotion == "fear", "prop_sr"]))` and `r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "disgust" & confusionMatrix_sr_ar_freq$sr_emotion == "surprise", "prop_sr"]))` of the videos self-reported).

A comparable explanation involving the amount of undetermined video based on self-reports can be provided, as the level of undetermined emotions are very high for the self- reports.

## Comparison Between Human and Automatic Recognition

As previously mentioned, human observers appear to be more accurate than the automatic classifier to recognize an individual's subjective feeling (human observers Accuracy = `r round(confusionMatrix_sr_hr$overall["Accuracy"],2)`; automatic classifier Accuracy = `r round(confusionMatrix_sr_ar$overall["Accuracy"],2)`). However, both make similar mistakes as the two-tailed t-tests of Pearson's product-moment correlation between human observers and automatic classifier recognition is significant (`r corr_hr_ar$full_result`).

A third confusion matrix is used to compare similarities (diagonal) and differences between human observers and automatic classifier in classifying the six emotion labels (Figure \@ref(fig:confusionMatrix-hr-ar)).

```{r confusionMatrix-hr-ar, fig.height=6, out.width ="\\textwidth", fig.pos = "!h", fig.cap="(ref:confusionMatrix-hr-ar)"}
comparison_hr_ar <- human_recognition_score %>% 
  dplyr::inner_join(automatic_recognition_score, by = "C_Video") %>% 
  dplyr::ungroup() %>% 
  dplyr::filter(ar_emotion != "undetermined") %>% 
  dplyr::filter(hr_emotion != "undetermined") %>% 
  dplyr::mutate_if(is.character, as.factor)

confusionMatrix_hr_ar <- 
  caret::confusionMatrix(
    data = comparison_hr_ar$ar_emotion,
    reference = comparison_hr_ar$hr_emotion
)

confusionMatrix_hr_ar_freq <- confusionMatrix_hr_ar$table %>% 
  as.data.frame() %>% 
  dplyr::rename(ar_emotion = Prediction, hr_emotion = Reference) %>% 
  dplyr::mutate(n_video = sum(Freq)) %>% 
  dplyr::mutate(prop_video = Freq/sum(Freq)) %>% 
  dplyr::group_by(hr_emotion) %>% 
  dplyr::mutate(n_hr = sum(Freq)) %>%
  dplyr::mutate(prop_hr = Freq/sum(Freq)) %>% 
  dplyr::ungroup()

confusionMatrix_hr_ar_freq %>%
  ggplot(mapping = aes(x = hr_emotion, y = ar_emotion)) +
  geom_tile(aes(fill = prop_hr), colour = "white") +
  geom_text(
    aes(label = paste0(
      scales::percent(prop_hr, accuracy = 1), "\n(", Freq, "/", n_hr, ")"
    )),
    color = "black",
    size = 4,
    family = "serif",
    parse = FALSE,
    lineheight = 0.7
  ) +
  scale_fill_gradient(
    name = "Proportion Recognized Emotion",
    low = "white",
    high = "red",
    labels = scales::percent,
    limits = c(0, 1)
  ) +
  scale_x_discrete(name = "Human Observers") +
  scale_y_discrete(name = "Automatic Classifier") +
  theme_minimal() +
  theme(
    text = element_text(size = 16, family = "serif"),
    axis.text.x = element_text(size = 16, angle = 45, hjust = 0.75, vjust = 0.9),
    axis.text.y = element_text(size = 16, hjust = 0.5),
    axis.title.y = element_text(margin = margin(t = 0,r = 20, b = 0, l = 0)),
    legend.text = element_text(size = 8),
    legend.position = "bottom"
  )
```

The overall agreement between human observers and the automatic classifier is in fact very low (Kappa = `r round(confusionMatrix_hr_ar$overall["Kappa"],2)`). Except for *happiness* and *disgust* (respectively `r scales::percent(as.numeric(confusionMatrix_hr_ar_freq[confusionMatrix_hr_ar_freq$ar_emotion == "happiness" & confusionMatrix_hr_ar_freq$hr_emotion == "happiness", "prop_hr"]))` and `r scales::percent(as.numeric(confusionMatrix_hr_ar_freq[confusionMatrix_hr_ar_freq$ar_emotion == "disgust" & confusionMatrix_hr_ar_freq$hr_emotion == "disgust", "prop_hr"]))` of common labelling), there is no clear common pattern. Moreover, the automatic classifier has a tendency to label as *disgust* videos labeled as *sadness* by human observers, and as *happiness* videos labeled as *fear* by human observers.

## Discussion and Conclusion

Despite being one of the most investigated questions in affective science, the consistency between emotion felt and facially displayed on the one hand, and facial expression recognized on the other is a hot topic. To date no clear evidence has been found to definitively solve the questions raised. Yet, with the growing interest of industries and government in monitoring individual’s psychological states, this issue is under intense scrutiny. The present research aims to provide some empirical data to answer some of the questions posed. The faces encoders spontaneously displayed when confronted with an emotional eliciting task were submitted both to human and to automatic recognition. The criterion for recognition accuracy was the subjective feeling self-reported by the encoder once the elicitation task was carried out. Results first reveal a low consistency between emotion felt and facial expression displayed. They show that facial expressions of emotion are often not displayed when the Basic Emotion View would predict them to be expressed. Secondly, results show low accuracy rates for both humans and the automatic classifier in identifying the inner emotional states of these encoders based on their facial expressions. Thirdly, human observers prove to be better at recognizing the emotion facially expressed than the automatic recognition tool is. Such results support the hypothesis advanced by some authors of low emotion–expression coherence [@kappas2003facial]. In many instances, facial displays are not associated with a concordant emotional state, even any emotional state at all [@bonanno2004brief; @fernandez2013emotion]. More and more evidence is showing that facial expressions are in reality not expressing emotions [@mckeown2013analogical]. Increasing studies show that, for most emotions, the EFE elicited by emotional triggers are scarce and partial, even when micro-expressions are taken into account [@duran2017coherence]. These studies are conducted either in laboratory settings or in the field. This is not to say that facial expression is not an informative modality for understanding the emotional state of a person. There is indeed an affinity between emotion and facial display [@frijda1997facial]. However, claiming that unique facial muscles configurations are used both to express and to infer the presence of a specific emotion is misleading. As well as other nonverbal behaviors, facial movements are not only assumed to be determined by emotion but also by various other causes, such as psychological states (*e.g.*, motivations or pain), to say nothing of social context and sociocultural norms [@ekman1987universals]. Hence, facial movements have causes and functions other than the expression of emotion. This multiple determination excludes any possibility of drawing a linear inference from facial activity on the underlying psychological state (emotional or other). Beyond the present observations showing a weak consistency between subjective feelings and spontaneous facial expressions, this study sheds some light on the controversy between the Basic Emotion View and the constructivist approach as to the facial recognition issue. The former [@ekman1992argument] assumes that expressions of emotion are brief and coherent patterns of facial muscle movements that co-vary with discrete subjective experiences. In return, this information displayed by the face corresponds to the one extracted by perceivers. Instead of viewing emotions as natural kinds [@barrett2006emotions], the constructivist approach supposes that emotions are social constructions and that facial behaviors intrinsically situated. The emotions that are recognized by the observer are constructed in her/his mind. Therefore, facial movements do not express specific emotions because they do not carry intrinsic emotional signification. It is the observer that infers the emotional meaning of the facial expression, this interpretation depending on availability of different information such as context or linguistic categories. As a consequence, one can predict from the first line of thinking that individuals’ emotional subjective feeling should be correlated to the recognition of facial expressions from both human observers and automatic classifiers whereas if emotions are social constructs, as stated by the second line of thinking, human observers should be better at perceiving emotions expressed on the face than automatic classifiers.

The present findings speak against any strong version of the Basic Emotion View. The correlations between the self-measured emotions and the observed facial behaviors are low and the latter are weakly recognized. Present results plead instead in favor of the constructivist stance. They show that human observers are more accurate than the automatic recognition tool to identify an individual’s subjective feeling on the basis of their face. Moreover, mistakes made by human observers look less arbitrary than the ones made by the classifier. For instance, even if a mix-up between disgust and anger is sometimes reported in recognition studies, odd confusions such as the present ones produced by the classifier have never been noted for human observers. The latter obviously make sense of the facial behavior they are witnessing. However, the human decoders in the present study were presented with faces without any contextual information which could have helped them to shape more precisely their interpretation. @hassin2013inherently reviewed evidence to demonstrate that faces are inherently ambiguous and that observers rely on situational cues when they process facial displays [see also @aviezer2008angry; @barrett2011context]. It happens that contextual information even shifts emotion perception. In the present case, the facial stimuli displayed to the decoders were as equivocal as real-life facial expressions [@aviezer2017inherently]. Yet, decoders had no cue at their disposal. So, without any possibility of integrating faces and context, their decoding accuracy is reduced [*e.g.*, @wagner1986communication]. However, their superiority to the classifier tool is probably owing to their capacity to rely on their previous personal experience to invent a context in which a face could display such an expression. Once a credible context is retrieved, they can affix an emotional label to the facial behavior.

Several limitations should be stated, highlighting the need for further research. One of them is effectiveness of the emotion elicitation tasks. One can consider that an intensity threshold needs to be exceeded for a visible emotional expression to occur. In the case of the present study, it may be that insufficient emotion intensity accounts for the low number of visibly reactive participants. However, such line of reasoning fits badly with the Basic Emotion View according to which emotions and their related prototypical facial behavior are universal because they are considered as innate mechanisms allowing individuals to respond adaptively to evolutionary significant events (threats, opportunities *etc.*) encountered in the environment, whatever their potency. Moreover, various studies have used fairly strong emotion inductions without obtaining any visible facial display of any kind of emotion [@duran2017coherence]. Moreover, the present results show a quite good correlation between reported emotion and facial behavior for happiness. Thus, there are no reasons to believe that an explanation in terms of too low an emotion threshold applies for the other emotions under consideration. The latter were triggered and measured with conceptually identical elicitation and assessment procedures [@reisenzein2013coherence]. The use of self-reports to evaluate encoders’ subjective feelings is another limitation that can also be put forward because of the numerous cognitive biases they entail. Well known problems with the reliability of self-reports are, among others, the reconstructive nature of memory, the influence of attentional biases on reports, demand characteristics, distorting effects of implicit causal theories and personal motives, as stressed by @nielsen2007conceptual. It is clearly obvious that self-reports are not simple outlets of inner mental processes but personal constructions and they are affected by many factors [@kappas2003facial]. The emotional feeling echoes motivational tendencies, bodily changes, and cognitive appraisals of events [@sander2014oxford]. All these are encapsulated into semantic categories referred to by labels. As things stand currently, there is unfortunately no objective way for accessing and assessing inner subjective emotional feelings except for asking people to report their subjective experience in words. The procedure used for human recognition can also be open to dispute. First, one can criticize the decoders' expertise level since they were not FACS coders but untrained students. Of course one can expect a difference between skilled annotators and novice ones regarding the assessment of emotional facial displays. This said, in everyday life, few people are specialist coders yet the quantity of successful social interactions proves lay persons recognize quite well others’ facial behaviors. Therefore, and especially in the interest of generalizability, asking inexpert people seems relevant. Secondly, instead of using a classic forced-choice procedure, a more subtle approach was chosen to mimic results provided by the automatic classifier. As explained above, annotators first delimitated a temporal sequence during which they noticed an emotional display on the face, and then attributed an emotion label to this behavioral sequence in a second step. Whereas this paradigm is longer and more complicated, it can lead to more robust results in reducing the forced-choice bias [@russell1993forced]. However this procedure can also reduce the human observers’ accuracy. In this regard, the results of the human observation could have been more ambiguous because it is not the natural way that people are inferring meaning from facial expressions. An alternative explanation relies in reducing the recognition bias involved in the classic recognition paradigm. Classic forced-choice paradigms obtain artificially high results, thus by using a more evolved approach, observers’ accuracy may have been lowered. Another flaw is the lack of comparison with various facial expression recognition methods. Human recognition has only been compared to the Affdex classifier. Future studies are needed to confront human assessments with different automatic recognition methods, both frame based methods and sequence based automatic ones. This latter issue is particularly decisive. Indeed, the issue of the recognition of dynamic expressive sequences is essential because ordinary facial behavior is made up of dynamically shifting morphological features [@krumhuber2013effects]. This temporal information is indisputably a key feature of facial activity. It is not only observer-based judgements of facial displays which must be compared to automated facial analysis, but also different kinds of human recognition measurements should be undertaken. The challenge researchers are especially confronted with is to find ways to appropriately collect data regarding the perception of spontaneous and dynamic facial behavior. Finally, our understanding of facial displays as they occur in everyday interactions requires a strong emphasis on ecological concerns. The present study is a laboratory experiment.  It has the advantage of controlling different parameters of the emotions investigated, such as intensity, quality, and temporal (onset, duration) features. Moreover, as encoders are alone, facing an emotional trigger, it also controls for the social context, removing its possible influence on their facial behaviors. However, it is known that encoders’ imagination can influence their expressiveness, for instance when they believe that their friend is doing the same (*vs.* a different) emotional task in another room [@jakobs1999social]. Trying to exclude social influence by leaving encoders alone may be illusionary. From an ecological perspective, it is even a mistake to exonerate behavioral observations from social contexts.  Facial activity measurement in dyadic interactions has shown that the facial behavior of the perceiver reflects sometimes more what the expresser is experiencing than what the perceiver is feeling. It is the case, for instance, of emotional mimicry in dynamic social interactions [@hess2016emotional]. It is also the case of healthy patients interacting with schizophrenic patients whose facial activity is almost identical [@krause1998dyadic]. Hence, in order to better comprehend emotional communication in human relationships, experimental research should be corroborated with more ecological protocols.

Nowadays, automatic recognition systems are based on the coding of the facial muscular activations from which they infer the expressed emotion. Such automatic classifier tools take for granted that, when experienced, an emotion is: firstly, displayed on the face; secondly, in the form of a configuration of facial muscles that is unique to a person; and thirdly, recognized by the perceiver (human being or automatic classifier). These are the Basic Emotion View assertions, jeopardized by field observations and laboratory experiments on spontaneous expression of emotions, such as the present study. All raise serious objections to the supposed close relation between emotion and face. They bring up several questions regarding the role the context plays in the emission and interpretation of the so-called facial expression of emotion. The finalization of operational and effective “reading emotional faces” devices rests on the answers, if any, to the questions raised. As a result, despite being one of the most investigated questions in affective science, the growing interest of industries and governments in tracking individual’s psychological states is supported by controversial assumptions. Considering the above, the present results provide additional evidence that an individual’s subjective feelings cannot be inferred from facial expressions, and they invalidate the hypothesis of hardwired emotions unambiguously displayed on the face. Even if emotions were hardwired, in everyday life one does not observe prototypical facial expressions because of their rarity, and therefore research should be focused on analyzing non-prototypical facial expressions. Advancements in identifying “non-basic” emotion labels as well as non-prototypical facial expression have indeed occured with automatic facial expression recognition tools [@mcduff2016discovering]. However, these present results suggest that automatic facial expression recognition tools should merely evaluate facial morphology features such as action units [already evaluated in OpenFace, @baltruvsaitis2016openface; Affectiva’s Affdex, @mcduff2016affdex; Vicar Vision’s FaceReader, @den2005facereader, to name a few] rather than inferring supposedly emotional or affective states. Trying to interpret facial displays as a means of determining underlying emotional state, in all likelihood, remains in vain. 

\newpage

# References
```{r create_r-references}
#r_refs(file = "bib/r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
