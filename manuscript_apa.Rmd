---
title             : "The Emotion–Facial expression link: Evidence from Human and Automatic Expression Recognition"
shorttitle        : "The Emotion–Facial expression link"

author: 
  - name          : "Anna Tcherkassof"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Bâtiment Michel Dubois, Université Grenoble Alpes, 1251 Avenue Centrale, 38400 Saint-Martin-d'Hères"
    email         : "anna.tcherkassof@univ-grenoble-alpes.fr"
  - name          : "Damien Dupré"
    affiliation   : "2"

affiliation:
  - id            : "1"
    institution   : "University Grenoble Alpes, Grenoble, France"
  - id            : "2"
    institution   : "Dublin City University, Dublin, Ireland"

abstract: |
  While it has been taken for granted in the development of several automatic facial expression recognition tools, the question of the coherence between subjective feelings and facial expressions is still a subject of debate. On one hand, the Basic Emotion View conceives emotions as genetically hardwired and therefore being genuinely displayed through facial expressions. On the other hand, the constructivist approach conceives emotions as socially constructed; the emotional meaning of a facial expression being inferred by the observer. In order to evaluate the coherence between the subjective feeling of emotions and their recognition based on facial expression, 232 videos of encoders recruited to carry out an emotion elicitation task were annotated by 1383 human observers as well as by an automatic facial expression classifier. Results show low accuracy of human observers and of the automatic classifier to infer the subjective feeling from the facial expressions displayed by encoders. They also show a weak consistency between self-reported emotional states and facial emotional displays. Based on these results, the hypothesis of genetically hardwired emotion genuinely displayed is difficult to support, whereas the idea of emotion and facial expression socially constructed appears to be more likely. Accordingly, automatic emotion recognition tools based on facial expressions should be questioned.
  
keywords          : "Facial expression, self-report, human observer, automatic recognition."
wordcount         : "7920"

bibliography      : ["manuscript_ref.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : 
  bookdown::pdf_book:
    base_format: papaja::apa6_pdf

header-includes:
  - \usepackage{booktabs}
  - \usepackage{float}
  - \usepackage{tabu}
  - \usepackage{wrapfig}
  - \usepackage[none]{hyphenat}
  - \floatplacement{figure}{H}
  - \usepackage{flushend}
  - \usepackage{biblatex}
---

```{r setup, include = FALSE}
library(papaja)
library(kableExtra) # Tables
library(tidyverse) # Data wrangling & grammar of graphics
library(here) # Locate files relative to project root
library(caret) # Confusion matrices
```

```{r analysis-preferences}
set.seed(123) # Seed for random number generation
options(scipen = 999)# Disable scientific number format
knitr::opts_chunk$set(
  cache.extra = knitr::rand_seed, 
  message = FALSE, 
  warning = FALSE, 
  error = FALSE, 
  echo = FALSE
  )
```

```{r data}
# self_report_data
source(here::here("script/data_wrangling_self_report.R"))
self_report_score <- self_report_score %>% 
  dplyr::select(C_Video, sr_emotion = emotion, sr_score = value)
# human_recognition_data
source(here::here("script/data_wrangling_human_recognition.R"))
human_recognition_score <- human_recognition_score %>% 
  dplyr::select(C_Video, hr_emotion = emotion, hr_score = confidence_score)
# automatic_recognition_data
source(here::here("script/data_wrangling_automatic_recognition.R"))
automatic_recognition_score <- automatic_recognition_score %>% 
  dplyr::select(C_Video, ar_emotion = emotion, ar_score = confidence_score) %>% 
  dplyr::filter(C_Video %in% ppt_per_video$C_Video)
```

# Introduction

With the development of commercial automatic facial expression recognition tools [@dupre2018accuracy], industries and governments are gradually implementing this technology in order to track humans’ emotions in various scenarios (*e.g.*, in marketing, healthcare, and the automotive industry to name a few). This technology rests on the premise that facial expressions provide a direct access to individuals’ subjective feelings, and that one can read the emotions displayed on a person’s face as in an open book. This premise is derived from the Basic Emotion View which Paul Ekman is one of the famous proponents of this perspective [@ekman1987universals; @ekman1988universality; @ekman1992argument; @ekman2007directed]. Even if this premise is central to the modern mainstream approach of human emotion, recent research in affective science is challenging it. Once the Basic Emotion View is briefly described, its foremost criticisms are synthetically exposed. Inconsistencies and unpredicted findings research has uncovered come both from field observations and laboratory experiments. They relate to two different issues. The first issue is the one of the sender’s production of spontaneous facial displays. The second issue has to do with the viewer’s interpretation of these facial displays. Building on this evidence, and as a further attempt to shed light on the emotion–face link, the present study is finally exposed. Its main goal is to tackle these two issues at the same time. It aims at identifying the extent to which ordinary people’s experienced emotions are displayed through identifiable emotional facial expressions (EFE) recognized as such both by human individuals and by automatic facial expression recognition tools.

## The Basic Emotion View

The belief that facial expression is linked to emotional states can be traced back to Darwin in *The Expression of the Emotions in Man and Animals* [@darwin1872expression]. Countless studies have fortified the idea that emotional states are inherently coupled to a set of prototypic facial expressions (*e.g.*, [@ekman2011meant]). The Basic Emotion View holds that facial expressions are genuine displays of an individual’s inner emotional state. More specifically, a set of six emotions (*fear*, *anger*, *surprise*, *disgust*, *sadness* and *joy*) are universally displayed and are genetically hardwired not only in humans [@ekman1992argument], but also in different animal species [@de2019mama]. According to the view of the Basic Emotion View, *“when emotions are aroused by perception of a social event, a set of central commands produce patterned emotion-specific changes in multiple systems, including […] facial expressions.”* [@ekman2007directed, 49]. To respond to criticisms, several amendments have been made to the Basic Emotion View, increasing the number of basic emotions from six to seven [@ekman1988universality] as well as adding the concept of “display rules” to explain cultural differences in the management of facial expressions [@ekman1987universals]. 

Relatively few studies have actually tested Basic Emotion View’s fundamental claim regarding the facial production (so-called ‘expression’) of emotions. @rosenberg1994coherence reported the first evidence of coherence between self-report of emotion and displayed facial expressions. Participants were showed emotionally evocative films and reported their own emotions using a moment-by-moment reporting procedure. Analysis of participants’ facial expressions and reports of emotions showed that there was a high degree of temporal linkage and emotional agreement between facial expressions and self-reports. Notwithstanding this study, few have used purely descriptive methods (such as electromyography or objective coding systems – FACS or others) to identify and to measure the actual changes in the face when a given emotion is felt (see @wagner1997methods, for methodological points). Much of the research has focused instead on the recognition of EFE, that is on the issue of the viewer’s interpretation of facial displays. As people have been thought to display their internal state, EFE supposedly serve as a window into the emotions of others. Viewers “recognize” the facial displays and infer the corresponding emotional state. The Readout Hypothesis [@buck1985prime] formalizes this folk theory. According to it, facial expressions have evolved to provide “an external readout of those motivational-emotional processes that have had social implications during the course of evolution” (pp. 396-397). Thus, as readouts of emotional states, spontaneous expressive displays are directly accessible to other organisms. Research on the inference of emotions from facial expressions has established that viewers show considerable agreement that the so-called basic emotions—happiness, anger, fear, disgust, sadness, and surprise—are associated with specific facial displays (*e.g.*, reviews from @elfenbein2002universality; @russell1994there). These findings are considered by many as constituting strong evidence for the Basic Emotion View. 

Yet, in spite of the popular support it has received, the empirical data called upon by the Basic Emotion View remains unpersuasive. Notably, it fails to explain, in instances in which display rules cannot be called upon, how individuals can feel emotions without expressing them or how individuals can express emotions without feeling them, and why observers are not totally accurate in recognizing facial expressions of basic emotions, among others. The following  is a summary of key findings and conclusions both from field observations and laboratory experiments on spontaneous expression of emotions, all of them strongly subverting the Basic Emotion viewpoint (for reviews, @fernandez2013emotion; @reisenzein2013coherence; @russell2003facial). Mains issues regarding recognition studies are also swept across afterwards. 

## Spontaneous Facial Expression in naturalistic studies

Naturalistic studies look at the ecological frequency of co-occurrence of certain emotions and facial displays [fernandez2013emotion]. Though they cannot be considered as a straightforward test of the triggering role of emotion on the facial behavior, they have the advantage of considering situations that cannot be created in a laboratory. For instance, the ethological study of @kraut1979social contradicts the Basic Emotion View premise that a smile is the major component of a facial display associated with and caused by feelings of joy or happiness. Naturalistic observation at a bowling alley showed that bowlers do not necessarily smile after scoring a spare or a strike (a situation likely to elicit a positive emotion). Rather, they often smile when interacting with other people. More generally, examination of bowlers’ facial display showed that they rarely smiled while facing the pins but often smiled when facing their friends. These findings were confirmed by @ruiz2003spontaneous who analyzed the facial displays of bowlers (after a strike) and soccer fans (after their team scored) reporting happiness in a field study. They found that the probability of smiling was very low when participants were not interacting with someone else. These findings have been supported in other realistic field settings such as the one of @fernandez1995smiles suggesting that happiness is not a sufficient cause of smiling. They watched extremely happy gold medalist athletes displaying facial expressions of sadness (sometimes associated with tears) during their Olympics awards ceremonies. More specifically, winners showed Duchenne smiles and other types of smiles when they were interacting with other people during the awards ceremony, but smiles were scarce or nonexistent when waiting behind the podium and/or when turning toward the flagpoles and focusing their attention on the flags and the national anthem. @crivelli2015smiles also found that the strongest predictor for the occurrence of Duchenne smiles in judo winners is when they are engaged in social interaction and not when they just won their match.

Happiness (or joy) is not the only emotion weakly associated with the predicted facial expression. A naturalistic study conducted by @scherer1997lost in an airport’s baggage handling office showed that passengers claiming for their lost luggage displayed very few facial expressions of negative emotions while self-reporting subjective feeling states of anger or sadness among others. The covariation between passengers’ self-ratings and the claims agents’ attributions of the passengers’ emotions was very low. Another refutation of the Basic Emotion View’s predictions regarding the link between emotions and facial expressions is provided by naturalistic observations of infants’ productions of facial expressions. It is often acknowledged that adults regulate their expressive behavior. In accordance with various display rules (personal, social, cultural ones), they exert a control over the (supposed) automatic readouts of their emotions. Developmental studies provide relevant settings in which display rules are inoperative. Camras and her colleagues observed that facial expressions of negative emotions were displayed in circumstances that were unlikely to have elicited those emotions. For instance, infants displayed "fear" expressions in settings not related to fear [@camras1991development]. @bennett2002facial videotaped a sample of 4-month-old infants during tickle, sour taste, jack-in-the-box, arm restraint, and masked stranger situations. Infants displayed a variety of facial expressions in each eliciting situation. Yet, more infants exhibited positive (joy and surprise) than negative facial expressions (anger, disgust, fear, and sadness) across all situations –except sour taste. No evidence for emotion-specific facial expressions corresponding to anger, fear, and sadness was obtained. Camras and her colleagues also observed that 11-month-old European-American, Chinese, and Japanese infants did not display distinct negative emotion-specific patterns of facial muscles in response to two elicitors meant to induce fear and anger [@camras2007infants]. Thus, these findings in naturalistic settings provide little support for the Basic Emotion View.

## Spontaneous Facial Expression in laboratory studies

Laboratory findings also support field studies. As stressed by @reisenzein2013coherence, experimental studies permit both better control of various factors (*e.g.*, emotion elicitors) and tests about likely moderators of the emotion–facial expression link (*e.g.*, the social context). As an example, a strong disconfirmation of the Basic Emotion View’s premise is put forward by Reisenzein’s studies on surprise (*e.g.*, @reisenzein2000exploring). In eight controlled laboratory situations, surprise was induced by @reisenzein2006evidence by establishing and then invalidating a set of beliefs concerning the experimental events (*e.g*., the unexpected appearance of a picture of one's own face as the last picture in a series of portraits that had to be rated). Visible or EMG-detected facial displays of surprise occurred only in few participants. Yet, most participants reported subjective feelings of surprise and most believed that they had shown a strong surprise facial expression. @schutzwohl2012facial also observed similarly low frequencies of surprise facial expressions when their participants, after leaving the laboratory, unexpectedly found themselves not in the corridor but in a new room with green walls and a red office chair. Less than a quarter of the participants displayed an expression of surprise: only 5% showed widened eyes, raised eyebrows, and opened mouth (*i.e.* the complete expression of surprise according to the Basic Emotion View) and 17%  showed widened eyes and raised eyebrows. Again, participants overestimated their surprise expressivity.

Studies on happiness and related positive emotions (*e.g.*, sensory pleasantness) also show a low coherence between emotion and facial display [@duran2017coherence; @reisenzein2013coherence]. In fact, only experiments on amusement provide a fairly strong association between emotion and smiling. When confronted to humorous events (*e.g.*, jokes, being tickled), the number of facially reactive participants (smiling and laughing) is indeed quite fair, whereas when confronted to other positive emotions (*e.g.*, happiness), few participants show the expected facial expressions [for instance, the Duchenne smile or any kind of “happy” expression yet reporting being happy; see @mehu2007smiles; @lee2002effect, social condition]. More largely, the meta-analysis conducted by @duran2017coherence on the degree of statistical covariation between emotions and facial expressions provides conclusive evidence. First of all, the commonly called basic emotions (happiness/amusement, sadness, fear, anger, disgust and surprise), on average, are weakly correlated with the specific configuration of facial muscles that the Basic Emotion View ascribes to them (the correlation even drops when happiness/amusement studies are excluded). Very few participants who relived an experience of sadness [*e.g.*, @tsai2002emotional] were reactive, only a small number of them showing components of a sad facial expression (*e.g.*, oblique eyebrows, lip corners pulled down). Regarding fear, spider phobic participants recruited by @vernon2002disgust, for instance, were exposed to a live tarantula. About one third of them only displayed some components of the prototypical fear expression (eye widening, brow raising and knitting, etc.). Similar observation is made for anger. The proportion of facially reactive participants (*i.e.* displaying at least one component among frowning, lid/lip tightening...) reporting anger in @johnson2010smile and @tsai2002emotional studies does not exceed 35%. The same proportion of reactive participants confronted to disgusting elicitors is observed [*e.g.*, @ekman1980facial; @fernandez1997spontaneous]. In any case neither an insufficient intensity of the emotion to cause a facial expression nor the intervention of display rules (nor measurement issues) can explain the low emotion–facial expression consistency.

Hence, laboratory studies show that facial expressions of emotion are often not displayed in situations in which the Basic Emotion View would predict them to be expressed. Moreover, when they are indeed displayed, they are more often displayed partially than completely. All in all, research on the spontaneous expression of emotions does not yield strong support for the Basic Emotion View. The available evidence steadily indicates weak links between emotions and their predicted facial expressions both in natural and in semi-naturalistic settings. 

## Emotional Facial Expression Recognition studies

The Basic Emotion View postulates that, when triggered, each basic emotion is expressed by a prototypical face (non-basic emotions being blends of the basic ones). In return, the recognition of emotional facial expressions (EFE) is claimed to be based on the identification of specific patterns of facial movements associated with each emotion, as if expression and recognition were the two sides of a same coin. Seemingly compelling evidence support this claim, sustaining the belief that one can read a face in the same way one reads the lines of a book. Besides, recognition systems rely on such a principle and are considered as an objective coding tool because they are based on the identification of specific muscular changes in the face. Many people highly agree that so-called basic emotions are associated with specific facial configurations (Ekman, 2017; Elfenbein & Ambady, 2002; Russell, 1994) and also agree that many believe that it is a strong evidence for the Basic Emotion View. Moreover, it implies that EFE recognition of both human observers and automatic classifiers should be as accurate.

Yet, some researchers have highlighted the limitations of Basic Emotion View empirical research. Among others, evidence has been questioned on methodological grounds [*e.g.*, @russell1994there]. The response format usually used in recognition studies (*i.e*. forced choice: selection of one word from a pre-specified list of emotion labels), notably, leads to a biased consensus as evidenced by @russell1993forced. Depending on the list of emotion labels at participants’ disposal, EFE of sadness can easily be categorized as sad expressions as well as fear expressions... @russell1987relativity also showed the same facial expression can be seen as expressing different types of emotions, depending on what other faces are seen. @digirolamo2017emotion conducted seven experiments that establish that high agreement between participants can be an artifact of the standard method. Thus, results gathered with forced choice cannot demonstrate the univocal link between emotion and facial expressions claimed by the Basic Emotion View. Using alternative recognition methods [emotion satiation procedure, face-matching task, sorting task; *e.g.*, @lindquist2006language; @gendron2014perceptions], it has been shown on the contrary that facial muscle movements are not linked in a one-to-one manner to a specific discrete emotional experience (2017,p. 418). Instead, emotions are probably mentally constructed by the perceiver and mental categories of emotions are needed to accurately categorize facial movements, among other things (*e.g.*, contextual information). 

To the methodological limitations contaminating the hundreds of studies apparently supporting the Basic Emotion View, a stimulus bias must be added. Facial stimuli used in experiments also constitute a methodological bias because they are unrepresentative of ordinary facial expressions. Basic Emotion View empirical evidence is based for the most part on methods using a static and unnatural material, namely, still photographs of posed facial expressions of emotion (*e.g.*, intentionally encoded by the sender). This kind of methodology raises questions about its ecological validity and the generalizability of the results to real interpersonal emotional communication [*e.g.*, @tcherkassof2007facial]. Indeed, a number of pieces of evidence indicate that research cannot content itself with data collected with static and posed material. These data come from researches studying the case of dynamic and/or spontaneous facial expressions of emotion. They show that the dynamic aspects of facial movement are likely to be of importance [*e.g.*, @kamachi2013dynamic]. @cohn2003timing have shown that spontaneous smiles are of smaller amplitude and have a more consistent relation between amplitude and duration than deliberate smiles. @hess1990differentiating have also pointed out the importance of the dynamics of facial movements, and particularly the irregularity, or phasic changes, of the expressions’ unfolding. Thus, the motion of facial expression provides observers with other information than the one provided by static expressions. It may be that differences in the social information displayed by static and dynamic expressions leads to facial recognition differential effects. Regarding the issue of spontaneous vs posed expressions (the latter are overused in experiments). As @meillon2010dynemo conclude, EFE have been typically studied as static displays. As a consequence, even though the central role of the dynamics of facial expressions is endorsed, little is still known about the temporal course of facial expressions. Furthermore, studied EFE exhibit emotions simulated or posed by actors. Yet, the lack of spontaneity and naturalness of this material constitutes a serious objection raised against such studies [@kanade2000comprehensive].

All in all, as many doubts can be raised about the standard method, experiments conducted with such a method cannot be considered as providing solid empirical support to the Basic Emotion View. Based on the numerous methodological criticisms, but also theoretical, addressed to this view, alternative conceptions have emerged. Among them, the constructivist approach is gaining in importance. The constructivist approach represents a totally different way of understanding the emotion–facial expression link. It affirms that facial expressions do not provide a direct access to individuals’ subjective feelings. Therefore, instead of considering that emotions can be “read” on facial displays, it claims that the emotion is “in the eye” of the perceiver [@barrett2019emotional]. 

## Constructivist Approach

Starting from the empirical evidence suggesting that spontaneous facial expressions in ordinary life are equivocal, @dols2017natural argues in favor of a pragmatic conception of natural facial displays. He makes a plea for the idea that natural facial displays, rather than "saying" – because they do not have a specific meaning he claims – “make” things. Facial expressions are actions in a communicative interaction. They do not express emotions but they *"prompt, on the receiver’s side, important inferences about the context, the sender, and the course of the interaction between sender and receiver"* [@dols2017natural, 466, underlined by the author]. As such, the fact that facial displays are able to signal emotions is a byproduct of one of its main function, the one of implementing actions performing practical ends. Therefore, expressive displays hold a motor intention [@pacherie2003modes]. They implement their aim which is their motor intention. They are not primarily communicative signals and even less the outlet of an internal state. Facial displays are (parts of) pragmatic actions aiming at orienting the person’s relation to its environment, for instance maintaining, breaking,  restoring, etc., the relation [@frijda2012recognition]. Facial displays are not “recognized” in semantic terms but are perceived as intentional actions. In the face of the continual flow of uninterrupted facial movements, the perceiver sees behaviors directed towards a goal. S.he translates the continuous flow of movements into coordinated sequences of actions holding a beginning and an end. Facial displays are not simple strings of action units which morphological configuration would be the prototype of a given emotion, and consequently identified as such [@ekman1987universals]. They are best conceived as a Gestalt, the same way as a string of musical notes establishes a melody [@tcherkassof2014emotions]. This is why even unauthentic facial displays can yet be recognized as emotional expressions. Guillaume Duchenne de Boulogne explains that the artist who has shaped the famous Laocoon antic sculpture, exhibited in the Vatican’s museum, has made a modelling mistake since no face can display its emotional expression [@duchenne1876mecanisme]. Indeed, no muscular contraction can produce it. He even rectifies the "mistake" by presenting a statue which face is shaped according to the physiology of facial expressive movements. His demonstration gives food for thought. Even though no objective coding system can correctly code the discordant facial features of the Laocoon’s face, yet anyone can easily recognize the suffering and despair he admirably expresses. This example goes along the lines of the constructivist approach.

The constructivist approach claims that facial displays are behaviors which meaning is inferred by the perceiver. Findings support this observer dependence [@lindquist2013s; niedenthal2017embodied]. They show that to make meaning of another person’s facial behavior, the perceiver relies in particular on her/his knowledge about emotion categories. For instance, Gendron and her colleagues used a face-sorting task allowing them to manipulate the influence of emotion concepts on how facial expressions were perceived. They conducted their experiment among U.S. participants and Himba participants from remote regions of Namibia [@gendron2014perceptions] and Hazda participants of the Eastern Rift Valley of Tanzania [@gendron2018emotion] both groups with limited exposure to Western culture. Gendron and her colleagues demonstrated that facial expressions were not universally “recognized” in discrete emotional terms. Indeed, when Himba and Hazda participants did not have emotion concepts at their disposal to structure perception, they perceived the facial expressions as behaviors (*e.g.*, looking, smelling) that didn’t have a necessary relationship to emotions. They didn’t infer inner states (such as emotional feelings) but they rather proceeded with action identification that pointed out the functions of behaviors [see also @crivelli2017recognizing for similar observations among a small-scale society of Papua New Guinea]. The constructivist approach considers that specific emotion categories, as conceptualized by Western cultures’ knowledge, are casted on the perceived face to make meaning of the sender’s facial displays. Following this approach, faces convey a range of information essential for social communication. They are best conceived as tools displaying signals in social interactions [@crivelli2018facial]. These signals can convey individuals’ motivations and readiness [@frijda1997facial] or social messages [@fridlund1994human]. As for emotional meaning, more specifically, it is shaped by the perceiver according to the specific context in which the facial displays are observed.

Having reached this point, one can assert that numerous questions regarding the link between emotions and facial expressions remain unanswered. The two main competing approaches to facial expressiveness, the Basic Emotion View and the constructivist approach, entail completely opposite predictions regarding the decoding of facial expressions, as evidenced above. The present study aims at examining these predictions in order to provide empirical evidence to allow the discussion to evolve. To date, no systematic study has looked at the same time at facial expressions spontaneously displayed in reaction to emotional triggers and how they are decoded, both by human observers and by automatic emotion recognition tools based on the detection of facial muscular configurations. This study fills that gap. It intends to investigate the consistency between the subjective feeling of emotions and its recognition from facial expressions. Spontaneous and dynamic facial reactions to emotional elicitations are under consideration to ensure the generalizability of the results to emotional behaviors in ordinary life. More specifically, this study aims to examine the recognition of EFE produced by ordinary people during situations judged and/or self-reported to involve different emotions. It (a) examines consistency between ordinary people’s self-reported emotional experience and observers’ judgments of these ordinary people’s EFE, (b) examines consistency between ordinary people’s self-reported emotional experience and an automatic classifier’s analysis of these ordinary people’s EFE. In other words, it is interested in how people actually move their faces to express self-reported emotions, in how human observers accurately infer the expresser’s emotional state, and in how automatic recognition accurately code the expresser’s emotional state. It is expected a human observers’ superior ability to exactly recognize EFE as compared to automatic EFE recognition tools.

# Methods

To evaluate the consistency between subjective feeling of emotions and their recognition from facial expressions, encoders were first recruited to perform an emotion elicitation task while their facial expression was video recorded. In order to reduce the likelihood of facial control, the encoders were alone in the room and were filmed by hidden cameras, so they had no reason to comply with social display rules. Then, the videos of the encoders’ faces were shown to human observers and were also analyzed by an automatic classifier in order to identify which emotion was displayed.

## Emotion Elicitation

```{r self_report_method}
self_report_gender <- self_report_data %>% 
  dplyr::group_by(genre_c) %>% 
  dplyr::summarise(n = n())

self_report_age <- self_report_data %>%
  dplyr::summarise(Age_m = mean(age), Age_sd = sd(age)) %>% 
  dplyr::mutate_all(round, 1)
```

For the emotion elicitation experiment, `r nrow(self_report_data)` encoding participants (`r self_report_gender[self_report_gender$genre_c == "F","n"]` females, `r self_report_gender[self_report_gender$genre_c == "H","n"]` males, *M*~age~ = `r self_report_age[,"Age_m"]`, *SD*~age~ = `r self_report_age[,"Age_sd"]`) were recruited to perform one out of 11 emotion elicitation tasks designed to trigger a positive, a specific negative or a neutral emotional state. Encoders’ faces were recorded using a hidden camera resulting `r nrow(self_report_data)` front facing 768x576 videos varying from `r min(metadata_video$ffprobe_duration)`s to `r max(metadata_video$ffprobe_duration)`s. These recordings form the DynEmo database [@tcherkassof2013dynemo].

After the emotion elicitation task, the encoders rated their subjective feeling on Likert scales from 0 ("not at all") to 5 ("strongly") related to six "basic" emotion labels (*i.e.*, *anger*, *disgust*, *fear*, *happiness*, *surprise* and *sadness*) as well as six "non-basic" emotion labels (*i.e.*, *pride*, *curiosity*, *boredom*, *shame*, *humiliation*, and *disappointment*).

Finally, a debriefing session was performed to ensure that encoders were not durably affected by the emotion elicitation task. The debriefing was also used to check that encoders did not guess the real purpose of the experiment (*e.g.*, being filmed while they were performing an emotional elicitation task) to guarantee facial expressions’ genuineness. All encoders gave their agreement on their data and video to be processed for research purposes only.

## Human Facial Expression Recognition

```{r human_recognition_method}
unique_ppt_count <- human_recognition_data %>% 
  dplyr::group_by(source,SEXE_Juge, C_Juge) %>% 
  dplyr::summarise(n = n())

unique_video_count <- human_recognition_data %>% 
  dplyr::select(source,SEXE_Juge, C_Juge, C_Video) %>% 
  unique() %>% 
  dplyr::group_by(C_Video) %>% 
  dplyr::summarise(n = n())
```

For the human facial expression recognition method, `r nrow(unique_ppt_count)` student participants were recruited to annotate `r nrow(unique_video_count)` out of the `r nrow(self_report_data)` videos, therefore only the `r nrow(unique_video_count)` annotated videos were analyzed in this study. Because videos have different durations, participants had to annotate a series of video corresponding to 30min long in total. Each video was annotated `r mean(unique_video_count$n) %>% round(0)` times on average (*SD* = `r sd(unique_video_count$n) %>% round(0)`).

The annotation of facial expressions was performed on-site using *Oudjat*, a software for designing video annotation experiments [@dupre2015oudjat]. For each video, the annotation procedure followed two steps. First, the participants had to identify the emotional sequences by pressing the space bar of their keyboard to indicate the beginning and the end of the emotional sequences while watching the video. Second, the participants watched each emotional sequence previously identified and labeled the sequence using one of the 12 emotions proposed including six "basic" emotion labels (*i.e.*, *anger*, *disgust*, *fear*, *happiness*, *surprise* and *sadness*) and six "non-basic" emotion labels (*i.e.*, *pride*, *curiosity*, *boredom*, *shame*, *humiliation*, and *disappointment*). They also had the possibility to indicate that the sequence was expressing none of the proposed emotion labels.

This annotation procedure results in a uni-dimensional time-series for each video per human observer identifying for each second of the video which emotion was recognized. Then, time-series corresponding to the same video were aggregated to calculate the proportion of human observers $x_{video_{i}.label_{j}.t_{k}}$ for each second of the video per emotional label (EQ\ref{eq:1}).

\begin{equation}
\label{eq:1}
x_{video_{i}.label_{j}.t_{k}} = \frac{n_{video_{i}.label_{j}.t_{k}}}{n_{video_{k}}}
\end{equation}

where *i* is one of the `r nrow(unique_video_count)` videos, *j* is one of the six “basic” emotion labels, *k* for each second of the video.

## Automatic Facial Expression Recognition

The `r nrow(unique_video_count)` annotated video were processed with Affdex (SDK v3.4.1). Affdex is an automatic facial expression recognition classifier developed and distributed by Affectiva is a spin-off company resulting from the research activities of MIT media lab created in 2009 [@mcduff2016affdex]. Affdex’s algorithm uses Histogram of Oriented Gradient (HOG) features and Support Vector Machine (SVM) classifiers in order to recognize facial expressions. For each video frame, Affdex identifies the probability $p_{video_{i}.label_{j}.t_{k}}$ from 0 to 100 (rescaled to 0 to 1 for the analysis) of the face as expressing one of the six "basic" emotion labels (*i.e.*, *anger*, *disgust*, *fear*, *happiness*, *surprise* and *sadness*) as well as additional psychological states such as *valence*, *engagement* or *contempt*, and facial features such as *cheek raise*, *eye widen* or *jaw drop*.

For both human and automatic recognition, to determine which of the six “basic” emotions can be used to identify each video, the recognition probability for each label by frame was converted into odd ratio by label [@dente2017measures]. The highest sum of each odd ratio time-series defines the label recognized by the automatic classifier (EQ\ref{eq:2} for human recognition and EQ\ref{eq:3} for automatic recognition). 

\begin{equation}
\label{eq:2}
video_{i}.label = \max\left(\frac{\sum_{k=1}^{n}x_{video_{i}.label_{j}.t_{k}}}{\sum_{k=1}^{n}x_{video_{i}.t_{k}}}\right)
\end{equation}

\begin{equation}
\label{eq:3}
video_{i}.label = \max\left(\frac{\sum_{k=1}^{f}p_{video_{i}.label_{j}.t_{k}}}{\sum_{k=1}^{f}p_{video_{i}.t_{k}}}\right)
\end{equation}

where *i* is one of the `r nrow(unique_video_count)` videos, *j* is one of the six “basic” emotion labels, *k* for each second of the *n* second video or for each frame of the *f* frame video.

# Results

Since encoders’ self-reports, human annotations and the automatic recognition include data on “non-basic” emotion labels and features, the analysis is performed using only the six "basic" emotion labels in order to compare them. The maximum score for self-reports, human annotations and automatic recognition is used to label the video. In case of more than one label obtaining the maximum value, the video is labeled as undetermined.

```{r corr-matrix}
list_video <- human_recognition_score$C_Video
list_emotion <- c("disgust","fear","happiness","surprise","sadness","anger")

grid_data <- expand.grid(list_video = list_video, list_emotion = list_emotion)

self_report_result <- self_report_score %>%
  dplyr::filter(sr_emotion != "undetermined") %>%
  dplyr::mutate(sr_score = 1)

human_recognition_result <- human_recognition_score %>%
  dplyr::filter(hr_emotion != "undetermined") %>%
  dplyr::mutate(hr_score = 1)

automatic_recognition_result <- automatic_recognition_score %>%
  dplyr::filter(ar_emotion != "undetermined") %>%
  dplyr::mutate(ar_score = 1)

recognition_result <- grid_data %>%
  dplyr::left_join(self_report_result, by = c(
    "list_video" = "C_Video",
    "list_emotion" = "sr_emotion"
    )
  ) %>%
  dplyr::left_join(human_recognition_result, by = c(
    "list_video" = "C_Video",
    "list_emotion" = "hr_emotion"
    )
  ) %>%
  dplyr::left_join(automatic_recognition_result, by = c(
    "list_video" = "C_Video",
    "list_emotion" = "ar_emotion"
    )
  ) %>%
  replace(is.na(.), 0)

corr_sr_hr <- cor.test(
  recognition_result$sr_score, 
  recognition_result$hr_score, 
  alternative = "two.sided"
  ) %>% 
  papaja::apa_print()

corr_sr_ar <- cor.test(
  recognition_result$sr_score, 
  recognition_result$ar_score, 
  alternative = "two.sided"
  ) %>% 
  papaja::apa_print()

corr_hr_ar <- cor.test(
  recognition_result$hr_score, 
  recognition_result$ar_score, 
  alternative = "two.sided"
  ) %>% 
  papaja::apa_print()
```

## Human Observers’ Accuracy

```{r sr_hr}
comparison_sr_hr <- dplyr::inner_join(self_report_score,human_recognition_score, by = "C_Video") %>% 
  dplyr::ungroup() %>% 
  dplyr::filter(hr_emotion != "undetermined") %>% 
  dplyr::filter(sr_emotion != "undetermined") %>% 
  dplyr::mutate_if(is.character,as.factor)

confusionMatrix_sr_hr <- caret::confusionMatrix(
  data = comparison_sr_hr$hr_emotion,
  reference = comparison_sr_hr$sr_emotion
  )
```

The overall correlation of recognition and non-recognition between self-reported emotions and human observers recognition is significant but low (`r corr_sr_hr$full_result`). In order to identify differences according to the emotional labels, encoders’ subjective feelings are compared with human observers’ recognition in a confusion matrix (Figure \ref{fig:confusionMatrix_sr_hr}).

```{r confusionMatrix_sr_hr, fig.height=6, fig.cap="\\label{fig:confusionMatrix_sr_hr}Confusion matrix between the emotion self-reported as being characteristic of the elicitation with the emotion recognized by the human observers."}
confusionMatrix_sr_hr_freq <- confusionMatrix_sr_hr$table %>% 
  as.data.frame() %>% 
  dplyr::rename(hr_emotion = Prediction, sr_emotion = Reference) %>% 
  dplyr::mutate(n_video = sum(Freq)) %>% 
  dplyr::mutate(prop_video = Freq/sum(Freq)) %>% 
  dplyr::group_by(sr_emotion) %>% 
  dplyr::mutate(n_sr = sum(Freq)) %>%
  dplyr::mutate(prop_sr = Freq/sum(Freq)) %>% 
  dplyr::ungroup()

confusionMatrix_sr_hr_freq %>% 
  ggplot(mapping = aes(x = sr_emotion, y = hr_emotion)) +
  geom_tile(aes(fill = prop_sr), colour = "white") +
  geom_text(
    aes(label = paste0(scales::percent(prop_sr), "\n(", Freq,"/",n_sr, ")")),
    color = "black",
    size = 4,
    family="serif",
    parse = FALSE,
    lineheight = 0.7)+
  scale_fill_gradient(
    name = "Proportion of Recognized\nSelf-Reported Emotion",
    low = "white", 
    high = "red", 
    labels = scales::percent,
    limits = c(0,1)
  ) +
  scale_x_discrete(name = "Self-Reports") +
  scale_y_discrete(name = "Human Observers") +
  theme_minimal() +
  theme(text = element_text(size=16,family="serif"),
        axis.text.x = element_text(size=16, angle = 45, hjust = 0.75,vjust=0.9),
        axis.text.y = element_text(size=16, hjust = 0.5),
        axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0)),
        legend.text = element_text(size=8),
        legend.position = "bottom")
```

Each emotion label used to describe encoders’ self-reported subjective feeling (*i.e.*, the label rated with the highest value) is compared with the emotion labels which were rated with the highest score by human observers. Results of the confusion matrix show a low agreement between the emotion felt by the encoder during the elicitation and the emotion recognized by the human observers (Accuracy \nolinebreak = \nolinebreak `r round(confusionMatrix_sr_hr$overall["Accuracy"],2)`, 95% CI [`r round(confusionMatrix_sr_hr$overall["AccuracyLower"],2)`,`r round(confusionMatrix_sr_hr$overall["AccuracyUpper"],2)`]; Kappa = `r round(confusionMatrix_sr_hr$overall["Kappa"],2)`) except for *disgust* (`r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "disgust" & confusionMatrix_sr_hr_freq$sr_emotion == "disgust", "prop_sr"]))` of the videos self-reported). These results are far from those classically obtained in the literature for emotional facial expression recognition which ranges between 60% and 80% accuracy. However these results are mostly obtained with static (*i.e.*, pictures) and posed (*i.e.*, displayed by actors) facial expressions using only 6 emotional labels in a forced-choice paradigm.

Interestingly human observers seem to recognize surprise expressed in videos where anger, fear happiness and sadness was the highest self-reported emotion (respectively `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "surprise" & confusionMatrix_sr_hr_freq$sr_emotion == "anger", "prop_sr"]))`, `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "surprise" & confusionMatrix_sr_hr_freq$sr_emotion == "fear", "prop_sr"]))`, `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "surprise" & confusionMatrix_sr_hr_freq$sr_emotion == "happiness", "prop_sr"]))` and `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "surprise" & confusionMatrix_sr_hr_freq$sr_emotion == "sadness", "prop_sr"]))` of the videos self-reported), and in a lower instance *happiness* was recognized in videos where *fear* and *surprise* was the highest self-reported emotion (respectively `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "happiness" & confusionMatrix_sr_hr_freq$sr_emotion == "fear", "prop_sr"]))` and `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "happiness" & confusionMatrix_sr_hr_freq$sr_emotion == "surprise", "prop_sr"]))` of the videos self-reported).

Sensitivity, specificity, precision and F1 scores for each emotion reveals that *happiness* has the highest coherence ratio whereas *sadness* has the lowest coherence ratio between true positives and false positives (Table \ref{table:confusionTable_sr_hr}).

```{r confusionTable_sr_hr, results="asis"}
confusionMatrix_sr_hr_table <- confusionMatrix_sr_hr %>%
  magrittr::use_series(byClass) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("Emotion") %>%
  dplyr::mutate(Emotion = gsub(pattern = "Class: ",replacement = "",x = Emotion)) %>% 
  dplyr::select(Emotion, Sensitivity, Specificity, Precision, F1) %>% 
  dplyr::mutate_if(is.numeric,round,2) %>% 
  replace(is.na(.), "\\textit{na.}")

confusionMatrix_sr_hr_table %>%
  knitr::kable(
    "latex",
    caption = "\\label{table:confusionTable_sr_hr}Human recognition accuracy metrics for each emotion.",
    booktabs = TRUE,
    digits = 2,
    linesep = "",
    escape = FALSE,
    align = c('l',rep('c', 4))
  ) %>%
  kableExtra::kable_styling(
    latex_options = "HOLD_position",
    font_size = 8,
    full_width = TRUE 
    ) %>%
   add_footnote("Note. \\textit{na.} values are produced when not enough data are available to compute accuracy indicators.", notation="none", escape = FALSE)
```

Accuracy metrics by emotional labels indicate a discrepancy in the ratio of true/false positives. Whereas *happiness* and *disgust* obtain the highest scores, *anger*, *surprise* and *sadness* have the lowest recognition ratio. The underlying effect of expression intensity may explain why *happiness* and *disgust* are easily recognized. *Anger* and *sadness* as non-socially desirable emotions may be have been felt but not expressed.

However, self-reports show a significant proportion of undetermined emotional states (`r scales::percent(nrow(list_automatic_ties)/358)` of the 358 videos) which reveals the potential limit of using 6-points Likert scales to measure emotional self-reports. Indeed, encoders can easily score to the maximum for more than one emotion.

## Automatic Classifier’s Accuracy

```{r sr_ar}
comparison_sr_ar <- dplyr::inner_join(self_report_score,automatic_recognition_score, by = "C_Video") %>% 
  dplyr::ungroup() %>% 
  dplyr::filter(ar_emotion != "undetermined") %>% 
  dplyr::filter(sr_emotion != "undetermined") %>% 
  dplyr::mutate_if(is.character,as.factor)

confusionMatrix_sr_ar <- caret::confusionMatrix(
  data = comparison_sr_ar$ar_emotion,
  reference = comparison_sr_ar$sr_emotion
  )
```

Similarly to the previous analysis, the overall correlation of recognition and non-recognition revealed a significant but very low coherence between self-reported emotions and automatic classifier’s recognition (`r corr_sr_ar$full_result`). A confusion matrix was used to compare encoders’ subjective feeling with the emotion label recognized by the automatic classifier (Figure \ref{fig:confusionMatrix_sr_ar}).

```{r confusionMatrix_sr_ar, fig.height=6, fig.cap="\\label{fig:confusionMatrix_sr_ar}Confusion matrix of between the emotion self-reported as being characteristic of the elicitation with the emotion recognized by the automatic classifier."}
confusionMatrix_sr_ar_freq <- confusionMatrix_sr_ar$table %>% 
  as.data.frame() %>% 
  dplyr::rename(ar_emotion = Prediction, sr_emotion = Reference) %>% 
  dplyr::mutate(n_video = sum(Freq)) %>% 
  dplyr::mutate(prop_video = Freq/sum(Freq)) %>% 
  dplyr::group_by(sr_emotion) %>% 
  dplyr::mutate(n_sr = sum(Freq)) %>%
  dplyr::mutate(prop_sr = Freq/sum(Freq)) %>% 
  dplyr::ungroup()

confusionMatrix_sr_ar_freq %>% 
  ggplot(mapping = aes(x = sr_emotion, y = ar_emotion)) +
  geom_tile(aes(fill = prop_sr), colour = "white") +
  geom_text(
    aes(label = paste0(scales::percent(prop_sr), "\n(", Freq,"/",n_sr, ")")),
    color = "black",
    size = 4,
    family="serif",
    parse = FALSE,
    lineheight = 0.7)+
  scale_fill_gradient(
    name = "Proportion of Recognized\nSelf-Reported Emotion",
    low = "white", 
    high = "red", 
    labels = scales::percent,
    limits = c(0,1)
  ) +
  scale_x_discrete(name = "Self-Reports") +
  scale_y_discrete(name = "Automatic Classifier") +
  theme_minimal() +
  theme(text = element_text(size=16,family="serif"),
        axis.text.x = element_text(size=16, angle = 45, hjust = 0.75,vjust=0.9),
        axis.text.y = element_text(size=16, hjust = 0.5),
        axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0)),
        legend.text = element_text(size=8),
        legend.position = "bottom")
```

Results obtained for the comparison between emotions self-reported and recognized by the automatic classifier are somewhat similar to the ones with human observers (Table \ref{table:confusionTable_sr_ar}). Overall, a low agreement between emotion self-reported and emotion recognized by the automatic classifier (Accuracy = `r round(confusionMatrix_sr_ar$overall["Accuracy"],2)`, 95% CI [`r round(confusionMatrix_sr_ar$overall["AccuracyLower"],2)`,`r round(confusionMatrix_sr_ar$overall["AccuracyUpper"],2)`]; Kappa = `r round(confusionMatrix_sr_ar$overall["Kappa"],2)`) except for *happiness* (`r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "happiness" & confusionMatrix_sr_ar_freq$sr_emotion == "happiness", "prop_sr"]))` of the video self-reported) is evident.

```{r confusionTable_sr_ar, results="asis"}
confusionMatrix_sr_ar_table <- confusionMatrix_sr_ar %>%
  magrittr::use_series(byClass) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("Emotion") %>%
  dplyr::mutate(Emotion = gsub(pattern = "Class: ",replacement = "",x = Emotion)) %>% 
  dplyr::select(Emotion, Sensitivity, Specificity, Precision, F1) %>% 
  dplyr::mutate_if(is.numeric,round,2) %>% 
  replace(is.na(.), "\\textit{na.}")

confusionMatrix_sr_ar_table %>%
  knitr::kable(
    "latex",
    caption = "\\label{table:confusionTable_sr_ar}Autonatic recognition accuracy metrics for each emotion.",
    booktabs = T,
    digits = 2,
    linesep = "",
    escape = FALSE,
    align = c('l',rep('c', 4))
  ) %>%
  kableExtra::kable_styling(
    latex_options = c("HOLD_position"),
    font_size = 8,
    full_width = TRUE
    ) %>%
   add_footnote("Note. \\textit{na.} values are produced when not enough data are available to compute accuracy indicators.", notation="none", escape = FALSE)
```

Surprisingly the automatic classifier incorrectly recognized as *disgust* an significant proportion of videos in which *anger*, *happiness* and *surprise* was the highest self-reported emotion (respectively `r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "disgust" & confusionMatrix_sr_ar_freq$sr_emotion == "anger", "prop_sr"]))`, `r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "disgust" & confusionMatrix_sr_ar_freq$sr_emotion == "happiness", "prop_sr"]))` and `r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "disgust" & confusionMatrix_sr_ar_freq$sr_emotion == "surprise", "prop_sr"]))` of the videos self-reported). In parallel, the automatic classifier recognized as *happiness* videos in which *fear* and *surprise* was the highest self-reported emotion (respectively `r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "happiness" & confusionMatrix_sr_ar_freq$sr_emotion == "fear", "prop_sr"]))` and `r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "disgust" & confusionMatrix_sr_ar_freq$sr_emotion == "surprise", "prop_sr"]))` of the videos self-reported).

A comparable explanation involving the amount of undetermined video based on self-reports can be provided, as the level of undetermined emotions are very high for the self- reports.

## Comparison Between Human and Automatic Recognition

As previously mentioned, human observers appear to be more accurate than the automatic classifier to recognize an individual's subjective feeling (human observers Accuracy = `r round(confusionMatrix_sr_hr$overall["Accuracy"],2)`; automatic classifier Accuracy = `r round(confusionMatrix_sr_ar$overall["Accuracy"],2)`; `r corr_hr_ar$full_result`). However, both make mistakes.

A third confusion matrix is used to compare similarities (diagonal) and differences between human observers and automatic classifier in classifying the six emotion labels (Figure \ref{fig:confusionMatrix_hr_ar}).

```{r confusionMatrix_hr_ar, fig.height=6, fig.cap="\\label{fig:confusionMatrix_hr_ar}Proportion of emotion labels classified by human observers which are recognized by the automatif classifier."}
comparison_hr_ar <- dplyr::inner_join(human_recognition_score,automatic_recognition_score, by = "C_Video") %>% 
  dplyr::ungroup() %>% 
  dplyr::filter(ar_emotion != "undetermined") %>% 
  dplyr::filter(hr_emotion != "undetermined") %>% 
  dplyr::mutate_if(is.character,as.factor)

confusionMatrix_hr_ar <- caret::confusionMatrix(
  data = comparison_hr_ar$ar_emotion,
  reference = comparison_hr_ar$hr_emotion
)

confusionMatrix_hr_ar_freq <- confusionMatrix_hr_ar$table %>% 
  as.data.frame() %>% 
  dplyr::rename(ar_emotion = Prediction, hr_emotion = Reference) %>% 
  dplyr::mutate(n_video = sum(Freq)) %>% 
  dplyr::mutate(prop_video = Freq/sum(Freq)) %>% 
  dplyr::group_by(hr_emotion) %>% 
  dplyr::mutate(n_hr = sum(Freq)) %>%
  dplyr::mutate(prop_hr = Freq/sum(Freq)) %>% 
  dplyr::ungroup()

confusionMatrix_hr_ar_freq %>% 
  ggplot(mapping = aes(x = hr_emotion, y = ar_emotion)) +
  geom_tile(aes(fill = prop_hr), colour = "white") +
  geom_text(
    aes(label = paste0(scales::percent(prop_hr), "\n(", Freq,"/",n_hr, ")")),
    color = "black",
    size = 4,
    family="serif",
    parse = FALSE,
    lineheight = 0.7)+
  scale_fill_gradient(
    name = "Proportion Recognized Emotion",
    low = "white", 
    high = "red", 
    labels = scales::percent,
    limits = c(0,1)
  ) +
  scale_x_discrete(name = "Human Observers") +
  scale_y_discrete(name = "Automatic Classifier") +
  theme_minimal() +
  theme(text = element_text(size=16,family="serif"),
        axis.text.x = element_text(size=16, angle = 45, hjust = 0.75,vjust=0.9),
        axis.text.y = element_text(size=16, hjust = 0.5),
        axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0)),
        legend.text = element_text(size=8),
        legend.position = "bottom")
```

The overall agreement between human observers and the automatic classifier is in fact very low (Kappa = `r round(confusionMatrix_hr_ar$overall["Kappa"],2)`). Except for *happiness* and *disgust* (respectively `r scales::percent(as.numeric(confusionMatrix_hr_ar_freq[confusionMatrix_hr_ar_freq$ar_emotion == "happiness" & confusionMatrix_hr_ar_freq$hr_emotion == "happiness", "prop_hr"]))` and `r scales::percent(as.numeric(confusionMatrix_hr_ar_freq[confusionMatrix_hr_ar_freq$ar_emotion == "disgust" & confusionMatrix_hr_ar_freq$hr_emotion == "disgust", "prop_hr"]))` of common labelling), there is no clear common pattern. Moreover, the automatic classifier has a tendency to label as *disgust* videos labeled as *sadness* by human observers, and as *happiness* videos labeled as *fear* by human observers.

## Discussion and Conclusion

Despite being one of the most investigated questions in affective science, the consistency between emotion felt and facially displayed on the one hand, and facial expression recognized on the other is a hot topic. To date no clear evidence has been found to definitively solve the questions raised. Yet, with the growing interest of industries and government in monitoring individual’s psychological states, this issue is under intense scrutiny. The present research aims to provide some empirical data to answer some of the questions posed. The faces encoders spontaneously displayed when confronted with an emotional eliciting task were submitted both to human and to automatic recognition. The criterion for recognition accuracy was the subjective feeling self-reported by the encoder once the elicitation task was carried out. Results first reveal a low consistency between emotion felt and facial expression displayed. They show that facial expressions of emotion are often not displayed when the Basic Emotion View would predict them to be expressed. Secondly, results show low accuracy rates for both humans and the automatic classifier in identifying the inner emotional states of these encoders based on their facial expressions. Thirdly, human observers prove to be better at recognizing the emotion facially expressed than the automatic recognition tool is. Such results support the hypothesis advanced by some authors of low emotion–expression coherence (Kappas 2003). In many instances, facial displays are not associated with a concordant emotional state, even any emotional state at all (Bonanno & Keltner 2004; Fernández-Dols & Crivelli, 2013]. More and more evidence is showing that facial expressions are in reality not expressing emotions (McKeown 2013]. Increasing studies show that, for most emotions, the EFE elicited by emotional triggers are scarce and partial, even when micro-expressions are taken into account (Dúran et al., 2017). These studies are conducted either in laboratory settings or in the field. This is not to say that facial expression is not an informative modality for understanding the emotional state of a person. There is indeed an affinity between emotion and facial display (Frijda & Tcherkassof, 1997). However, claiming that there are unique “signatures” allowing using configurations of facial muscles to infer the presence of a specific emotion is misleading. As well as other nonverbal behaviors, facial movements are not only assumed to be determined by emotion but also by various other causes, such as psychological states (e.g., motivations or pain), to say nothing of social context and sociocultural norms (Ekman et al. 1987). Hence, facial movements have causes and functions other than the expression of emotion. This multiple determination excludes any possibility of drawing a linear inference from facial activity on the underlying psychological state (emotional or other). Beyond the present observations showing a weak consistency between subjective feelings and spontaneous facial expressions, this study sheds some light on the controversy between the Basic Emotion View and the constructivist approach as to the facial recognition issue. The former (Ekman 1992) assumes that expressions of emotion are brief and coherent patterns of facial muscle movements that co-vary with discrete subjective experiences. In return, this information displayed by the face corresponds to the one extracted by perceivers. Instead of viewing emotions as natural kinds (Barrett, 2006), the constructivist approach supposes that emotions are social constructions and that facial behaviors intrinsically situated. The emotions that are recognized by the observer are constructed in her/his mind. Therefore, facial movements do not express specific emotions because they do not carry intrinsic emotional signification. It is the observer that infers the emotional meaning of the facial expression, this interpretation depending on availability of different kinds of information (contextual information, linguistic categories…). As a consequence, one can predict from the first line of thinking that individuals’ emotional subjective feeling should be correlated to the recognition of facial expressions from both human observers and automatic classifiers whereas if emotions are social constructs, as stated by the second line of thinking, human observer’s should be better at perceiving emotions expressed on the face than automatic classifiers.

The present findings speak against any strong version of the Basic Emotion View. The correlations between the self-measured emotions and the observed facial behaviors are low and the latter are weakly recognized. Present results plead instead in favor of the constructivist stance. They show that human observers are more accurate than the automatic recognition tool to identify an individual’s subjective feeling on the basis of their face. Moreover, mistakes made by human observers look less arbitrary to the ones made by the classifier. For instance, even if a mix-up between disgust and anger is sometimes reported in recognition studies, odd confusions such as the present ones produced by the classifier have never been noted for human observers. The latter obviously make sense of the facial behavior they are witnessing. However, the human decoders in the present study were presented with faces without any contextual information which could have helped them to shape more precisely their interpretation. Hassin, Aviezer and Bentin (2013) reviewed evidence to demonstrate that faces are inherently ambiguous and that observers rely on situational cues when they process facial displays (see also Aviezer, Hassin, Ryan, Grady, Susskind, Anderson, Moscovitch & Bentin, 2008; Barrett, Mesquita, Gendron, 2011). It happens that contextual information even shifts emotion perception. In the present case, the facial stimuli displayed to the decoders were as equivocal as are real-life facial expressions (Aviezer, Ensenberg, & Hassin, 2017). Yet, decoders had no cue at disposal. So, without any possibility of integrating faces and context, their decoding accuracy is pretty much degraded as it is usual in such cases (e.g., Wagner, MacDonald, & Manstead, 1986). However, they demonstrate their superiority on the classifier tool probably thanks to their capacity to rely on their previous personal experience to invent a context in which a face could display such an expression. Once a credible context retrieved, they can affix an emotional label to the facial behavior.

Several limitations should be stated, highlighting the need for further research. One of them is effectiveness of the emotion elicitation tasks. One can consider that an intensity threshold needs to be exceeded for a visible emotional expression to occur. In the case of the present study, it may be that insufficient emotion intensity accounts for the low number of visibly reactive participants. However, such line of reasoning fits badly with the Basic Emotion View according to which emotions and their related prototypical facial behavior are universal because they are considered as innate mechanisms allowing individuals to respond adaptively to evolutionary significant events (threats, opportunities…) encountered in the environment, whatever their potency. Moreover, various studies have used fairly strong emotion inductions without obtaining any visible facial display of any kind of emotion (Dúran et al, 2017). Besides, the present results show a quite good correlation between reported emotion and facial behavior for happiness. Thus, there are no reasons to believe that an explanation in terms of a too low emotion threshold applies for the other emotions under consideration. The latter were triggered and measured with conceptually identical elicitation and assessment procedures (Reisenzein et al., 2013). The use of self-reports to evaluate encoders’ subjective feelings is another limitation that can also be put forward because of the numerous cognitive biases they entail. Well known problems with the reliability of self-reports are, among others, the reconstructive nature of memory, the influence of attentional biases on reports, demand characteristics, distorting effects of implicit causal theories and personal motives, as stressed by Nielsen & Kaszniak (2007). It is clearly obvious that self-reports are not simple outlets of inner mental processes but personal constructions and they are affected by many factors (Kappas, 2003). The emotional feeling echoes motivational tendencies, bodily changes, and cognitive appraisals of events (Sander & Scherer, 2009). All these are encapsulated into semantic categories referred to by labels. As things stand currently, there is unfortunately no objective way for accessing and assessing inner subjective emotional feelings except for asking people to report their subjective experience in words. The procedure used for human recognition can also be open to dispute. First, one can criticize the decoders' expertise level since they were not FACS coders but untrained students. Of course one can expect a difference between skilled annotators and novice ones regarding the assessment of emotional facial displays. This said, in everyday life, few people are specialist coders yet the quantity of successful social interactions proves lay persons recognize pretty well others’ facial behaviors. Therefore, and especially in the interest of generalizability, asking inexpert people seems relevant. Secondly, instead of using a classic forced-choice procedure, a more subtle approach was chosen to mimic results provided by the automatic classifier. As explained above, annotators first delimitated a temporal sequence during which they noticed an emotional display on the face, and then attributed an emotion label to this behavioral sequence in a second step. Whereas this paradigm is longer and more complicated, it can lead to more robust results in reducing the forced-choice bias (Russell, 1993). However this procedure can also reduce the human observers’ accuracy. In this regard, the results of the human observation could have been more ambiguous because it is not the natural way that people are inferring meaning from facial expressions. An alternative explanation relies in reducing the recognition bias involved in the classic recognition paradigm. Classic forced-choice paradigms obtain artificially high results, thus by using a more evolved approach observers’ accuracy may have been lowered. Another flaw is the lack of comparison with various facial expression recognition methods. Human recognition has only been compared to the Affdex classifier. Future studies are needed to confront human assessments with different automatic recognition methods, both frame based methods and sequence based automatic ones. This latter issue is particularly decisive. Indeed, the issue of the recognition of dynamic expressive sequences is essential because ordinary facial behavior is made up of dynamically shifting morphological features (Krumhuber, Kappas & Manstead, 2013). This temporal information is indisputably a key feature of facial activity. Not only observer-based judgements of facial displays must be compared to automated facial analysis, but also different kinds of human recognition measurements should be undertaken. The challenge researchers are especially confronted with is to find ways to appropriately collect data regarding the perception of spontaneous and dynamic facial behavior. Finally, our understanding of facial displays as they occur in everyday interactions requires a strong emphasis on ecological concerns. The present study is a laboratory experiment.  It has the advantage of controlling different parameters of the emotions investigated, such as intensity, quality, and temporal (onset, duration) features. Moreover, as encoders are alone, facing an emotional trigger, it also controls for the social context, removing its possible influence on their facial behaviors. However, it is known that encoders’ imagination can influence their expressiveness, for instance when they believe that their friend is doing the same (vs a different) emotional task in another room (Jakobs, Manstead, & Fischer, 1999). Trying to exclude social influence by leaving encoders alone may be illusionary then. From an ecological perspective, it is even a mistake to exonerate behavioral observations from social contexts.  Facial activity measurement in dyadic interactions has shown that the facial behavior of the perceiver reflects sometimes more what the expresser is experiencing than what the perceiver is feeling. It is the case, for instance, of emotional mimicry in dynamic social interactions (Hess & Fischer, 2016).  It is also the case of healthy partners interacting with schizophrenic patients whose facial activity is pretty much identical (Krause, Steimer-Krause, Merten & Burkhard, 1998). Hence, in order to better comprehend emotional communication in human relationships, experimental research should be corroborated with more ecological protocols.

Nowadays, automatic recognition systems are based on the coding of the facial muscular activations from which they infer the expressed emotion. Such automatic classifier tools take for granted that, when experienced, firstly an emotion is displayed on the face, secondly, in the form of a configuration of facial muscles that is all his own, and thirdly, which is recognized by the perceiver (be it a human being or an automatic system). These are the Basic Emotion View assertions, jeopardized by field observations and laboratory experiments on spontaneous expression of emotions, such as the present study. All raise serious objections to the supposed close relation between emotion and face. They bring up several questions regarding the role the context plays in the emission and interpretation of the so-called facial expression of emotion. The finalization of operational and effective “reading emotional faces” devices rests on their answer. As a result, despite being one of the most investigated questions in affective science, the growing interest of industries and governments in tracking individual’s psychological states is hardly given satisfaction. Considering the above, the present results provide additional evidence that an individual’s subjective feeling cannot be inferred from facial expressions and in our case invalidate the hypothesis of hardwired emotions unambiguously displayed on the face. Even if emotions were hardwired, in everyday life one does not observe prototypical facial expressions because of their rarity and therefore research should be focused on analyzing non-prototypical facial expressions. Advancements in identifying “non-basic” emotion labels (McDuff, 2016) as well as non-prototypical facial expression have indeed been made in the development of automatic facial expression recognition tools. However, these present results suggest that automatic facial expression recognition tools should merely evaluate facial morphology features such as action units (already evaluated in OpenFace, Baltrušaitis, Robinson, & Morency, 2016, Affectiva’s Affdex, McDuff, Mahmoud, Mavadati, Amr, Turcot, & el Kaliouby, 2016, or Vicar Vision’s FaceReader, Den Uyl & Van Kuilenburg, 2005, to name a few) rather than inferring supposedly emotional or affective states. Trying to interpret facial displays as a means of determining underlying emotional state, in all likelihood, remains vain. 

\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
