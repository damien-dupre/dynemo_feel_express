---
title: "Coherence between subjective feeling and facial expression: a time-series analysis of human and automatic recognition."
output: word_document
bibliography: mybibfile.bib
abstract: |
  While it has been taken for granted in the development of several automatic facial expression recognition tools, the question of the link between subjective feelings and facial expressions is still a subject of debate. On one hand the behaviorist approach conceives emotions as genetically hardwired and therefore being genuinely displayed through facial expressions. On the other hand the constructivist approach conceives emotions are socially constructed and facial expression as social messages that are not related to emotions. In order to evaluate the link between the subjective feeling of emotions and their recognition based on facial expression, 232 videos of participants recruited to perform an emotion elicitation task were annotated by 1383 human observers as well as by an automatic facial expression classifier. The results show a low accuracy of human observers and of the automatic classifier to infer from the facial expression the subjective feeling of the participants recorded. Based on these results, the hypothesis of genetically hardwired emotion genuinely display is difficult to support whereas the idea of emotion socially constructed and facial expression as display of social messages appears to be more likely. Then, the way to infer emotional and mental state based on facial expressions should be questioned.
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  cache.extra = knitr::rand_seed, 
  message = FALSE, 
  warning = FALSE, 
  error = FALSE, 
  echo = FALSE,
  fig.pos='H')
#
set.seed(123)# Seed for random number generation
options(scipen = 999)# Disable scientific number format
#
library(kableExtra)
library(tidyverse) # Data wrangling & grammar of graphics
library(here) # Locate files relative to project root
library(caret)
library(plotROC)
library(pROC)
library(pander)
library(papaja)
library(captioner)
table_nums <- captioner::captioner(prefix = "Table")
table_nums(
  name = "confusionTable_sr_hr", 
  caption = "Agreement accuracy metrics for each emotion.")
table_nums(
  name = "confusionTable_sr_ar", 
  caption = "Agreement accuracy metrics for each emotion.")
#
fig_nums <- captioner::captioner()
fig_nums(
  name = "dynemo_img", 
  caption = "Example of a front facing recording synced with the full view of the participant and the elicitation task. This picture is taken from a pilot with projects collaborators and all gave a consent for the publication of their photos and videos.")
fig_nums(
  name = "confusionMatrix_sr_hr", 
  caption = "Confusion matrix of between the emotion self-reported as being characteristic of the elicitation with the emotion recognized by the human observers.")
fig_nums(
  name = "confusionMatrix_sr_ar", 
  caption = "Confusion matrix of between the emotion self-reported as being characteristic of the elicitation with the emotion recognized by the automatic classifier.")
fig_nums(
  name = "confusionMatrix_hr_ar", 
  caption = "Confusion matrix of between the human obersers and automatic classifier recognition.")
```

```{r data}
# self_report_data
source(here::here("script/data_wrangling_self_report.R"))
self_report_score <- self_report_score %>% 
  dplyr::select(C_Video, sr_emotion = emotion, sr_score = value)
# human_recognition_data
source(here::here("script/data_wrangling_human_recognition.R"))
human_recognition_score <- human_recognition_score %>% 
  dplyr::select(C_Video, hr_emotion = emotion, hr_score = confidence_score)
# automatic_recognition_data
source(here::here("script/data_wrangling_automatic_recognition.R"))
automatic_recognition_score <- automatic_recognition_score %>% 
  dplyr::select(C_Video, ar_emotion = emotion, ar_score = confidence_score) %>% 
  dplyr::filter(C_Video %in% ppt_per_video$C_Video)
```

Introduction
=============

With the development of commercial automatic facial expression recognition tools [see @dupre2018accuracy for a non-exhaustive list of available tools], industries and governments are gradually implementing this technology in order to track humans' emotions in various scenarios (e.g., marketing, healthcare, automotive to name a few). This technoloogy rests on the premise that facial expressions provide a direct access to individuals' subjective feeling. Even if this premise is central to the modern mainstream approach of human emotion, recent research in affective science are challenging it. After presenting the arguments supporting both sides, an experiment testing these hypotheses will be presented and its results analyzed in order to provide empirical evidence to contribute to answer the question.

## The link between emotion and facial expression through the behaviorist approach
Based on the behaviorist approach initiated by Darwin in *The Expression of the Emotions in Man and Animals* [@darwin1872expression], facial expression are conceived as a genuine display of individuals inner emotional state. This hypothesis is used as a basis for the Basic Emotion Theory (BET) which states that a set of six emotions are universally displayed and are genetically hardwired not only in humans [@ekman1992argument] but also in different animal species [@de2019mama]. According to this view, "when emotions are aroused by perception of a social event, a set of central commands produce patterned emotion-specific changes in multiple systems, including [...] facial expressions." [@ekman2007directed, p49]. To cope with critics, several amendments have been made to the BET, increasing the number of basic emotions from six to seven  [@ekman1988universality] as well as adding the concept of "display rules" to explain cultural differences in the management of facial expressions [@ekman1987universals]. Even if this theory obtained a popular support, it fails to explain how individuals can feel emotions without expressing them and how individuals can express emotions without feeling them, cases for which display rules cannot be applied to [@kraut1979social, @duran2017coherence].

## The link between emotion and facial expression through the social constructivist approach
Detractors of the Basic Emotion Theory are perceiving emotion not as genetically hardwired but as a learnt association between a given situation and an appropriate response [@averill1980constructivist, @barrett2017emotions]. For the tenants of the constructivist approach, emotions are "concepts" based on past experiences and which are "a collection of embodied, whole brain representations that predict what is about to happen in the sensory environment, what the best action is to deal with impending events, and their consequences for allostasis" [@barrett2017theory, p12]. Following this assumption, faces are used as tools to display signals in social interactions [@crivelli2018facial]. These signals can convey individuals' motivations and readiness [@frijda1997facial] or social messages [@fridlund1995human]. Therefore, facial expressions are thought as behaviors which meaning is inferred by the observer. 

This current paper investigates the link between the subjective feeling of emotions and their recognition from facial expressions. If emotions are hardwired, individuals emotional subjective feeling should be correlated to the recognition of facial expressions from both human observers and automatic classifiers whereas if emotions are social constructs, no correlation between subjective feeling and facial expression recognition should be observed.

Method
=============

To evaluate the link between subjective feeling of emotions and their recognition from facial expressions, participants were recruited to perform an emotion elicitation task while their facial expression was video recorded. Then, the videos was shown to human observer and analysed by an automatic classifier in order to identify which emotion was displayed.

## Emotion Elicitation

```{r self_report_method}
self_report_gender <- self_report_data %>% 
  dplyr::group_by(genre_c) %>% 
  dplyr::summarise(n= n())

self_report_age <- self_report_data %>%
  dplyr::summarise(Age_m = mean(age), Age_sd = sd(age)) %>% 
  dplyr::mutate_all(round,1)
```

For the emotion elicitation experiment, `r nrow(self_report_data)` French participants (`r self_report_gender[self_report_gender$genre_c == "F","n"]` females, `r self_report_gender[self_report_gender$genre_c == "H","n"]` males, *M*age = `r self_report_age[,"Age_m"]`, *SD*age = `r self_report_age[,"Age_sd"]`) were recruited to perform one out of 11 emotion elicitation tasks designed to trigger a positive, negative or neutral emotional state. Participants' face were recorded using an hidden camera resulting `r nrow(self_report_data)` front facing 768x576 videos varying from `r min(metadata_video$ffprobe_duration)`s to `r max(metadata_video$ffprobe_duration)`s (`r fig_nums("dynemo_img", display = "cite")`). These recordings are constituting the DynEmo database [see @tcherkassof2013dynemo for a full description of tasks and procedure].

```{r dynemo_img, fig.cap=fig_nums("dynemo_img")}
knitr::include_graphics(here::here("./img/dynemo_img.jpg"))
```

After their emotion elicitation task the participants had to rate their subjective feeling during the task on likert scales from 0 ("not at all") to 5 ("strongly") according six "basic" emotion labels (i.e., *anger*, *disgust*, *fear*, *happiness*, *surprise* and *sadness*) as well as six "non-basic" emotion labels (i.e., *pride*, *curiosity*, *boredom*, *shame*, *humiliation*, and *disappointment*).

Finally, a debriefing session was perform to ensure that participants were not durably affected by the emotion elicitation task. The debriefing was also used to check that participants did not guess the real purpose of the experiment (e.g. being filmed while they were performing an emotional elicitation task) to guarantee facial expressions genuineness. All the participants gave their agreement on their data and video to be processed for research purpose only.

## Human Facial Expression Recognition

```{r human_recognition_method}
unique_ppt_count <- human_recognition_data %>% 
  dplyr::group_by(source,SEXE_Juge, C_Juge) %>% 
  dplyr::summarise(n = n())

unique_video_count <- human_recognition_data %>% 
  dplyr::select(source,SEXE_Juge, C_Juge, C_Video) %>% 
  unique() %>% 
  dplyr::group_by(C_Video) %>% 
  dplyr::summarise(n = n())
```

For the human facial expression recognition method, `r nrow(unique_ppt_count)` student participants were recruited to annotate `r nrow(unique_video_count)` out of the `r nrow(self_report_data)` video, therefore only the `r nrow(unique_video_count)` annotated videos will be analysed in this paper. Each participants had to annotate between `r min(unique_ppt_count$n)` and `r max(unique_ppt_count$n)` videos resulting that each video was annotated `r mean(unique_video_count$n) %>% round(0)` times on average (*SD* = `r sd(unique_video_count$n) %>% round(0)`).

The annotation of facial expressions was performed on-site using *Oudjat*, a software for designing video annotation experiments [@dupre2015oudjat]. For each video, the annotation procedure hat two steps. First, the participants had to identify the emotional sequences by pressing the space bar of their keyboard to indicate the beginning and the end of the emotional sequences while watching the video. Second, the participants watched each emotional sequence previously identified and had to label the sequence using one of the 12 emotions proposed including six "basic" emotion labels (i.e., *anger*, *disgust*, *fear*, *happiness*, *surprise* and *sadness*) and six "non-basic" emotion labels (i.e., *pride*, *curiosity*, *boredom*, *shame*, *humiliation*, and *disappointment*). They also had the possibility to indicate that the sequence was not expressing one of the proposed emotion. 

This annotation procedure results in a uni-dimensional time-series for each video per human observer identifying for each second of the video which emotion was recognized. Then time-series corresponding to the same video were aggregated to calculate the proportion of human observers for each second of the video per emotional label (EQ1).

$$x_{video_{i}.label_{i}.t_{i}} = \frac{n_{video_{i}.label_{i}.t_{i}}}{n_{video_{i}}}$$

The sum of each label proportion per second was used as a score to determine which labels corresponds to the overall video (i.e., the highest score). 

$$x_{video.label.t_{i}} = \frac{n_{video.label.t_{i}}}{n_{video}}$$

In case of more than one label having the maximum value, the emotion is described as undetermined.

## Automatic Facial Expression Recognition

The `r nrow(unique_video_count)` annotated video were processed with Affdex (SDK v3.4.1). Affdex is an automatic facial expression recognition classifier developed and distributed by Affectiva is a spin-off company resulting from the research activities of MIT media lab created in 2009 [@mcduff2016affdex]. Affdex’s algorithm uses Histogram of Oriented Gradient (HOG) features and Support Vector Machine (SVM) classifiers in order to recognize facial expressions. For each video frame, Affdex identify the probability of the face as expressing one of the six "basic" emotion labels (i.e., *anger*, *disgust*, *fear*, *happiness*, *surprise* and *sadness*) as well as additional psychological states such as *valence*, *engagement* or *contempt*, and facial features such as *cheek raise*, *eye widen* or *jaw drop*.

To determine which of the six "basic" emotion can be used to identify each video, the recognition probability for each label by frame was converted into odd ratio by frame [@dente2017measures]. The highest sum of each odd ratio time-series defines the label recognized by the automatic classifier.

Results
============

Whereas the self-reports, the human observation and the automatic recognition include data on "non-basic" emotion labels and features, the analysis is performed using only the six "basic" emotion labels in order to compare them. The maximum score for self-reports, human annotations and automatic recognition is used to label the video. In case of more than one label obtaining the maximum value, the video is labeled as undetermined.

## Correlation between self-report and human facial expression recognition

```{r sr_hr}
comparison_sr_hr <- dplyr::inner_join(self_report_score,human_recognition_score, by = "C_Video") %>% 
  dplyr::ungroup() %>% 
  dplyr::filter(hr_emotion != "undetermined") %>% 
  dplyr::filter(sr_emotion != "undetermined") %>% 
  dplyr::mutate_if(is.character,as.factor)

confusionMatrix_sr_hr <- caret::confusionMatrix(
  data = comparison_sr_hr$hr_emotion,
  reference = comparison_sr_hr$sr_emotion
  )
```

Participants' subjective feeling is compared with human observers recognition in a confusion matrix (`r fig_nums("confusionMatrix_sr_hr", display = "cite")`). Each emotion label used to describe participants subjective feeling (i.e., the label rated with the highest value) are compared with the emotion labels which was rated with the highest score by human observers

```{r confusionMatrix_sr_hr, fig.height=4, fig.cap=fig_nums("confusionMatrix_sr_hr")}
confusionMatrix_sr_hr_freq <- confusionMatrix_sr_hr$table %>% 
  as.data.frame() %>% 
  dplyr::rename(hr_emotion = Prediction, sr_emotion = Reference) %>% 
  dplyr::mutate(n_video = sum(Freq)) %>% 
  dplyr::mutate(prop_video = Freq/sum(Freq)) %>% 
  dplyr::group_by(sr_emotion) %>% 
  dplyr::mutate(n_sr = sum(Freq)) %>%
  dplyr::mutate(prop_sr = Freq/sum(Freq)) %>% 
  dplyr::ungroup()

confusionMatrix_sr_hr_freq %>% 
  ggplot(mapping = aes(x = sr_emotion, y = hr_emotion)) +
  geom_tile(aes(fill = prop_sr), colour = "white") +
  geom_text(
    aes(label = paste0(scales::percent(prop_sr), "\n(", Freq,"/",n_sr, ")")),
    color = "black",
    size = 4,
    family="serif",
    parse = FALSE,
    lineheight = 0.7)+
  scale_fill_gradient(
    name = "Proportion of Recognized\nSelf-Reported Emotion",
    low = "white", 
    high = "red", 
    labels = scales::percent,
    limits = c(0,1)
  ) +
  scale_x_discrete(name = "Self-Reports") +
  scale_y_discrete(name = "Human Observers") +
  theme_minimal() +
  theme(text = element_text(size=16,family="serif"),
        axis.text.x = element_text(size=16, angle = 45, hjust = 0.75,vjust=0.9),
        axis.text.y = element_text(size=16, hjust = 0.5),
        axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0)),
        legend.text = element_text(size=8),
        legend.position = "bottom")
```

The result of the confusion matrix show a moderate agreement between emotion felt during the elicitation and emotion recognized by the human observers (Accuracy = `r round(confusionMatrix_sr_hr$overall["Accuracy"],2)`, 95%CI[`r round(confusionMatrix_sr_hr$overall["AccuracyLower"],2)`,`r round(confusionMatrix_sr_hr$overall["AccuracyUpper"],2)`]; Kappa = `r round(confusionMatrix_sr_hr$overall["Kappa"],2)`) except for *disgust* (`r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "disgust" & confusionMatrix_sr_hr_freq$sr_emotion == "disgust", "prop_sr"]))` of the videos self-reported). Sensitivity, specificity, precision and F1 score for each emotion reveals that *happiness* has the highest coherence ratio whereas *sadness* has the lowest coherence ratio between true positives and false positives `r table_nums("confusionTable_sr_hr", display = "cite")`. Interestingly human observers seem to recognize *surprise* expressed in videos where *anger*, *fear* *happiness* and *sadness* was the highest self-reported emotion (respectively `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "surprise" & confusionMatrix_sr_hr_freq$sr_emotion == "anger", "prop_sr"]))`, `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "surprise" & confusionMatrix_sr_hr_freq$sr_emotion == "fear", "prop_sr"]))`, `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "surprise" & confusionMatrix_sr_hr_freq$sr_emotion == "happiness", "prop_sr"]))` and `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "surprise" & confusionMatrix_sr_hr_freq$sr_emotion == "sadness", "prop_sr"]))` of the videos self-reported), and in a lower instance *happiness* was recognized in videos where *fear* and *surprise* was the highest self-reported emotion (respectively `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "happiness" & confusionMatrix_sr_hr_freq$sr_emotion == "fear", "prop_sr"]))` and `r scales::percent(as.numeric(confusionMatrix_sr_hr_freq[confusionMatrix_sr_hr_freq$hr_emotion == "happiness" & confusionMatrix_sr_hr_freq$sr_emotion == "surprise", "prop_sr"]))` of the videos self-reported).

```{r confusionTable_sr_hr, results="asis"}
confusionMatrix_sr_hr_table <- confusionMatrix_sr_hr %>%
  magrittr::use_series(byClass) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("Emotion") %>%
  dplyr::mutate(Emotion = gsub(pattern = "Class: ",replacement = "",x = Emotion)) %>% 
  dplyr::select(Emotion, Sensitivity, Specificity, Precision, F1) %>% 
  dplyr::mutate_if(is.numeric,round,2) %>% 
  replace(is.na(.), "\\textit{na.}")

# confusionMatrix_sr_hr_table %>%
#   knitr::kable(
#     "latex",
#     caption = "Agreement accuracy metrics for each emotion. ",
#     booktabs = TRUE,
#     digits = 2,
#     linesep = "",
#     escape = FALSE,
#     align = c('l',rep('c', 4))
#   ) %>%
#   kableExtra::kable_styling(
#     font_size = 8,
#     full_width = TRUE,
#     latex_options = c("hold_position")) %>%
#    add_footnote("Note. \\textit{na.} values are produced when not enough data are available to compute accuracy indicators.", notation="none", escape = FALSE)

confusionMatrix_sr_hr_table %>%
  papaja::apa_table(
    caption = table_nums("confusionTable_sr_hr"),
    row.names = FALSE,
    format = "word",
    escape = FALSE
    )
```

However, the self-report show a very high proportion of undetermined emotional states which reveals the potential limit of using 6-points likert scales for which the participants can easily score to the maximum for more than one emotion.

## Correlation between self-report and automatic facial expression recognition

```{r sr_ar}
comparison_sr_ar <- dplyr::inner_join(self_report_score,automatic_recognition_score, by = "C_Video") %>% 
  dplyr::ungroup() %>% 
  dplyr::filter(ar_emotion != "undetermined") %>% 
  dplyr::filter(sr_emotion != "undetermined") %>% 
  dplyr::mutate_if(is.character,as.factor)

confusionMatrix_sr_ar <- caret::confusionMatrix(
  data = comparison_sr_ar$ar_emotion,
  reference = comparison_sr_ar$sr_emotion
  )
```

Similarly to the previous analysis, a confusion matrix was used to compare participants subjective feeling with the emotion label recognized by the automatic classifier (`r fig_nums("confusionMatrix_sr_ar", display = "cite")`).

```{r confusionMatrix_sr_ar, fig.height=4, fig.cap=fig_nums("confusionMatrix_sr_ar")}
confusionMatrix_sr_ar_freq <- confusionMatrix_sr_ar$table %>% 
  as.data.frame() %>% 
  dplyr::rename(ar_emotion = Prediction, sr_emotion = Reference) %>% 
  dplyr::mutate(n_video = sum(Freq)) %>% 
  dplyr::mutate(prop_video = Freq/sum(Freq)) %>% 
  dplyr::group_by(sr_emotion) %>% 
  dplyr::mutate(n_sr = sum(Freq)) %>%
  dplyr::mutate(prop_sr = Freq/sum(Freq)) %>% 
  dplyr::ungroup()

confusionMatrix_sr_ar_freq %>% 
  ggplot(mapping = aes(x = sr_emotion, y = ar_emotion)) +
  geom_tile(aes(fill = prop_sr), colour = "white") +
  geom_text(
    aes(label = paste0(scales::percent(prop_sr), "\n(", Freq,"/",n_sr, ")")),
    color = "black",
    size = 4,
    family="serif",
    parse = FALSE,
    lineheight = 0.7)+
  scale_fill_gradient(
    name = "Proportion of Recognized\nSelf-Reported Emotion",
    low = "white", 
    high = "red", 
    labels = scales::percent,
    limits = c(0,1)
  ) +
  scale_x_discrete(name = "Self-Reports") +
  scale_y_discrete(name = "Automatic Classifier") +
  theme_minimal() +
  theme(text = element_text(size=16,family="serif"),
        axis.text.x = element_text(size=16, angle = 45, hjust = 0.75,vjust=0.9),
        axis.text.y = element_text(size=16, hjust = 0.5),
        axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0)),
        legend.text = element_text(size=8),
        legend.position = "bottom")
```

Results obtained for the comparison between emotions self-reported and recognized by the automatic classifier are similar to the ones with human observers (`r table_nums("confusionTable_sr_ar", display = "cite")`). Overall there is a low agreement between emotion self-reported and emotion recognized by the automatic classifier (Accuracy = `r round(confusionMatrix_sr_ar$overall["Accuracy"],2)`, 95%CI[`r round(confusionMatrix_sr_ar$overall["AccuracyLower"],2)`,`r round(confusionMatrix_sr_ar$overall["AccuracyUpper"],2)`]; Kappa = `r round(confusionMatrix_sr_ar$overall["Kappa"],2)`) except for *happiness* (`r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "happiness" & confusionMatrix_sr_ar_freq$sr_emotion == "happiness", "prop_sr"]))` of the video self-reported). Surprisingly the automatic classifier incorrectly recognized as *disgust* an important proportion of videos in which *anger*, *happiness* and *surprise* was the highest self-reported emotion (respectively `r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "disgust" & confusionMatrix_sr_ar_freq$sr_emotion == "anger", "prop_sr"]))`, `r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "disgust" & confusionMatrix_sr_ar_freq$sr_emotion == "happiness", "prop_sr"]))` and `r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "disgust" & confusionMatrix_sr_ar_freq$sr_emotion == "surprise", "prop_sr"]))` of the videos self-reported). In parallel, the automatic classifier recognized as *happiness* videos in which *fear* and *surprise* was the highest self-reported emotion (respectively `r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "happiness" & confusionMatrix_sr_ar_freq$sr_emotion == "fear", "prop_sr"]))` and `r scales::percent(as.numeric(confusionMatrix_sr_ar_freq[confusionMatrix_sr_ar_freq$ar_emotion == "disgust" & confusionMatrix_sr_ar_freq$sr_emotion == "surprise", "prop_sr"]))` of the videos self-reported).

```{r confusionTable_sr_ar, results="asis"}
confusionMatrix_sr_ar_table <- confusionMatrix_sr_ar %>%
  magrittr::use_series(byClass) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("Emotion") %>%
  dplyr::mutate(Emotion = gsub(pattern = "Class: ",replacement = "",x = Emotion)) %>% 
  dplyr::select(Emotion, Sensitivity, Specificity, Precision, F1) %>% 
  dplyr::mutate_if(is.numeric,round,2) %>% 
  replace(is.na(.), "\\textit{na.}")

# confusionMatrix_sr_ar_table %>%
#   knitr::kable(
#     "latex",
#     caption = "Agreement accuracy metrics for each emotion.",
#     booktabs = T,
#     digits = 2,
#     linesep = "",
#     escape = FALSE,
#     align = c('l',rep('c', 4))
#   ) %>%
#   kableExtra::kable_styling(
#     font_size = 8,
#     full_width = TRUE,
#     latex_options = c("hold_position")) %>%
#    add_footnote("Note. \\textit{na.} values are produced when not enough data are available to compute accuracy indicators.", notation="none", escape = FALSE)

confusionMatrix_sr_ar_table %>%
  papaja::apa_table(
    caption = table_nums("confusionTable_sr_ar"),
    row.names = FALSE,
    format = "word",
    escape = FALSE
    )
```

A comparable explanation can be provided as the level of undetermined emotion are very high for the self reports.

## Comparison between human and automatic recognition

As previously mentioned, the accuracy of humans observers and the automatic classifier have some similarities in there mistakes. However human observers appears to be more accurate than the automatic classifier to recognize individuals' subjective feeling (human observer Accuracy = `r round(confusionMatrix_sr_hr$overall["Accuracy"],2)`; automatic classifier Accuracy = `r round(confusionMatrix_sr_ar$overall["Accuracy"],2)`. A third confusion matrix is used to compare similarities between human observers and automatic classifier (`r fig_nums("confusionMatrix_hr_ar", display = "cite")`).

```{r confusionMatrix_hr_ar, fig.height=4, fig.cap=fig_nums("confusionMatrix_hr_ar")}
comparison_hr_ar <- dplyr::inner_join(human_recognition_score,automatic_recognition_score, by = "C_Video") %>% 
  dplyr::ungroup() %>% 
  dplyr::filter(ar_emotion != "undetermined") %>% 
  dplyr::filter(hr_emotion != "undetermined") %>% 
  dplyr::mutate_if(is.character,as.factor)

confusionMatrix_hr_ar <- caret::confusionMatrix(
  data = comparison_hr_ar$ar_emotion,
  reference = comparison_hr_ar$hr_emotion
)

confusionMatrix_hr_ar_freq <- confusionMatrix_hr_ar$table %>% 
  as.data.frame() %>% 
  dplyr::rename(ar_emotion = Prediction, hr_emotion = Reference) %>% 
  dplyr::mutate(n_video = sum(Freq)) %>% 
  dplyr::mutate(prop_video = Freq/sum(Freq)) %>% 
  dplyr::group_by(hr_emotion) %>% 
  dplyr::mutate(n_hr = sum(Freq)) %>%
  dplyr::mutate(prop_hr = Freq/sum(Freq)) %>% 
  dplyr::ungroup()

confusionMatrix_hr_ar_freq %>% 
  ggplot(mapping = aes(x = hr_emotion, y = ar_emotion)) +
  geom_tile(aes(fill = prop_hr), colour = "white") +
  geom_text(
    aes(label = paste0(scales::percent(prop_hr), "\n(", Freq,"/",n_hr, ")")),
    color = "black",
    size = 4,
    family="serif",
    parse = FALSE,
    lineheight = 0.7)+
  scale_fill_gradient(
    name = "Proportion of Recognized\nEmotion",
    low = "white", 
    high = "red", 
    labels = scales::percent,
    limits = c(0,1)
  ) +
  scale_x_discrete(name = "Human Observers") +
  scale_y_discrete(name = "Automatic Classifier") +
  theme_minimal() +
  theme(text = element_text(size=16,family="serif"),
        axis.text.x = element_text(size=16, angle = 45, hjust = 0.75,vjust=0.9),
        axis.text.y = element_text(size=16, hjust = 0.5),
        axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0)),
        legend.text = element_text(size=8),
        legend.position = "bottom")
```

The overall agreement between human observers and the automatic classifier is in fact very low (Kappa = `r round(confusionMatrix_hr_ar$overall["Kappa"],2)`). Except for *happiness* and *disgust* (respectively `r scales::percent(as.numeric(confusionMatrix_hr_ar_freq[confusionMatrix_hr_ar_freq$ar_emotion == "happiness" & confusionMatrix_hr_ar_freq$hr_emotion == "happiness", "prop_hr"]))` and `r scales::percent(as.numeric(confusionMatrix_hr_ar_freq[confusionMatrix_hr_ar_freq$ar_emotion == "disgust" & confusionMatrix_hr_ar_freq$hr_emotion == "disgust", "prop_hr"]))` of common labelling), there is no clear common pattern. Moreover, the automatic classifier has a tendency to label as *disgust* videos labeled as *sadness* by human observers and as *happiness* videos labaled as *fear* by human observers 

Conclusion
============
Despite being one on the most investigated question in affective science, the coherence between emotion felt and facial expression is a hot topic and no clear evidence have been found to definitely answer it. However, with the growing interest of industries and government to monitor individual's psychological states, evidences are showing that facial expressions are in reality not expressing emotions [@mckeown2013analogical]. This research aimed to provide some empirical data to the question. The subjective feeling of participants was compared with human recognition on one side and automatic recognition on the other side. The results reveals a low accuracy for both humans and automatic classifier to accurately identify the inner emotional states of these individuals based on their facial expressions. Moreover it appeared that human observers obtain a higher accuracy to recognize the corresponding emotion label than the automatic classifier is. A possible explaination is that human oberservers are assessing expressing sequences while having access to the full video which provides a context whereas the automatic classifier only assess the videos frame by frame.

Some limitations to this process should be stated over the use of self-reports to evaluate individual's subjective feelings. Accessing the inner subjective feeling can be biased if not impossible. Moreover the procedure used for human observation can also be open to dispute. Instead of asking the human annotators to provide an unique label, a more subtle approach was chosen to mimic results provided by the automatic classifier. Whereas this paradigm is longer and more complicated, it can lead to more robust results in reducing the forced-choice biais [@russell1993forced]. However this procedure can also reduce the human obersvers accuracy. In this regard, the results of the human observation could have been more ambiguous because it is not the natural way that people are inferring meaning from facial expressions. An alternative explaination relies in reducing the recognition biais involved in the classic recognition paradigm. Classic forced-choice paradigms obtain artificial high results, thus by using a more evolved approach observers' accuracy may have been reduced. 

Considering the above, the results provides an additional evidence that individuals' subjective feeling can not be inferred from facial expressions and in our case invalidate the hypothesis of hardwired emotions. Even if emotions were hardwired, in everyday life one does not observe prototypical facial expressions and therefore research should be focused on analysing non-prototypical facial expressions.  Advancements in identifying "non-basic" emotion labels [@mcduff2016discovering] as well as non-prototypical facial expression have been made in the developpement of automatic facial expression recognition tools. However this result suggests that automatic facial expression recognition tools should evaluate facial morphology features such as action units [e.g., OpenFace @baltruvsaitis2016openface] rather than inferring potential emotional or affective states.

Acknowledgment {#acknowledgment}
==============

The authors would like to thank Brigitte Meillon and Jean Michel Adam who developed the software used to collect and preprocess human observer results.

References {#references .numbered}
==========
